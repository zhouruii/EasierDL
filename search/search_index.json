{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"config.html","text":"\u914d\u7f6e\u6587\u4ef6\u8bb2\u89e3 \u6570\u636e \u5173\u4e8e\u6570\u636e\u52a0\u8f7d\u76f8\u5173\u7684\u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff1a data : train : dataset : type : HDF5MultiLevelRainHSIDataset h5_path : /home/disk2/ZR/datasets/AVIRIS/128/dataset.h5 split_file : /home/disk2/ZR/datasets/AVIRIS/128/npy/train.txt pipelines : - type : RandomFlip directions : [ original , horizontal , vertical , diagonal ] probs : [ 0.3 , 0.3 , 0.3 , 0.1 ] - type : RandomRotation angles : [ 0 , 90 , 180 , 270 ] prob : 0.5 - type : EasyToTensor mode : CHW dataloader : batch_size : 8 shuffle : true num_workers : 8 pin_memory : true persistent_workers : false prefetch_factor : 8 drop_last : true val : dataset : type : HDF5MultiLevelRainHSIDataset h5_path : /home/disk2/ZR/datasets/AVIRIS/128/dataset.h5 split_file : /home/disk2/ZR/datasets/AVIRIS/128/npy/val.txt pipelines : - type : EasyToTensor mode : CHW dataloader : batch_size : 8 shuffle : false \u5176\u4e2d\uff0c\u5206\u522b\u5b9a\u4e49\u4e86\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u6570\u636e\u52a0\u8f7d\uff0c\u6bcf\u4e2a\u6570\u636e\u96c6\u90fd\u4f1a\u6709\u5404\u81ea\u7684\u53c2\u6570\uff0c\u4f8b\u5982\uff08 h5_path\uff0csplit_file \uff09 \u6bcf\u4e2a\u6570\u636e\u96c6\u8fd8\u4f1a\u6709\u5bf9\u5e94\u7684\u6570\u636e\u589e\u5f3a\u6d41\u7a0b pipelines \uff0c\u4f8b\u5982\u4e0a\u9762\u7684\u793a\u4f8b\u5c31\u91c7\u7528\u4e86\u968f\u673a\u7ffb\u8f6c\uff0c\u968f\u673a\u65cb\u8f6c\u7b49\u6570\u636e\u589e\u5f3a\u3002 \u6a21\u578b \u6a21\u677f\u76ee\u5f55\u4e2d\u5b58\u653e\u7740\u9884\u8bbe\u7684\u6a21\u578b\u4ee5\u53ca\u5404\u7c7b\u6a21\u5757\u7684\u4f7f\u7528\u793a\u4f8b \u4f8b\u5982\uff1a Swin Transformer\u4f7f\u7528\uff1a model : type : SwinTransformer embedding : type : PatchEmbedding img_size : 4 patch_size : 2 in_channel : 330 embed_dim : 512 norm_layer : nn.LayerNorm ape : false basemodule : type : SwinTransformerLayers dims : 512, 1024 input_resolutions : 2, 1 depths : 1, 1 num_heads : 8, 16 window_sizes : 1,1 mlp_ratio : 4.0 qkv_bias : true qk_scale : null drop_rate : 0.2 attn_drop : 0.0 drop_path_rate : 0.2 norm_layer : nn.LayerNorm downsamples : type : PatchMerging input_resolution : 2 in_channel : 512 head : type : FCHead embed_dim : 1024 pred_num : 1 \u53ea\u9700\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u8fdb\u884c\u76f8\u5173\u53c2\u6570\u6539\u52a8\uff0c\u5373\u53ef\u6620\u5c04\u5230\u5b9e\u9645\u4f7f\u7528\u7684\u6a21\u578b\u3002 \u4e0d\u4ec5\u5982\u6b64\uff0c\u8fd8\u53ef\u4ee5\u591a\u4e2a\u6a21\u5757\u8fdb\u884c\u7ec4\u5408\u4f7f\u7528\uff0c\u4f8b\u5982\u6211\u4eec\u81ea\u5b9a\u4e49\u7684\u6a21\u578b\uff1a model : type : Parallel preprocessor : type : DWT1d scales : 1 wave : haar padding : zero parallels : - type : ChannelTransformer embedding : type : PatchEmbedding img_size : 4 patch_size : 2 in_channel : 165 embed_dim : 256 norm_layer : nn.LayerNorm basemodule : type : ChannelTransformerLayer dim : 256 input_resolution : 2 depth : 2 num_heads : 16 mlp_ratio : 2.0 qkv_bias : true qk_scale : null drop_rate : 0.2 # drop_rate attn_drop : 0.0 drop_path : 0.1 head : type : FCHead embed_dim : 256 pred_num : 1 - type : ChannelTransformer embedding : type : PatchEmbedding img_size : 4 patch_size : 2 in_channel : 165 embed_dim : 256 norm_layer : nn.LayerNorm basemodule : type : ChannelTransformerLayer dim : 256 input_resolution : 2 depth : 1 num_heads : 16 mlp_ratio : 2.0 qkv_bias : true qk_scale : null drop_rate : 0.2 # drop_rate attn_drop : 0.0 drop_path : 0.1 head : type : FCHead embed_dim : 256 pred_num : 1 postprocessor : type : WeightedSum weights : - 0.8 - 0.2 \u8be5\u914d\u7f6e\u6587\u4ef6\uff0c\u5c31\u662f\u7528\u4e86\u5e76\u884c\u7684\u7ed3\u6784\uff0c\u5e76\u884c\u7684\u6bcf\u4e00\u6761\u652f\u8def\u90fd\u662f\u4e00\u4e2a Channel Transformer \u7684\u4e32\u884c\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u9aa8\u5e72\u7f51\u7edc\u524d\u540e\u90fd\u4f1a\u653e\u7f6e\u4e00\u4e2a\u5904\u7406\u5668\uff0c\u8be5\u6a21\u5757\u4ecd\u7136\u53ef\u4ee5\u81ea\u5b9a\u4e49\uff01 \u8bad\u7ec3\u53c2\u6570 \u8bad\u7ec3\u65f6\u9700\u8981\u6709\u4f17\u591a\u7684\u8d85\u53c2\u6570\u9700\u8981\u8c03\u6574\uff0c\u4f7f\u7528\u793a\u4f8b\u5982\u4e0b\uff1a train : total_epoch : &total_epoch 100 use_grad_clip : true print_freq : 50 loss : type : MSELoss optimizer : type : AdamW lr : 3.0e-4 weight_decay : 1.0e-4 betas : [ 0.9 , 0.999 ] scheduler : type : LinearWarmupCosineLR total_epochs : *total_epoch warmup_epochs : 10 warmup_start_lr : 3.0e-4 min_lr : 1.0e-6 val : val_freq : 1 metric : psnr checkpoint : save_freq : 10 resume_from : null auto_resume : false \u4e0a\u8ff0\u914d\u7f6e\u6307\u5b9a\u4e86\u4f17\u591a\u8d85\u53c2\u8bbe\u7f6e\uff0c\u4f8b\u5982\uff0c\u521d\u59cb\u5b66\u4e60\u7387\uff0c\u5b66\u4e60\u7387\u8c03\u6574\uff0c\u4f18\u5316\u5668\uff0c\u8fed\u4ee3\u6570\u7b49\u7b49\u3002","title":"\u914d\u7f6e\u6587\u4ef6\u8bb2\u89e3"},{"location":"config.html#_1","text":"","title":"\u914d\u7f6e\u6587\u4ef6\u8bb2\u89e3"},{"location":"config.html#_2","text":"\u5173\u4e8e\u6570\u636e\u52a0\u8f7d\u76f8\u5173\u7684\u914d\u7f6e\u793a\u4f8b\u5982\u4e0b\uff1a data : train : dataset : type : HDF5MultiLevelRainHSIDataset h5_path : /home/disk2/ZR/datasets/AVIRIS/128/dataset.h5 split_file : /home/disk2/ZR/datasets/AVIRIS/128/npy/train.txt pipelines : - type : RandomFlip directions : [ original , horizontal , vertical , diagonal ] probs : [ 0.3 , 0.3 , 0.3 , 0.1 ] - type : RandomRotation angles : [ 0 , 90 , 180 , 270 ] prob : 0.5 - type : EasyToTensor mode : CHW dataloader : batch_size : 8 shuffle : true num_workers : 8 pin_memory : true persistent_workers : false prefetch_factor : 8 drop_last : true val : dataset : type : HDF5MultiLevelRainHSIDataset h5_path : /home/disk2/ZR/datasets/AVIRIS/128/dataset.h5 split_file : /home/disk2/ZR/datasets/AVIRIS/128/npy/val.txt pipelines : - type : EasyToTensor mode : CHW dataloader : batch_size : 8 shuffle : false \u5176\u4e2d\uff0c\u5206\u522b\u5b9a\u4e49\u4e86\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u6570\u636e\u52a0\u8f7d\uff0c\u6bcf\u4e2a\u6570\u636e\u96c6\u90fd\u4f1a\u6709\u5404\u81ea\u7684\u53c2\u6570\uff0c\u4f8b\u5982\uff08 h5_path\uff0csplit_file \uff09 \u6bcf\u4e2a\u6570\u636e\u96c6\u8fd8\u4f1a\u6709\u5bf9\u5e94\u7684\u6570\u636e\u589e\u5f3a\u6d41\u7a0b pipelines \uff0c\u4f8b\u5982\u4e0a\u9762\u7684\u793a\u4f8b\u5c31\u91c7\u7528\u4e86\u968f\u673a\u7ffb\u8f6c\uff0c\u968f\u673a\u65cb\u8f6c\u7b49\u6570\u636e\u589e\u5f3a\u3002","title":"\u6570\u636e"},{"location":"config.html#_3","text":"\u6a21\u677f\u76ee\u5f55\u4e2d\u5b58\u653e\u7740\u9884\u8bbe\u7684\u6a21\u578b\u4ee5\u53ca\u5404\u7c7b\u6a21\u5757\u7684\u4f7f\u7528\u793a\u4f8b \u4f8b\u5982\uff1a Swin Transformer\u4f7f\u7528\uff1a model : type : SwinTransformer embedding : type : PatchEmbedding img_size : 4 patch_size : 2 in_channel : 330 embed_dim : 512 norm_layer : nn.LayerNorm ape : false basemodule : type : SwinTransformerLayers dims : 512, 1024 input_resolutions : 2, 1 depths : 1, 1 num_heads : 8, 16 window_sizes : 1,1 mlp_ratio : 4.0 qkv_bias : true qk_scale : null drop_rate : 0.2 attn_drop : 0.0 drop_path_rate : 0.2 norm_layer : nn.LayerNorm downsamples : type : PatchMerging input_resolution : 2 in_channel : 512 head : type : FCHead embed_dim : 1024 pred_num : 1 \u53ea\u9700\u5728\u914d\u7f6e\u6587\u4ef6\u4e2d\u8fdb\u884c\u76f8\u5173\u53c2\u6570\u6539\u52a8\uff0c\u5373\u53ef\u6620\u5c04\u5230\u5b9e\u9645\u4f7f\u7528\u7684\u6a21\u578b\u3002 \u4e0d\u4ec5\u5982\u6b64\uff0c\u8fd8\u53ef\u4ee5\u591a\u4e2a\u6a21\u5757\u8fdb\u884c\u7ec4\u5408\u4f7f\u7528\uff0c\u4f8b\u5982\u6211\u4eec\u81ea\u5b9a\u4e49\u7684\u6a21\u578b\uff1a model : type : Parallel preprocessor : type : DWT1d scales : 1 wave : haar padding : zero parallels : - type : ChannelTransformer embedding : type : PatchEmbedding img_size : 4 patch_size : 2 in_channel : 165 embed_dim : 256 norm_layer : nn.LayerNorm basemodule : type : ChannelTransformerLayer dim : 256 input_resolution : 2 depth : 2 num_heads : 16 mlp_ratio : 2.0 qkv_bias : true qk_scale : null drop_rate : 0.2 # drop_rate attn_drop : 0.0 drop_path : 0.1 head : type : FCHead embed_dim : 256 pred_num : 1 - type : ChannelTransformer embedding : type : PatchEmbedding img_size : 4 patch_size : 2 in_channel : 165 embed_dim : 256 norm_layer : nn.LayerNorm basemodule : type : ChannelTransformerLayer dim : 256 input_resolution : 2 depth : 1 num_heads : 16 mlp_ratio : 2.0 qkv_bias : true qk_scale : null drop_rate : 0.2 # drop_rate attn_drop : 0.0 drop_path : 0.1 head : type : FCHead embed_dim : 256 pred_num : 1 postprocessor : type : WeightedSum weights : - 0.8 - 0.2 \u8be5\u914d\u7f6e\u6587\u4ef6\uff0c\u5c31\u662f\u7528\u4e86\u5e76\u884c\u7684\u7ed3\u6784\uff0c\u5e76\u884c\u7684\u6bcf\u4e00\u6761\u652f\u8def\u90fd\u662f\u4e00\u4e2a Channel Transformer \u7684\u4e32\u884c\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u9aa8\u5e72\u7f51\u7edc\u524d\u540e\u90fd\u4f1a\u653e\u7f6e\u4e00\u4e2a\u5904\u7406\u5668\uff0c\u8be5\u6a21\u5757\u4ecd\u7136\u53ef\u4ee5\u81ea\u5b9a\u4e49\uff01","title":"\u6a21\u578b"},{"location":"config.html#_4","text":"\u8bad\u7ec3\u65f6\u9700\u8981\u6709\u4f17\u591a\u7684\u8d85\u53c2\u6570\u9700\u8981\u8c03\u6574\uff0c\u4f7f\u7528\u793a\u4f8b\u5982\u4e0b\uff1a train : total_epoch : &total_epoch 100 use_grad_clip : true print_freq : 50 loss : type : MSELoss optimizer : type : AdamW lr : 3.0e-4 weight_decay : 1.0e-4 betas : [ 0.9 , 0.999 ] scheduler : type : LinearWarmupCosineLR total_epochs : *total_epoch warmup_epochs : 10 warmup_start_lr : 3.0e-4 min_lr : 1.0e-6 val : val_freq : 1 metric : psnr checkpoint : save_freq : 10 resume_from : null auto_resume : false \u4e0a\u8ff0\u914d\u7f6e\u6307\u5b9a\u4e86\u4f17\u591a\u8d85\u53c2\u8bbe\u7f6e\uff0c\u4f8b\u5982\uff0c\u521d\u59cb\u5b66\u4e60\u7387\uff0c\u5b66\u4e60\u7387\u8c03\u6574\uff0c\u4f18\u5316\u5668\uff0c\u8fed\u4ee3\u6570\u7b49\u7b49\u3002","title":"\u8bad\u7ec3\u53c2\u6570"},{"location":"data.html","text":"\u6570\u636e\u96c6\u51c6\u5907 \u5728\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u65b0\u5efadata\u76ee\u5f55\u6765\u5b58\u653e\u6570\u636e\u96c6 \u9ad8\u5149\u8c31\u56fe\u50cf data\u76ee\u5f55\u4e0b\u7684\u6570\u636e\u7ec4\u7ec7\u5f62\u5f0f\u4e3a\uff1a Project Root \u251c\u2500\u2500 uchiha \u251c\u2500\u2500 scripts \u251c\u2500\u2500 configs \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 spectral \u2502 \u2502 \u251c\u2500\u2500 train \u2502 \u2502 \u2502 \u2502\u2500\u2500 reflectivity \u2502 \u2502 \u2502 \u2502\u2500\u2500 GT.txt \u2502 \u2502 \u251c\u2500\u2500 val \u2502 \u2502 \u2502 \u2502\u2500\u2500 reflectivity \u2502 \u2502 \u2502 \u2502\u2500\u2500 GT.txt \u2502 \u2502 \u251c\u2500\u2500 trainval(optional) \u2502 \u2502 \u2502 \u2502\u2500\u2500 reflectivity \u2502 \u2502 \u2502 \u2502\u2500\u2500 GT.txt \u2502 \u2502 \u251c\u2500\u2500 test \u2502 \u2502 \u2502 \u2502\u2500\u2500 reflectivity \u2502 \u2502 \u2502 \u2502\u2500\u2500 GT.txt","title":"\u6570\u636e\u96c6\u51c6\u5907"},{"location":"data.html#_1","text":"\u5728\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u65b0\u5efadata\u76ee\u5f55\u6765\u5b58\u653e\u6570\u636e\u96c6","title":"\u6570\u636e\u96c6\u51c6\u5907"},{"location":"data.html#_2","text":"data\u76ee\u5f55\u4e0b\u7684\u6570\u636e\u7ec4\u7ec7\u5f62\u5f0f\u4e3a\uff1a Project Root \u251c\u2500\u2500 uchiha \u251c\u2500\u2500 scripts \u251c\u2500\u2500 configs \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 spectral \u2502 \u2502 \u251c\u2500\u2500 train \u2502 \u2502 \u2502 \u2502\u2500\u2500 reflectivity \u2502 \u2502 \u2502 \u2502\u2500\u2500 GT.txt \u2502 \u2502 \u251c\u2500\u2500 val \u2502 \u2502 \u2502 \u2502\u2500\u2500 reflectivity \u2502 \u2502 \u2502 \u2502\u2500\u2500 GT.txt \u2502 \u2502 \u251c\u2500\u2500 trainval(optional) \u2502 \u2502 \u2502 \u2502\u2500\u2500 reflectivity \u2502 \u2502 \u2502 \u2502\u2500\u2500 GT.txt \u2502 \u2502 \u251c\u2500\u2500 test \u2502 \u2502 \u2502 \u2502\u2500\u2500 reflectivity \u2502 \u2502 \u2502 \u2502\u2500\u2500 GT.txt","title":"\u9ad8\u5149\u8c31\u56fe\u50cf"},{"location":"install.html","text":"1.\u514b\u9686\u672c\u4ed3\u5e93 git clone https://github.com/zhouruii/uchiha.git cd uchiha-main 2.\u521b\u5efa\u865a\u62df\u73af\u5883 conda create -n HDRformer python=3.9 3.\u5b89\u88c5torch \u6ce8\u610fpytorch\u4e0eCUDA\u7248\u672c\u7684\u9009\u62e9,\u53c2\u8003 pytorch\u5b98\u7f51 pip3 install torch==1.8.2 torchvision==0.9.2 torchaudio==0.8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111 4.\u5b89\u88c5\u4f9d\u8d56 pip install -r requirements.txt","title":"Install"},{"location":"install.html#1","text":"git clone https://github.com/zhouruii/uchiha.git cd uchiha-main","title":"1.\u514b\u9686\u672c\u4ed3\u5e93"},{"location":"install.html#2","text":"conda create -n HDRformer python=3.9","title":"2.\u521b\u5efa\u865a\u62df\u73af\u5883"},{"location":"install.html#3torch","text":"\u6ce8\u610fpytorch\u4e0eCUDA\u7248\u672c\u7684\u9009\u62e9,\u53c2\u8003 pytorch\u5b98\u7f51 pip3 install torch==1.8.2 torchvision==0.9.2 torchaudio==0.8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111","title":"3.\u5b89\u88c5torch"},{"location":"install.html#4","text":"pip install -r requirements.txt","title":"4.\u5b89\u88c5\u4f9d\u8d56"},{"location":"models.html","text":"Models Stack Stack = pre-process + stack-modules + post-process Parallel Parallel = pre-process + Stacks + post-process","title":"Models"},{"location":"models.html#models","text":"","title":"Models"},{"location":"models.html#stack","text":"Stack = pre-process + stack-modules + post-process","title":"Stack"},{"location":"models.html#parallel","text":"Parallel = pre-process + Stacks + post-process","title":"Parallel"},{"location":"mkdocs/index.html","text":"Welcome to Uchiha ! For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout configs docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. scripts/ ***.py ... uchiha/ apis/ cores/ datasets/ pipelines/ ... models/ ... ... mkdocs.yml # The configuration file.","title":"Home"},{"location":"mkdocs/index.html#welcome-to-uchiha","text":"For full documentation visit mkdocs.org .","title":"Welcome to Uchiha !"},{"location":"mkdocs/index.html#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"mkdocs/index.html#project-layout","text":"configs docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. scripts/ ***.py ... uchiha/ apis/ cores/ datasets/ pipelines/ ... models/ ... ... mkdocs.yml # The configuration file.","title":"Project layout"},{"location":"mkdocs/api.html","text":"API Welcome to this document! TODO","title":"API"},{"location":"mkdocs/api.html#api","text":"Welcome to this document! TODO","title":"API"},{"location":"mkdocs/dataset.html","text":"Datasets Pipelines Compose Compose multiple transforms sequentially. Parameters: transforms ( Sequence [ dict | callable ] ) \u2013 Sequence of transform object or config dict to be composed. Source code in uchiha\\datasets\\pipelines\\compose.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @PIPELINES . register_module () class Compose : \"\"\" Compose multiple transforms sequentially. Args: transforms (Sequence[dict | callable]): Sequence of transform object or config dict to be composed. \"\"\" def __init__ ( self , transforms ): # assert isinstance(transforms, collections.abc.Sequence) self . transforms = [] for transform in transforms : if isinstance ( transform , dict ): transform = build_from_cfg ( transform , PIPELINES ) self . transforms . append ( transform ) elif callable ( transform ): self . transforms . append ( transform ) else : raise TypeError ( 'transform must be callable or a dict' ) def __call__ ( self , data ): \"\"\"Call function to apply transforms sequentially. Args: data (dict): A result dict contains the data to transform. Returns: data (dict): Transformed data. \"\"\" for t in self . transforms : data = t ( data ) if data is None : return None return data def __repr__ ( self ): format_string = self . __class__ . __name__ + '(' for t in self . transforms : str_ = t . __repr__ () if 'Compose(' in str_ : str_ = str_ . replace ( ' \\n ' , ' \\n ' ) format_string += ' \\n ' format_string += f ' { str_ } ' format_string += ' \\n )' return format_string __call__ ( data ) Call function to apply transforms sequentially. Parameters: data ( dict ) \u2013 A result dict contains the data to transform. Returns: data ( dict ) \u2013 Transformed data. Source code in uchiha\\datasets\\pipelines\\compose.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __call__ ( self , data ): \"\"\"Call function to apply transforms sequentially. Args: data (dict): A result dict contains the data to transform. Returns: data (dict): Transformed data. \"\"\" for t in self . transforms : data = t ( data ) if data is None : return None return data CenterCrop Source code in uchiha\\datasets\\pipelines\\crop.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @PIPELINES . register_module () class CenterCrop : def __init__ ( self , size ): \"\"\" Args: size (tuple): the target size of the crop height width \"\"\" self . size = size def __call__ ( self , data ): sample = data [ 'sample' ] target = data [ 'target' ] h , w = sample . shape [: 2 ] th , tw = self . size # If the image is smaller than the crop size, resize to the crop size first if h < th or w < tw : import cv2 sample = cv2 . resize ( sample , ( tw , th ), interpolation = cv2 . INTER_LINEAR ) target = cv2 . resize ( target , ( tw , th ), interpolation = cv2 . INTER_NEAREST ) data [ 'sample' ] = sample data [ 'target' ] = target return data # Calculate the starting coordinates of the center clipping i = ( h - th ) // 2 j = ( w - tw ) // 2 # crop if len ( sample . shape ) == 3 : sample_cropped = sample [ i : i + th , j : j + tw , :] else : sample_cropped = sample [ i : i + th , j : j + tw ] if len ( target . shape ) == 3 : target_cropped = target [ i : i + th , j : j + tw , :] else : target_cropped = target [ i : i + th , j : j + tw ] data [ 'sample' ] = sample_cropped data [ 'target' ] = target_cropped return data __init__ ( size ) Parameters: size ( tuple ) \u2013 the target size of the crop height width Source code in uchiha\\datasets\\pipelines\\crop.py 60 61 62 63 64 65 def __init__ ( self , size ): \"\"\" Args: size (tuple): the target size of the crop height width \"\"\" self . size = size RandomCrop Source code in uchiha\\datasets\\pipelines\\crop.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @PIPELINES . register_module () class RandomCrop : def __init__ ( self , crop_size , prob = 0.5 ): \"\"\" Args: crop_size (tuple): the target size of the crop height width prob (float): probability of triggering enhancement \"\"\" if isinstance ( crop_size , int ): crop_size = [ crop_size , crop_size ] self . size = crop_size self . p = prob def __call__ ( self , data ): if random . random () < self . p : sample = data [ 'sample' ] target = data [ 'target' ] h , w = sample . shape [: 2 ] th , tw = self . size # If the image is smaller than the crop size, resize to the crop size first if h < th or w < tw : sample = cv2 . resize ( sample , ( tw , th ), interpolation = cv2 . INTER_LINEAR ) target = cv2 . resize ( target , ( tw , th ), interpolation = cv2 . INTER_NEAREST ) data [ 'sample' ] = sample data [ 'target' ] = target return data # randomly select crop start point i = random . randint ( 0 , h - th ) j = random . randint ( 0 , w - tw ) # crop image if len ( sample . shape ) == 3 : sample_cropped = sample [ i : i + th , j : j + tw , :] else : sample_cropped = sample [ i : i + th , j : j + tw ] if len ( target . shape ) == 3 : target_cropped = target [ i : i + th , j : j + tw , :] else : target_cropped = target [ i : i + th , j : j + tw ] data [ 'sample' ] = sample_cropped data [ 'target' ] = target_cropped return data __init__ ( crop_size , prob = 0.5 ) Parameters: crop_size ( tuple ) \u2013 the target size of the crop height width prob ( float , default: 0.5 ) \u2013 probability of triggering enhancement Source code in uchiha\\datasets\\pipelines\\crop.py 10 11 12 13 14 15 16 17 18 19 def __init__ ( self , crop_size , prob = 0.5 ): \"\"\" Args: crop_size (tuple): the target size of the crop height width prob (float): probability of triggering enhancement \"\"\" if isinstance ( crop_size , int ): crop_size = [ crop_size , crop_size ] self . size = crop_size self . p = prob RandomFlip Source code in uchiha\\datasets\\pipelines\\flip.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @PIPELINES . register_module () class RandomFlip : def __init__ ( self , probs = None , directions = None ): \"\"\" Args: probs (list): The probability weights of the four flips, in the order: [Original probability, horizontal flip probability, vertical flip probability, diagonal flip probability] \"\"\" if directions is None : directions = [ 'original' , 'horizontal' , 'vertical' , 'diagonal' ] if probs is None : probs = [ 0.5 , 0.25 , 0.25 , 0.0 ] self . p = probs # normalized probability total = sum ( probs ) self . norm_p = [ x / total for x in probs ] # building a choice space self . directions = directions # Cumulative probability is used for random selection cum = 0.0 self . cum_probs = [] for probs in self . norm_p : cum += probs self . cum_probs . append ( cum ) def _apply_flip ( self , img , flip_type ): \"\"\"perform specific flip operations\"\"\" if flip_type == 'original' : return img elif flip_type == 'horizontal' : return img [:, :: - 1 , ... ] . copy () # HWC / HW elif flip_type == 'vertical' : return img [:: - 1 , :, ... ] . copy () elif flip_type == 'diagonal' : return img [:, :: - 1 , ... ][:: - 1 , :, ... ] . copy () else : raise ValueError ( f \"Unknown flip type: { flip_type } \" ) def __call__ ( self , data ): sample = data [ 'sample' ] target = data [ 'target' ] # use cumulative probability to select flip type rand_val = random . random () selected = self . directions [ - 1 ] for i , cp in enumerate ( self . cum_probs ): if rand_val < cp : selected = self . directions [ i ] break # flip sample = self . _apply_flip ( sample , selected ) target = self . _apply_flip ( target , selected ) data [ 'sample' ] = sample data [ 'target' ] = target return data __init__ ( probs = None , directions = None ) Args: probs (list): The probability weights of the four flips, in the order: [Original probability, horizontal flip probability, vertical flip probability, diagonal flip probability] Source code in uchiha\\datasets\\pipelines\\flip.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , probs = None , directions = None ): \"\"\" Args: probs (list): The probability weights of the four flips, in the order: [Original probability, horizontal flip probability, vertical flip probability, diagonal flip probability] \"\"\" if directions is None : directions = [ 'original' , 'horizontal' , 'vertical' , 'diagonal' ] if probs is None : probs = [ 0.5 , 0.25 , 0.25 , 0.0 ] self . p = probs # normalized probability total = sum ( probs ) self . norm_p = [ x / total for x in probs ] # building a choice space self . directions = directions # Cumulative probability is used for random selection cum = 0.0 self . cum_probs = [] for probs in self . norm_p : cum += probs self . cum_probs . append ( cum ) EasyToTensor convert data to tensor The default input data is in B H W C form, converting it to B C H W form Parameters: mode ( str , default: 'CHW' ) \u2013 data order form Source code in uchiha\\datasets\\pipelines\\formatting.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @PIPELINES . register_module () class EasyToTensor : \"\"\" convert data to tensor The default input data is in `B H W C` form, converting it to `B C H W` form Args: mode (str): data order form \"\"\" def __init__ ( self , mode = 'CHW' ): self . mode = mode def __call__ ( self , results ): \"\"\"Call function to convert the type of the data. Args: results (dict): Result dict from pipeline. Returns: results (dict): converted results \"\"\" assert 'sample' in results and 'target' in results , 'sample and target should be loaded to results' if self . mode == 'CHW' : results [ 'sample' ] = torch . Tensor ( results [ 'sample' ]) . permute ( 2 , 0 , 1 ) results [ 'target' ] = torch . Tensor ( results [ 'target' ]) . permute ( 2 , 0 , 1 ) else : results [ 'sample' ] = torch . Tensor ( results [ 'sample' ]) results [ 'target' ] = torch . Tensor ( results [ 'target' ]) return results __call__ ( results ) Call function to convert the type of the data. Parameters: results ( dict ) \u2013 Result dict from pipeline. Returns: results ( dict ) \u2013 converted results Source code in uchiha\\datasets\\pipelines\\formatting.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def __call__ ( self , results ): \"\"\"Call function to convert the type of the data. Args: results (dict): Result dict from pipeline. Returns: results (dict): converted results \"\"\" assert 'sample' in results and 'target' in results , 'sample and target should be loaded to results' if self . mode == 'CHW' : results [ 'sample' ] = torch . Tensor ( results [ 'sample' ]) . permute ( 2 , 0 , 1 ) results [ 'target' ] = torch . Tensor ( results [ 'target' ]) . permute ( 2 , 0 , 1 ) else : results [ 'sample' ] = torch . Tensor ( results [ 'sample' ]) results [ 'target' ] = torch . Tensor ( results [ 'target' ]) return results img_normalize ( img , mean , std , to_rgb = True , scope = 'spatial' , mode = 'standard' ) Normalize an image with mean and std. Parameters: img ( ndarray ) \u2013 Image to be normalized. mean ( ndarray ) \u2013 The mean to be used for normalize. std ( ndarray ) \u2013 The std to be used for normalize. to_rgb ( bool , default: True ) \u2013 Whether to convert to rgb. scope ( str , default: 'spatial' ) \u2013 Normalized scope. Returns: ndarray \u2013 The normalized image. Source code in uchiha\\datasets\\pipelines\\geometric.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def img_normalize ( img , mean , std , to_rgb = True , scope = 'spatial' , mode = 'standard' ): \"\"\"Normalize an image with mean and std. Args: img (ndarray): Image to be normalized. mean (ndarray): The mean to be used for normalize. std (ndarray): The std to be used for normalize. to_rgb (bool): Whether to convert to rgb. scope (str): Normalized scope. Returns: ndarray: The normalized image. \"\"\" img = img . copy () . astype ( np . float32 ) return img_normalize_ ( img , mean , std , to_rgb , scope , mode ) img_normalize_ ( img , mean , std , to_rgb = True , scope = 'spatial' , mode = 'standard' ) Inplace normalize an image with mean and std. Parameters: img ( ndarray ) \u2013 Image to be normalized. mean ( ndarray ) \u2013 The mean to be used for normalize. std ( ndarray ) \u2013 The std to be used for normalize. to_rgb ( bool , default: True ) \u2013 Whether to convert to rgb. scope ( str , default: 'spatial' ) \u2013 Normalized scope. Returns: ndarray \u2013 The normalized image. Source code in uchiha\\datasets\\pipelines\\geometric.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def img_normalize_ ( img , mean , std , to_rgb = True , scope = 'spatial' , mode = 'standard' ): \"\"\"Inplace normalize an image with mean and std. Args: img (ndarray): Image to be normalized. mean (ndarray): The mean to be used for normalize. std (ndarray): The std to be used for normalize. to_rgb (bool): Whether to convert to rgb. scope (str): Normalized scope. Returns: ndarray: The normalized image. \"\"\" # cv2 inplace normalization does not accept uint8 assert img . dtype != np . uint8 channel = img . shape [ - 1 ] mean = np . float64 ( mean . reshape ( 1 , - 1 )) if mean else None stdinv = 1 / np . float64 ( std . reshape ( 1 , - 1 )) if std else None if to_rgb and channel == 3 : cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB , img ) # inplace if scope == 'spatial' : if not mean : mean = np . float64 ( np . mean ( img , axis = ( 0 , 1 )) . reshape ( 1 , - 1 )) if not std : std = np . float64 ( np . std ( img , axis = ( 0 , 1 )) . reshape ( 1 , - 1 )) mask = std != 0 stdinv = np . zeros_like ( std ) stdinv [ mask ] = 1 / std [ mask ] _min = np . float64 ( np . min ( img , axis = ( 0 , 1 )) . reshape ( 1 , - 1 )) _max = np . float64 ( np . max ( img , axis = ( 0 , 1 )) . reshape ( 1 , - 1 )) else : if not mean : mean = np . float32 ( np . mean ( img , axis = 2 ))[:, :, None ] mean = np . tile ( mean , ( 1 , 1 , channel )) if not std : std = np . float32 ( np . std ( img , axis = 2 ))[:, :, None ] std = np . tile ( std , ( 1 , 1 , channel )) stdinv = 1 / std _min = np . tile ( np . float32 ( np . min ( img , axis = 2 ))[:, :, None ], ( 1 , 1 , channel )) _max = np . tile ( np . float32 ( np . max ( img , axis = 2 ))[:, :, None ], ( 1 , 1 , channel )) if mode == 'standard' : cv2 . subtract ( img , mean , img ) # inplace cv2 . multiply ( img , stdinv , img ) # inplace elif mode == 'minmax' : cv2 . subtract ( img , _min , img ) # inplace cv2 . multiply ( img , 1 / ( _max - _min ), img ) # inplace else : raise NotImplementedError ( f \"normalize mode: { mode } is not supported yet\" ) return img impad ( img , * , shape = None , padding = None , pad_val = 0 , padding_mode = 'constant' ) Pad the given image to a certain shape or pad on all sides with specified padding mode and padding value. Parameters: img ( ndarray ) \u2013 Image to be padded. shape ( tuple [ int ] , default: None ) \u2013 Expected padding shape (h, w). Default: None. padding ( int or tuple [ int ] , default: None ) \u2013 Padding on each border. If a single int is provided this is used to pad all borders. If tuple of length 2 is provided this is the padding on top/bottom and left/right respectively. If a tuple of length 4 is provided this is the padding for the top, bottom, left and right borders respectively. Default: None. Note that shape and padding can not be both set. pad_val ( Number | Sequence [ Number ] , default: 0 ) \u2013 Values to be filled in padding areas when padding_mode is 'constant'. Default: 0. padding_mode ( str , default: 'constant' ) \u2013 Type of padding. Should be: constant, edge, reflect or symmetric. Default: constant. - constant: pads with a constant value, this value is specified with pad_val. - edge: pads with the last value at the edge of the image. - reflect: pads with reflection of image without repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]. - symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] Returns: ndarray ( ndarray ) \u2013 The padded image. Source code in uchiha\\datasets\\pipelines\\geometric.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def impad ( img : np . ndarray , * , shape : Optional [ Tuple [ int , int ]] = None , padding : Union [ int , tuple , None ] = None , pad_val : Union [ float , List ] = 0 , padding_mode : str = 'constant' ) -> np . ndarray : \"\"\"Pad the given image to a certain shape or pad on all sides with specified padding mode and padding value. Args: img (ndarray): Image to be padded. shape (tuple[int]): Expected padding shape (h, w). Default: None. padding (int or tuple[int]): Padding on each border. If a single int is provided this is used to pad all borders. If tuple of length 2 is provided this is the padding on top/bottom and left/right respectively. If a tuple of length 4 is provided this is the padding for the top, bottom, left and right borders respectively. Default: None. Note that `shape` and `padding` can not be both set. pad_val (Number | Sequence[Number]): Values to be filled in padding areas when padding_mode is 'constant'. Default: 0. padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric. Default: constant. - constant: pads with a constant value, this value is specified with pad_val. - edge: pads with the last value at the edge of the image. - reflect: pads with reflection of image without repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]. - symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] Returns: ndarray: The padded image. \"\"\" assert ( shape is not None ) ^ ( padding is not None ) if shape is not None : width = max ( shape [ 1 ] - img . shape [ 1 ], 0 ) height = max ( shape [ 0 ] - img . shape [ 0 ], 0 ) padding = ( height // 2 , math . ceil ( height / 2 ), width // 2 , math . ceil ( width / 2 )) # check pad_val if isinstance ( pad_val , tuple ): assert len ( pad_val ) == img . shape [ - 1 ] elif not isinstance ( pad_val , numbers . Number ): raise TypeError ( 'pad_val must be a int or a tuple. ' f 'But received { type ( pad_val ) } ' ) # check padding if isinstance ( padding , tuple ) and len ( padding ) in [ 2 , 4 ]: if len ( padding ) == 2 : padding = ( padding [ 0 ], padding [ 1 ], padding [ 0 ], padding [ 1 ]) elif isinstance ( padding , numbers . Number ): padding = ( padding , padding , padding , padding ) else : raise ValueError ( 'Padding must be a int or a 2, or 4 element tuple.' f 'But received { padding } ' ) # check padding mode assert padding_mode in [ 'constant' , 'edge' , 'reflect' , 'symmetric' ] border_type = { 'constant' : cv2 . BORDER_CONSTANT , 'edge' : cv2 . BORDER_REPLICATE , 'reflect' : cv2 . BORDER_REFLECT_101 , 'symmetric' : cv2 . BORDER_REFLECT } # check dim <= 512 if img . shape [ - 1 ] > 512 : img_patch1 = cv2 . copyMakeBorder ( img [:, :, : 512 ], padding [ 0 ], padding [ 1 ], padding [ 2 ], padding [ 3 ], border_type [ padding_mode ], value = pad_val ) img_patch2 = cv2 . copyMakeBorder ( img [:, :, 512 :], padding [ 0 ], padding [ 1 ], padding [ 2 ], padding [ 3 ], border_type [ padding_mode ], value = pad_val ) img = np . concatenate (( img_patch1 , img_patch2 ), axis =- 1 ) else : img = cv2 . copyMakeBorder ( img , padding [ 0 ], padding [ 1 ], padding [ 2 ], padding [ 3 ], border_type [ padding_mode ], value = pad_val ) return img impad_to_multiple ( img , divisor , pad_val = 0 , padding_mode = 'constant' ) Pad an image to ensure each edge to be multiple to some number. Parameters: img ( ndarray ) \u2013 Image to be padded. divisor ( int ) \u2013 Padded image edges will be multiple to divisor. pad_val ( Number | Sequence [ Number ] , default: 0 ) \u2013 Same as :func: impad . padding_mode ( str , default: 'constant' ) \u2013 refer to impad Returns: ndarray ( ndarray ) \u2013 The padded image. Source code in uchiha\\datasets\\pipelines\\geometric.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def impad_to_multiple ( img : np . ndarray , divisor : int , pad_val : Union [ float , List ] = 0 , padding_mode : str = 'constant' ) -> np . ndarray : \"\"\"Pad an image to ensure each edge to be multiple to some number. Args: img (ndarray): Image to be padded. divisor (int): Padded image edges will be multiple to divisor. pad_val (Number | Sequence[Number]): Same as :func:`impad`. padding_mode: refer to impad Returns: ndarray: The padded image. \"\"\" pad_h = int ( np . ceil ( img . shape [ 0 ] / divisor )) * divisor pad_w = int ( np . ceil ( img . shape [ 1 ] / divisor )) * divisor return impad ( img , shape = ( pad_h , pad_w ), pad_val = pad_val , padding_mode = padding_mode ) Normalize Normalize the image. When the mean and standard deviation are given,the data will be normalized according to the given data, otherwise the mean and standard deviation of the input data will be calculated for normalization. There are two normalization modes: (1) standardize: (x-mean)/std (2) normalize(minmax): (x-min)/(max-min) There are two normalization scopes: (1) spatial: normalize the spatial data for each channel (2) channel: normalize the channel data for each pixel in space Added key is \"img_norm_cfg\". Parameters: mean ( sequence , default: None ) \u2013 Mean values of 3 channels. Default: None. std ( sequence , default: None ) \u2013 Std values of 3 channels. Default: None. to_rgb ( bool , default: True ) \u2013 Whether to convert the image from BGR to RGB. Default: true. mode ( str , default: 'standard' ) \u2013 normalization mode. scope ( str , default: 'spatial' ) \u2013 normalization scope Source code in uchiha\\datasets\\pipelines\\normalize.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @PIPELINES . register_module () class Normalize : \"\"\"Normalize the image. When the mean and standard deviation are given,the data will be normalized according to the given data, otherwise the mean and standard deviation of the input data will be calculated for normalization. There are two normalization modes: (1) standardize: (x-mean)/std (2) normalize(minmax): (x-min)/(max-min) There are two normalization scopes: (1) spatial: normalize the spatial data for each channel (2) channel: normalize the channel data for each pixel in space Added key is \"img_norm_cfg\". Args: mean (sequence): Mean values of 3 channels. Default: None. std (sequence): Std values of 3 channels. Default: None. to_rgb (bool): Whether to convert the image from BGR to RGB. Default: true. mode (str): normalization mode. scope (str): normalization scope \"\"\" def __init__ ( self , mean = None , std = None , to_rgb = True , scope = 'spatial' , mode = 'standard' ): self . mean = np . array ( mean , dtype = np . float32 ) if mean else None self . std = np . array ( std , dtype = np . float32 ) if std else None self . to_rgb = to_rgb self . scope = scope self . mode = mode def __call__ ( self , results ): \"\"\"Call function to normalize images. Args: results (dict): Result dict from pipeline. Returns: results (dict): Normalized results, 'img_norm_cfg' key is added into result dict. \"\"\" results [ 'sample' ] = img_normalize ( results [ 'sample' ], self . mean , self . std , self . to_rgb , self . scope , self . mode ) if self . mean and self . std : results [ 'norm_cfg' ] = dict ( mean = self . mean , std = self . std , to_rgb = self . to_rgb , scope = self . scope ) else : results [ 'norm_cfg' ] = dict ( mean = 'auto' , std = 'auto' , to_rgb = self . to_rgb , scope = self . scope ) return results def __repr__ ( self ): repr_str = self . __class__ . __name__ repr_str += f '(mean= { self . mean } , std= { self . std } , to_rgb= { self . to_rgb } )' return repr_str __call__ ( results ) Call function to normalize images. Parameters: results ( dict ) \u2013 Result dict from pipeline. Returns: results ( dict ) \u2013 Normalized results, 'img_norm_cfg' key is added into result dict. Source code in uchiha\\datasets\\pipelines\\normalize.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __call__ ( self , results ): \"\"\"Call function to normalize images. Args: results (dict): Result dict from pipeline. Returns: results (dict): Normalized results, 'img_norm_cfg' key is added into result dict. \"\"\" results [ 'sample' ] = img_normalize ( results [ 'sample' ], self . mean , self . std , self . to_rgb , self . scope , self . mode ) if self . mean and self . std : results [ 'norm_cfg' ] = dict ( mean = self . mean , std = self . std , to_rgb = self . to_rgb , scope = self . scope ) else : results [ 'norm_cfg' ] = dict ( mean = 'auto' , std = 'auto' , to_rgb = self . to_rgb , scope = self . scope ) return results Pad Pad the image. There are two padding modes: (1) pad to a fixed size (2) pad to the minimum size that is divisible by some number. there are four ways to handle edges: (1) constant: pad with a constant -> the given pad_val (2) edge: pad with the last value at the edge of the image. (3) reflect: pads with reflection of image without repeating the last value on the edge. For example: padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]. (4) symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] Added key is 'pad_cfg'. Parameters: size ( tuple , default: None ) \u2013 Fixed padding size -> mode(1). size_divisor ( int , default: None ) \u2013 The divisor of padded size -> mode(2). pad_val ( dict , default: 0 ) \u2013 A dict for padding value, Default: 0 . mode ( str , default: 'constant' ) \u2013 the way to handle edges Source code in uchiha\\datasets\\pipelines\\pad.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @PIPELINES . register_module () class Pad : \"\"\" Pad the image. There are two padding modes: (1) pad to a fixed size (2) pad to the minimum size that is divisible by some number. there are four ways to handle edges: (1) constant: pad with a constant -> the given pad_val (2) edge: pad with the last value at the edge of the image. (3) reflect: pads with reflection of image without repeating the last value on the edge. For example: padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]. (4) symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] Added key is 'pad_cfg'. Args: size (tuple, optional): Fixed padding size -> mode(1). size_divisor (int, optional): The divisor of padded size -> mode(2). pad_val (dict, optional): A dict for padding value, Default: `0`. mode (str): the way to handle edges \"\"\" def __init__ ( self , size = None , size_divisor = None , pad_val = 0 , mode = 'constant' ): self . size = size self . size_divisor = size_divisor self . pad_val = pad_val self . mode = mode assert size is not None or size_divisor is not None , \\ 'only one of size and size_divisor should be valid' def _pad_img ( self , results ): \"\"\"Pad images according to ``self.size``.\"\"\" pad_val = self . pad_val if 'sample' in results : key = 'sample' else : key = None results [ 'pad_cfg' ] = dict ( ori_shape = results [ key ] . shape , pad_shape = None ) if self . size is not None : padded_img = impad ( results [ key ], shape = self . size , pad_val = pad_val , padding_mode = self . mode ) elif self . size_divisor is not None : padded_img = impad_to_multiple ( results [ key ], self . size_divisor , pad_val = pad_val , padding_mode = self . mode ) else : padded_img = None results [ key ] = padded_img results [ 'pad_cfg' ][ 'pad_shape' ] = padded_img . shape def __call__ ( self , results ): \"\"\"Call function to pad images. Args: results (dict): Result dict from loading pipeline. Returns: results (dict): Updated result dict,'pad_cfg' key is added into result dict. \"\"\" self . _pad_img ( results ) return results def __repr__ ( self ): repr_str = self . __class__ . __name__ repr_str += f '(size= { self . size } , ' repr_str += f 'size_divisor= { self . size_divisor } , ' repr_str += f 'pad_val= { self . pad_val } )' return repr_str __call__ ( results ) Call function to pad images. Parameters: results ( dict ) \u2013 Result dict from loading pipeline. Returns: results ( dict ) \u2013 Updated result dict,'pad_cfg' key is added into result dict. Source code in uchiha\\datasets\\pipelines\\pad.py 64 65 66 67 68 69 70 71 72 73 74 def __call__ ( self , results ): \"\"\"Call function to pad images. Args: results (dict): Result dict from loading pipeline. Returns: results (dict): Updated result dict,'pad_cfg' key is added into result dict. \"\"\" self . _pad_img ( results ) return results RandomRotation Source code in uchiha\\datasets\\pipelines\\rotation.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 @PIPELINES . register_module () class RandomRotation : def __init__ ( self , angles = None , prob = 0.5 , interpolation = cv2 . INTER_LINEAR , border_mode = cv2 . BORDER_REFLECT_101 ): \"\"\" Args: angles (tuple): rotation angle range for example 30 30 prob (float): probability of triggering enhancement interpolation (int): interpolation method\uff08cv2.INTER_NEAREST / INTER_LINEAR / INTER_CUBIC\uff09 border_mode (int): boundary fill method\uff08cv2.BORDER_CONSTANT / BORDER_REPLICATE / BORDER_REFLECT_101\uff09 \"\"\" if angles is None : angles = [ - 90 , 90 ] self . angles = angles self . p = prob self . interpolation = interpolation self . border_mode = border_mode def __call__ ( self , data ): if random . random () < self . p : sample = data [ 'sample' ] target = data [ 'target' ] h , w = sample . shape [: 2 ] center = ( w / 2 , h / 2 ) # randomly generate rotation angle angle = random . choice ( self . angles ) if isinstance ( angle , int ): angle = float ( angle ) # compute the rotation matrix M = cv2 . getRotationMatrix2D ( center , angle , scale = 1.0 ) # applying affine transformation if len ( sample . shape ) == 3 : # HWC sample_rotated = cv2 . warpAffine ( sample , M , ( w , h ), flags = self . interpolation , borderMode = self . border_mode ) else : # HW sample_rotated = cv2 . warpAffine ( sample , M , ( w , h ), flags = self . interpolation , borderMode = self . border_mode ) if len ( target . shape ) == 3 : # HWC target_rotated = cv2 . warpAffine ( target , M , ( w , h ), flags = self . interpolation , borderMode = self . border_mode ) else : # HW target_rotated = cv2 . warpAffine ( target , M , ( w , h ), flags = self . interpolation , borderMode = self . border_mode ) data [ 'sample' ] = sample_rotated data [ 'target' ] = target_rotated return data __init__ ( angles = None , prob = 0.5 , interpolation = cv2 . INTER_LINEAR , border_mode = cv2 . BORDER_REFLECT_101 ) Parameters: angles ( tuple , default: None ) \u2013 rotation angle range for example 30 30 prob ( float , default: 0.5 ) \u2013 probability of triggering enhancement interpolation ( int , default: INTER_LINEAR ) \u2013 interpolation method\uff08cv2.INTER_NEAREST / INTER_LINEAR / INTER_CUBIC\uff09 border_mode ( int , default: BORDER_REFLECT_101 ) \u2013 boundary fill method\uff08cv2.BORDER_CONSTANT / BORDER_REPLICATE / BORDER_REFLECT_101\uff09 Source code in uchiha\\datasets\\pipelines\\rotation.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def __init__ ( self , angles = None , prob = 0.5 , interpolation = cv2 . INTER_LINEAR , border_mode = cv2 . BORDER_REFLECT_101 ): \"\"\" Args: angles (tuple): rotation angle range for example 30 30 prob (float): probability of triggering enhancement interpolation (int): interpolation method\uff08cv2.INTER_NEAREST / INTER_LINEAR / INTER_CUBIC\uff09 border_mode (int): boundary fill method\uff08cv2.BORDER_CONSTANT / BORDER_REPLICATE / BORDER_REFLECT_101\uff09 \"\"\" if angles is None : angles = [ - 90 , 90 ] self . angles = angles self . p = prob self . interpolation = interpolation self . border_mode = border_mode CustomDataset SoilDataset1d Bases: Dataset the dataset for hyperspectral sequence-data Parameters: data_root ( str ) \u2013 the root directory where the dataset is stored gt_path ( str ) \u2013 path of GT data elements ( str , default: None ) \u2013 Elements to be predicted, a comma-separated string is parsed into a list pipelines ( Sequence [ dict ] , default: None ) \u2013 Data processing flow: a sequence each element is different data processing configuration information Source code in uchiha\\datasets\\soil_pred.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 @DATASET . register_module () class SoilDataset1d ( Dataset ): \"\"\" the dataset for hyperspectral sequence-data Args: data_root (str): the root directory where the dataset is stored gt_path (str): path of GT data elements (str): Elements to be predicted, a comma-separated string is parsed into a list pipelines (Sequence[dict]): Data processing flow: a sequence each element is different data processing configuration information \"\"\" ELEMENTS = [ 'Zn' , 'Subs' ] def __init__ ( self , data_root , gt_path , elements = None , pipelines = None ): self . data_root = data_root self . spectral_data = read_pts ( data_root , is1d = True ) self . gt = read_txt ( gt_path ) self . ELEMENTS = self . get_elements ( elements ) self . pipelines = Compose ( pipelines ) if pipelines else None def __getitem__ ( self , index ): results = dict ( sample = self . spectral_data [ index ], target = self . gt [ index ], index = index ) return self . pipelines ( results ) if self . pipelines else results def __len__ ( self ): return len ( self . spectral_data ) def get_elements ( self , elements ): \"\"\" get the elements to be predicted Gets a list of containing elements based on the provided string separated by commas Args: elements (str): Elements to be predicted Returns: element_names (list): the list containing elements that need to be predicted \"\"\" if elements is None : return self . ELEMENTS if isinstance ( elements , str ): element_names = strings_to_list ( elements ) elif isinstance ( elements , ( tuple , list )): element_names = elements else : raise ValueError ( f 'Unsupported type { type ( elements ) } of elements.' ) return element_names def evaluate ( self , preds , targets = None , metric = 'MAE' ): \"\"\" evaluation of predicted values Args: preds (Tensor): output prediction of the model targets (Tensor): the true value of the predicted element metric (str): evaluation metric. Default: 'MAE' Returns: results (dict): evaluation results for each element \"\"\" elements = self . ELEMENTS if targets is None : targets = self . gt results = regression_eval ( preds , targets , elements , metric ) return results evaluate ( preds , targets = None , metric = 'MAE' ) evaluation of predicted values Parameters: preds ( Tensor ) \u2013 output prediction of the model targets ( Tensor , default: None ) \u2013 the true value of the predicted element metric ( str , default: 'MAE' ) \u2013 evaluation metric. Default: 'MAE' Returns: results ( dict ) \u2013 evaluation results for each element Source code in uchiha\\datasets\\soil_pred.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def evaluate ( self , preds , targets = None , metric = 'MAE' ): \"\"\" evaluation of predicted values Args: preds (Tensor): output prediction of the model targets (Tensor): the true value of the predicted element metric (str): evaluation metric. Default: 'MAE' Returns: results (dict): evaluation results for each element \"\"\" elements = self . ELEMENTS if targets is None : targets = self . gt results = regression_eval ( preds , targets , elements , metric ) return results get_elements ( elements ) get the elements to be predicted Gets a list of containing elements based on the provided string separated by commas Parameters: elements ( str ) \u2013 Elements to be predicted Returns: element_names ( list ) \u2013 the list containing elements that need to be predicted Source code in uchiha\\datasets\\soil_pred.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def get_elements ( self , elements ): \"\"\" get the elements to be predicted Gets a list of containing elements based on the provided string separated by commas Args: elements (str): Elements to be predicted Returns: element_names (list): the list containing elements that need to be predicted \"\"\" if elements is None : return self . ELEMENTS if isinstance ( elements , str ): element_names = strings_to_list ( elements ) elif isinstance ( elements , ( tuple , list )): element_names = elements else : raise ValueError ( f 'Unsupported type { type ( elements ) } of elements.' ) return element_names SoilDataset2d Bases: Dataset the dataset for hyperspectral images Parameters: data_root ( str ) \u2013 the root directory where the dataset is stored gt_path ( str ) \u2013 path of GT data elements ( str , default: None ) \u2013 Elements to be predicted, a comma-separated string is parsed into a list pipelines ( Sequence [ dict ] , default: None ) \u2013 Data processing flow: a sequence each element is different data processing configuration information Source code in uchiha\\datasets\\soil_pred.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @DATASET . register_module () class SoilDataset2d ( Dataset ): \"\"\" the dataset for hyperspectral images Args: data_root (str): the root directory where the dataset is stored gt_path (str): path of GT data elements (str): Elements to be predicted, a comma-separated string is parsed into a list pipelines (Sequence[dict]): Data processing flow: a sequence each element is different data processing configuration information \"\"\" ELEMENTS = [ 'Zn' , 'Others' ] def __init__ ( self , data_root , gt_path , elements = None , pipelines = None ): self . data_root = data_root self . spectral_data = read_npy ( data_root ) self . gt = read_txt ( gt_path ) self . ELEMENTS = self . get_elements ( elements ) # postprocess self . pipelines = Compose ( pipelines ) def __getitem__ ( self , index ): results = dict ( sample = self . spectral_data [ index ], target = self . gt [ index ], index = index ) return self . pipelines ( results ) def __len__ ( self ): return len ( self . spectral_data ) def get_elements ( self , elements ): \"\"\" get the elements to be predicted Gets a list of containing elements based on the provided string separated by commas Args: elements (str): Elements to be predicted Returns: element_names (list): the list containing elements that need to be predicted \"\"\" if elements is None : return self . ELEMENTS if isinstance ( elements , str ): element_names = strings_to_list ( elements ) elif isinstance ( elements , ( tuple , list )): element_names = elements else : raise ValueError ( f 'Unsupported type { type ( elements ) } of elements.' ) return element_names def evaluate ( self , preds , targets = None , metric = 'MAE' , indexes = None ): \"\"\" evaluation of predicted values Args: preds (Tensor): output prediction of the model targets (Tensor): the true value of the predicted element metric (str): evaluation metric. Default: 'MAE' indexes (int): index of original data Returns: results (dict): evaluation results for each element \"\"\" elements = self . ELEMENTS if targets is None : targets = self . gt results = regression_eval ( preds , targets , elements , metric ) return results evaluate ( preds , targets = None , metric = 'MAE' , indexes = None ) evaluation of predicted values Parameters: preds ( Tensor ) \u2013 output prediction of the model targets ( Tensor , default: None ) \u2013 the true value of the predicted element metric ( str , default: 'MAE' ) \u2013 evaluation metric. Default: 'MAE' indexes ( int , default: None ) \u2013 index of original data Returns: results (dict): evaluation results for each element Source code in uchiha\\datasets\\soil_pred.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def evaluate ( self , preds , targets = None , metric = 'MAE' , indexes = None ): \"\"\" evaluation of predicted values Args: preds (Tensor): output prediction of the model targets (Tensor): the true value of the predicted element metric (str): evaluation metric. Default: 'MAE' indexes (int): index of original data Returns: results (dict): evaluation results for each element \"\"\" elements = self . ELEMENTS if targets is None : targets = self . gt results = regression_eval ( preds , targets , elements , metric ) return results get_elements ( elements ) get the elements to be predicted Gets a list of containing elements based on the provided string separated by commas Parameters: elements ( str ) \u2013 Elements to be predicted Returns: element_names ( list ) \u2013 the list containing elements that need to be predicted Source code in uchiha\\datasets\\soil_pred.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def get_elements ( self , elements ): \"\"\" get the elements to be predicted Gets a list of containing elements based on the provided string separated by commas Args: elements (str): Elements to be predicted Returns: element_names (list): the list containing elements that need to be predicted \"\"\" if elements is None : return self . ELEMENTS if isinstance ( elements , str ): element_names = strings_to_list ( elements ) elif isinstance ( elements , ( tuple , list )): element_names = elements else : raise ValueError ( f 'Unsupported type { type ( elements ) } of elements.' ) return element_names compute_metric ( pred , target , metric = 'MAE' ) given metric, calculation result Parameters: pred ( ndarray ) \u2013 Prediction target ( ndarray ) \u2013 GT metric ( str , default: 'MAE' ) \u2013 Evaluation metric. Default: MAE. Returns: ndarray \u2013 Calculation results Source code in uchiha\\datasets\\soil_pred.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def compute_metric ( pred , target , metric = 'MAE' ): \"\"\" given metric, calculation result Args: pred (ndarray): Prediction target (ndarray): GT metric (str): Evaluation metric. Default: MAE. Returns: ndarray: Calculation results \"\"\" if metric == 'MAE' : return mean_absolute_error ( pred , target ) elif metric == 'R2' : return 1 - ( np . sum (( target - pred ) ** 2 , axis = 0 ) / np . sum (( target - np . mean ( target )) ** 2 , axis = 0 )) # return (np.sum((pred - np.mean(target)) ** 2) / np.sum((target - np.mean(target)) ** 2)) else : raise NotImplementedError ( f 'metric: { metric } is not supported yet' ) print_metrics ( result , metric ) print metrics in the console Parameters: result ( dict ) \u2013 Evaluation results for each element . metric ( str ) \u2013 Evaluation metric. Source code in uchiha\\datasets\\soil_pred.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def print_metrics ( result , metric ): \"\"\" print metrics in the console Args: result (dict): Evaluation results for each element . metric (str): Evaluation metric. \"\"\" header = [ 'Element' , f ' { metric } ' ] table_data = [ header ] for key , value in result . items (): row_data = [ key , f ' { value : .6f } ' ] table_data . append ( row_data ) table = AsciiTable ( table_data ) table . inner_footing_row_border = True table . justify_columns [ 1 ] = 'center' logger = get_root_logger () print_log ( ' \\n ' + table . table , logger = logger ) regression_eval ( preds , targets , elements , metric = 'MAE' ) evaluation of regression tasks Parameters: preds ( Tensor ) \u2013 Prediction of the model targets ( Tensor ) \u2013 GT data elements ( List [ str ] ) \u2013 Elements that need to be predicted metric ( str , default: 'MAE' ) \u2013 Evaluation metric. Default: MAE. Returns: dict \u2013 evaluation results for each element Source code in uchiha\\datasets\\soil_pred.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def regression_eval ( preds , targets , elements , metric = 'MAE' ): \"\"\" evaluation of regression tasks Args: preds (Tensor): Prediction of the model targets (Tensor): GT data elements (List[str]): Elements that need to be predicted metric (str): Evaluation metric. Default: MAE. Returns: dict: evaluation results for each element \"\"\" results = { element : 0.0 for element in elements } preds = np . vstack ( tensor2np ( preds )) targets = np . vstack ( tensor2np ( targets )) metrics = compute_metric ( preds , targets , metric ) if len ( results ) == 1 : results [ elements [ 0 ]] = metrics [ 0 ] else : for idx , element in enumerate ( elements ): results [ element ] = metrics [ idx ] print_metrics ( results , metric ) return results tensor2np ( x ) tensor --> ndarray Parameters: x ( Tensor ) \u2013 Input tensor data Returns: ndarray \u2013 converted ndarray data Source code in uchiha\\datasets\\soil_pred.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def tensor2np ( x ): \"\"\" tensor --> ndarray Args: x (Tensor): Input tensor data Returns: ndarray: converted ndarray data \"\"\" if isinstance ( x , list ): for idx , data in enumerate ( x ): x [ idx ] = tensor2np ( data ) if isinstance ( x , torch . Tensor ): x = x . cpu () . numpy () return x HDF5MultiLevelRainHSIDataset Bases: Dataset Multi-level noise HSI data set based on HDF5 storage Parameters: h5_path \u2013 HDF5 file path split_file \u2013 txt path of the divided file pipelines \u2013 data preprocessing pipeline Source code in uchiha\\datasets\\hdr.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 @DATASET . register_module () class HDF5MultiLevelRainHSIDataset ( Dataset ): \"\"\"Multi-level noise HSI data set based on HDF5 storage Args: h5_path: HDF5 file path split_file: txt path of the divided file pipelines: data preprocessing pipeline \"\"\" NOISE_LEVELS = [ 'small' , 'medium' , 'heavy' , 'storm' ] def __init__ ( self , h5_path , split_file , pipelines = None ): self . h5_path = h5_path self . pipelines = Compose ( pipelines ) if pipelines else None # load partition file with open ( split_file , 'r' ) as f : self . sample_names = [ line . strip () for line in f if line . strip ()] # Initialize HDF5 connection (delayed until it is turned on for actual use) self . _h5 = None self . _validate_samples () @property def h5 ( self ): \"\"\"Delayed loading of HDF5 files (multi-process support)\"\"\" if self . _h5 is None : self . _h5 = h5py . File ( self . h5_path , 'r' , libver = 'latest' , swmr = True ) return self . _h5 def _validate_samples ( self ): \"\"\"verify all samples are present in hdf5\"\"\" # no suffix in hdf5 with h5py . File ( self . h5_path , 'r' ) as hf : missing = [] for name in self . sample_names : name = name . replace ( '.npy' , '' ) # verify gt if f 'gt/ { name } ' not in hf : missing . append ( f 'gt/ { name } ' ) # verify each noise level for level in self . NOISE_LEVELS : if f 'rain/ { level } / { name } ' not in hf : missing . append ( f 'rain/ { level } / { name } ' ) if missing : raise KeyError ( f \"Missing { len ( missing ) } samples in HDF5, e.g.: { missing [: 3 ] } \" ) def __len__ ( self ): return len ( self . sample_names ) * len ( self . NOISE_LEVELS ) def __getitem__ ( self , idx ): # Calculate the correspondence between sample name and noise level sample_idx = idx // len ( self . NOISE_LEVELS ) noise_idx = idx % len ( self . NOISE_LEVELS ) sample_name = self . sample_names [ sample_idx ] noise_level = self . NOISE_LEVELS [ noise_idx ] sample_name = sample_name . replace ( '.npy' , '' ) # load data from hdf5 memory mapped gt = self . h5 [ f 'gt/ { sample_name } ' ][:] lq = self . h5 [ f 'rain/ { noise_level } / { sample_name } ' ][:] results = { 'sample' : lq , 'target' : gt , 'index' : idx , 'noise_level' : noise_level } return self . pipelines ( results ) if self . pipelines else results def evaluate ( self , preds : List [ np . ndarray ], targets : List [ np . ndarray ], metric , indexes : List [ int ]) -> dict : \"\"\" Calculates the similarity between the predicted data and the real data. PSNR and SSIM are supported by default. Args: preds: Model output list, each element shape is (B, C, H, W) targets: A list of real labels, each element of shape (B, C, H, W) metric: metric to be calculated. Default value: ['PSNR','SSIM'] indexes: raw data index list Returns: dict: dictionary with average psnr and ssim \"\"\" assert len ( preds ) == len ( targets ) == len ( indexes ), \"input list length must be the same\" assert metric is None or metric . upper () == 'PSNR' or metric . upper () == 'SSIM' , \\ 'only psnr and ssim are supported by default' results = { 'global' : { 'psnr' : [], 'ssim' : []}, 'by_level' : { level : { 'psnr' : [], 'ssim' : []} for level in self . NOISE_LEVELS } } logger = get_root_logger () logger . info ( 'start evaluating...' ) for pred_batch , target_batch , index_batch in zip ( preds , targets , indexes ): B , C , H , W = pred_batch . shape for i in range ( B ): pred = pred_batch [ i ] # (C,H,W) target = target_batch [ i ] idx = index_batch [ i ] # acquire noise level noise_level = self . NOISE_LEVELS [ idx % len ( self . NOISE_LEVELS )] # convert to (H W C) format if C == 1 : pred_img = pred [ 0 ] target_img = target [ 0 ] else : pred_img = np . transpose ( pred , ( 1 , 2 , 0 )) target_img = np . transpose ( target , ( 1 , 2 , 0 )) # calculate psnr data_range = target_img . max () - target_img . min () psnr_val = psnr ( target_img , pred_img , data_range = data_range ) # calculate ssim multichannel = C > 1 ssim_val = ssim ( target_img , pred_img , multichannel = multichannel , channel_axis = 2 if multichannel else None , data_range = data_range ) # store results results [ 'global' ][ 'psnr' ] . append ( psnr_val ) results [ 'global' ][ 'ssim' ] . append ( ssim_val ) results [ 'by_level' ][ noise_level ][ 'psnr' ] . append ( psnr_val ) results [ 'by_level' ][ noise_level ][ 'ssim' ] . append ( ssim_val ) # computational statistics def calc_stats ( values ): return { 'mean' : np . mean ( values ), 'std' : np . std ( values ), 'min' : np . min ( values ), 'max' : np . max ( values ), 'count' : len ( values ) } stats = { 'global' : { 'psnr' : calc_stats ( results [ 'global' ][ 'psnr' ]), 'ssim' : calc_stats ( results [ 'global' ][ 'ssim' ]) }, 'by_level' : { level : { 'psnr' : calc_stats ( results [ 'by_level' ][ level ][ 'psnr' ]), 'ssim' : calc_stats ( results [ 'by_level' ][ level ][ 'ssim' ]) } for level in self . NOISE_LEVELS } } # log output logger . info ( f \" { ' Evaluation Results ' : =^60 } \" ) # Build hierarchical indicator tables table_data = [] for level in self . NOISE_LEVELS : table_data . append ([ level . upper (), f \" { stats [ 'by_level' ][ level ][ 'psnr' ][ 'mean' ] : .2f } \u00b1 { stats [ 'by_level' ][ level ][ 'psnr' ][ 'std' ] : .2f } \" , f \" { stats [ 'by_level' ][ level ][ 'psnr' ][ 'min' ] : .2f } - { stats [ 'by_level' ][ level ][ 'psnr' ][ 'max' ] : .2f } \" , f \" { stats [ 'by_level' ][ level ][ 'ssim' ][ 'mean' ] : .4f } \u00b1 { stats [ 'by_level' ][ level ][ 'ssim' ][ 'std' ] : .4f } \" , f \" { stats [ 'by_level' ][ level ][ 'ssim' ][ 'min' ] : .4f } - { stats [ 'by_level' ][ level ][ 'ssim' ][ 'max' ] : .4f } \" , stats [ 'by_level' ][ level ][ 'psnr' ][ 'count' ] ]) table_data . append ([ 'TOTAL' , f \" { stats [ 'global' ][ 'psnr' ][ 'mean' ] : .2f } \u00b1 { stats [ 'global' ][ 'psnr' ][ 'std' ] : .2f } \" , f \" { stats [ 'global' ][ 'psnr' ][ 'min' ] : .2f } - { stats [ 'global' ][ 'psnr' ][ 'max' ] : .2f } \" , f \" { stats [ 'global' ][ 'ssim' ][ 'mean' ] : .4f } \u00b1 { stats [ 'global' ][ 'ssim' ][ 'std' ] : .4f } \" , f \" { stats [ 'global' ][ 'ssim' ][ 'min' ] : .4f } - { stats [ 'global' ][ 'ssim' ][ 'max' ] : .4f } \" , stats [ 'global' ][ 'psnr' ][ 'count' ] ]) # log logger . info ( \" \\n \" + tabulate ( table_data , headers = [ \"Noise Level\" , \"PSNR (dB)\" , \"PSNR Range\" , \"SSIM\" , \"SSIM Range\" , \"Samples\" ], tablefmt = \"fancy_grid\" , floatfmt = ( \"\" , \".2f\" , \"\" , \".4f\" , \"\" , \"d\" ) )) logger . info ( f \"Mean PSNR: { stats [ 'global' ][ 'psnr' ][ 'mean' ] : .2f } dB\" ) logger . info ( f \"Mean SSIM: { stats [ 'global' ][ 'ssim' ][ 'mean' ] : .4f } \" ) logger . info ( \"=\" * 60 ) return { 'psnr' : stats [ 'global' ][ 'psnr' ][ 'mean' ], 'ssim' : stats [ 'global' ][ 'ssim' ][ 'mean' ] } def get_filename ( self , idx ): \"\"\"Get the original file name (without noise level information)\"\"\" sample_idx = idx // len ( self . NOISE_LEVELS ) return self . sample_names [ sample_idx ] def __del__ ( self ): \"\"\"ensure hdf5 connection is closed\"\"\" if self . _h5 is not None : self . _h5 . close () h5 property Delayed loading of HDF5 files (multi-process support) __del__ () ensure hdf5 connection is closed Source code in uchiha\\datasets\\hdr.py 228 229 230 231 def __del__ ( self ): \"\"\"ensure hdf5 connection is closed\"\"\" if self . _h5 is not None : self . _h5 . close () evaluate ( preds , targets , metric , indexes ) Calculates the similarity between the predicted data and the real data. PSNR and SSIM are supported by default. Parameters: preds ( List [ ndarray ] ) \u2013 Model output list, each element shape is (B, C, H, W) targets ( List [ ndarray ] ) \u2013 A list of real labels, each element of shape (B, C, H, W) metric \u2013 metric to be calculated. Default value: ['PSNR','SSIM'] indexes ( List [ int ] ) \u2013 raw data index list Returns: dict ( dict ) \u2013 dictionary with average psnr and ssim Source code in uchiha\\datasets\\hdr.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def evaluate ( self , preds : List [ np . ndarray ], targets : List [ np . ndarray ], metric , indexes : List [ int ]) -> dict : \"\"\" Calculates the similarity between the predicted data and the real data. PSNR and SSIM are supported by default. Args: preds: Model output list, each element shape is (B, C, H, W) targets: A list of real labels, each element of shape (B, C, H, W) metric: metric to be calculated. Default value: ['PSNR','SSIM'] indexes: raw data index list Returns: dict: dictionary with average psnr and ssim \"\"\" assert len ( preds ) == len ( targets ) == len ( indexes ), \"input list length must be the same\" assert metric is None or metric . upper () == 'PSNR' or metric . upper () == 'SSIM' , \\ 'only psnr and ssim are supported by default' results = { 'global' : { 'psnr' : [], 'ssim' : []}, 'by_level' : { level : { 'psnr' : [], 'ssim' : []} for level in self . NOISE_LEVELS } } logger = get_root_logger () logger . info ( 'start evaluating...' ) for pred_batch , target_batch , index_batch in zip ( preds , targets , indexes ): B , C , H , W = pred_batch . shape for i in range ( B ): pred = pred_batch [ i ] # (C,H,W) target = target_batch [ i ] idx = index_batch [ i ] # acquire noise level noise_level = self . NOISE_LEVELS [ idx % len ( self . NOISE_LEVELS )] # convert to (H W C) format if C == 1 : pred_img = pred [ 0 ] target_img = target [ 0 ] else : pred_img = np . transpose ( pred , ( 1 , 2 , 0 )) target_img = np . transpose ( target , ( 1 , 2 , 0 )) # calculate psnr data_range = target_img . max () - target_img . min () psnr_val = psnr ( target_img , pred_img , data_range = data_range ) # calculate ssim multichannel = C > 1 ssim_val = ssim ( target_img , pred_img , multichannel = multichannel , channel_axis = 2 if multichannel else None , data_range = data_range ) # store results results [ 'global' ][ 'psnr' ] . append ( psnr_val ) results [ 'global' ][ 'ssim' ] . append ( ssim_val ) results [ 'by_level' ][ noise_level ][ 'psnr' ] . append ( psnr_val ) results [ 'by_level' ][ noise_level ][ 'ssim' ] . append ( ssim_val ) # computational statistics def calc_stats ( values ): return { 'mean' : np . mean ( values ), 'std' : np . std ( values ), 'min' : np . min ( values ), 'max' : np . max ( values ), 'count' : len ( values ) } stats = { 'global' : { 'psnr' : calc_stats ( results [ 'global' ][ 'psnr' ]), 'ssim' : calc_stats ( results [ 'global' ][ 'ssim' ]) }, 'by_level' : { level : { 'psnr' : calc_stats ( results [ 'by_level' ][ level ][ 'psnr' ]), 'ssim' : calc_stats ( results [ 'by_level' ][ level ][ 'ssim' ]) } for level in self . NOISE_LEVELS } } # log output logger . info ( f \" { ' Evaluation Results ' : =^60 } \" ) # Build hierarchical indicator tables table_data = [] for level in self . NOISE_LEVELS : table_data . append ([ level . upper (), f \" { stats [ 'by_level' ][ level ][ 'psnr' ][ 'mean' ] : .2f } \u00b1 { stats [ 'by_level' ][ level ][ 'psnr' ][ 'std' ] : .2f } \" , f \" { stats [ 'by_level' ][ level ][ 'psnr' ][ 'min' ] : .2f } - { stats [ 'by_level' ][ level ][ 'psnr' ][ 'max' ] : .2f } \" , f \" { stats [ 'by_level' ][ level ][ 'ssim' ][ 'mean' ] : .4f } \u00b1 { stats [ 'by_level' ][ level ][ 'ssim' ][ 'std' ] : .4f } \" , f \" { stats [ 'by_level' ][ level ][ 'ssim' ][ 'min' ] : .4f } - { stats [ 'by_level' ][ level ][ 'ssim' ][ 'max' ] : .4f } \" , stats [ 'by_level' ][ level ][ 'psnr' ][ 'count' ] ]) table_data . append ([ 'TOTAL' , f \" { stats [ 'global' ][ 'psnr' ][ 'mean' ] : .2f } \u00b1 { stats [ 'global' ][ 'psnr' ][ 'std' ] : .2f } \" , f \" { stats [ 'global' ][ 'psnr' ][ 'min' ] : .2f } - { stats [ 'global' ][ 'psnr' ][ 'max' ] : .2f } \" , f \" { stats [ 'global' ][ 'ssim' ][ 'mean' ] : .4f } \u00b1 { stats [ 'global' ][ 'ssim' ][ 'std' ] : .4f } \" , f \" { stats [ 'global' ][ 'ssim' ][ 'min' ] : .4f } - { stats [ 'global' ][ 'ssim' ][ 'max' ] : .4f } \" , stats [ 'global' ][ 'psnr' ][ 'count' ] ]) # log logger . info ( \" \\n \" + tabulate ( table_data , headers = [ \"Noise Level\" , \"PSNR (dB)\" , \"PSNR Range\" , \"SSIM\" , \"SSIM Range\" , \"Samples\" ], tablefmt = \"fancy_grid\" , floatfmt = ( \"\" , \".2f\" , \"\" , \".4f\" , \"\" , \"d\" ) )) logger . info ( f \"Mean PSNR: { stats [ 'global' ][ 'psnr' ][ 'mean' ] : .2f } dB\" ) logger . info ( f \"Mean SSIM: { stats [ 'global' ][ 'ssim' ][ 'mean' ] : .4f } \" ) logger . info ( \"=\" * 60 ) return { 'psnr' : stats [ 'global' ][ 'psnr' ][ 'mean' ], 'ssim' : stats [ 'global' ][ 'ssim' ][ 'mean' ] } get_filename ( idx ) Get the original file name (without noise level information) Source code in uchiha\\datasets\\hdr.py 223 224 225 226 def get_filename ( self , idx ): \"\"\"Get the original file name (without noise level information)\"\"\" sample_idx = idx // len ( self . NOISE_LEVELS ) return self . sample_names [ sample_idx ] MultiLevelRainHSIDatasetV0 Bases: Dataset Source code in uchiha\\datasets\\hdr.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 @DeprecationWarning @DATASET . register_module () class MultiLevelRainHSIDatasetV0 ( Dataset ): RAIN_TYPES = [ 'small' , 'medium' , 'heavy' , 'storm' ] # Four noise types are fixed def __init__ ( self , data_root : str , pipelines : Dict = None ): \"\"\" Args: data_root: The root directory of the dataset \"\"\" self . root = data_root self . pipelines = Compose ( pipelines ) if pipelines else None # Check the directory structure self . _validate_directory_structure () # Load all sample paths self . gt_paths = sorted ( glob . glob ( os . path . join ( data_root , 'gt' , '*.npy' ))) if not self . gt_paths : raise FileNotFoundError ( f \"No .npy files found in { os . path . join ( data_root , 'gt' ) } \" ) self . paths = self . _build_paths_index () def _validate_directory_structure ( self ): \"\"\"Verify that the directory structure is as expected\"\"\" required_dirs = [ 'gt' ] + [ os . path . join ( 'rain' , t ) for t in self . RAIN_TYPES ] for d in required_dirs : if not os . path . isdir ( os . path . join ( self . root , d )): raise RuntimeError ( f \"Missing required directory: { d } \" ) def _build_paths_index ( self ) -> List [ Dict [ str , str ]]: \"\"\"Build a list of sample index dictionaries\"\"\" paths = [] for gt_path in self . gt_paths : sample = { 'gt_path' : gt_path } # Obtain the corresponding noise file path filename , ext = os . path . basename ( gt_path ) . split ( '.' ) for rain_type in self . RAIN_TYPES : rain_path = os . path . join ( self . root , 'rain' , rain_type , f ' { filename } _ { rain_type } . { ext } ' ) if not os . path . exists ( rain_path ): raise FileNotFoundError ( f \"Missing corresponding rain file: { rain_path } \" ) sample [ 'lq_path' ] = rain_path paths . append ( sample . copy ()) return paths def __len__ ( self ) -> int : return len ( self . paths ) def __getitem__ ( self , idx : int ) -> Dict : sample = self . paths [ idx ] # \u52a0\u8f7d\u6570\u636e gt_path = sample [ 'gt_path' ] lq_path = sample [ 'lq_path' ] gt = np . load ( gt_path ) lq = np . load ( lq_path ) results = { 'sample' : lq , 'target' : gt , 'index' : idx } return self . pipelines ( results ) if self . pipelines else results def get_rain_types ( self ) -> List [ str ]: \"\"\"\u83b7\u53d6\u652f\u6301\u7684\u566a\u58f0\u7c7b\u578b\u5217\u8868\"\"\" return self . RAIN_TYPES . copy () __init__ ( data_root , pipelines = None ) Parameters: data_root ( str ) \u2013 The root directory of the dataset Source code in uchiha\\datasets\\hdr.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def __init__ ( self , data_root : str , pipelines : Dict = None ): \"\"\" Args: data_root: The root directory of the dataset \"\"\" self . root = data_root self . pipelines = Compose ( pipelines ) if pipelines else None # Check the directory structure self . _validate_directory_structure () # Load all sample paths self . gt_paths = sorted ( glob . glob ( os . path . join ( data_root , 'gt' , '*.npy' ))) if not self . gt_paths : raise FileNotFoundError ( f \"No .npy files found in { os . path . join ( data_root , 'gt' ) } \" ) self . paths = self . _build_paths_index () get_rain_types () \u83b7\u53d6\u652f\u6301\u7684\u566a\u58f0\u7c7b\u578b\u5217\u8868 Source code in uchiha\\datasets\\hdr.py 303 304 305 def get_rain_types ( self ) -> List [ str ]: \"\"\"\u83b7\u53d6\u652f\u6301\u7684\u566a\u58f0\u7c7b\u578b\u5217\u8868\"\"\" return self . RAIN_TYPES . copy () HSIDehazeDataset Bases: Dataset Multi-level noise HSI data set based on HDF5 storage Parameters: gt_path \u2013 gt data directory lq_path \u2013 lq data directory loader_type \u2013 type of loader dataset_name \u2013 name of the dataset pipelines \u2013 data preprocessing pipeline Source code in uchiha\\datasets\\hsi_dehaze.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 @DATASET . register_module () class HSIDehazeDataset ( Dataset ): \"\"\"Multi-level noise HSI data set based on HDF5 storage Args: gt_path: gt data directory lq_path: lq data directory loader_type: type of loader dataset_name: name of the dataset pipelines: data preprocessing pipeline \"\"\" def __init__ ( self , gt_path = None , lq_path = None , loader_type = 'tiff' , dataset_name = 'HD' , pipelines = None ): super ( HSIDehazeDataset , self ) . __init__ () self . loader = partial ( get_loader , loader_type ) self . gt_path = glob . glob ( gt_path ) self . lq_path = glob . glob ( lq_path ) self . dataset_name = dataset_name self . pipelines = Compose ( pipelines ) if pipelines else None def __getitem__ ( self , index ): if self . dataset_name == 'HD' : lq , gt = load_hd ( self . loader , self . lq_path [ index ]) elif self . dataset_name == 'AVIRIS' : lq , gt = load_mat ( self . loader , self . gt_path [ index ], self . lq_path [ index ]) elif self . dataset_name == 'UAV' : lq , gt = load_mat ( self . loader , self . gt_path [ index ], self . lq_path [ index ]) else : raise ValueError ( 'Invalid dataset name' ) results = { 'sample' : lq , 'target' : gt , 'index' : index , } return self . pipelines ( results ) if self . pipelines else results def __len__ ( self ): return len ( self . lq_path ) def evaluate ( self , preds : List [ np . ndarray ], targets : List [ np . ndarray ], metric , indexes : List [ int ]) -> dict : \"\"\" Calculates the similarity between the predicted data and the real data. PSNR and SSIM are supported by default. Args: preds: Model output list, each element shape is (B, C, H, W) targets: A list of real labels, each element of shape (B, C, H, W) metric: metric to be calculated. Default value: ['PSNR','SSIM'] indexes: raw data index list Returns: dict: dictionary with average psnr and ssim \"\"\" assert len ( preds ) == len ( targets ) == len ( indexes ), \"input list length must be the same\" assert metric is None or metric . upper () == 'PSNR' or metric . upper () == 'SSIM' , \\ 'only psnr and ssim are supported by default' logger = get_root_logger () logger . info ( 'start evaluating...' ) psnrs = [] ssims = [] for pred_batch , target_batch , index_batch in zip ( preds , targets , indexes ): B , C , H , W = pred_batch . shape for i in range ( B ): pred = pred_batch [ i ] # (C,H,W) target = target_batch [ i ] #convert to (H W C) format if C == 1 : pred_img = pred [ 0 ] target_img = target [ 0 ] else : pred_img = np . transpose ( pred , ( 1 , 2 , 0 )) target_img = np . transpose ( target , ( 1 , 2 , 0 )) # calculate psnr data_range = target_img . max () - target_img . min () psnr_val = psnr ( target_img , pred_img , data_range = data_range ) # calculate ssim multichannel = C > 1 ssim_val = ssim ( target_img , pred_img , multichannel = multichannel , channel_axis = 2 if multichannel else None , data_range = data_range ) psnrs . append ( psnr_val ) ssims . append ( ssim_val ) mean_psnr = np . mean ( psnrs ) mean_ssim = np . mean ( ssims ) logger . info ( f \"Mean PSNR: { mean_psnr : .2f } dB\" ) logger . info ( f \"Mean SSIM: { mean_ssim : .4f } \" ) return { 'psnr' : mean_psnr , 'ssim' : mean_ssim } evaluate ( preds , targets , metric , indexes ) Calculates the similarity between the predicted data and the real data. PSNR and SSIM are supported by default. Parameters: preds ( List [ ndarray ] ) \u2013 Model output list, each element shape is (B, C, H, W) targets ( List [ ndarray ] ) \u2013 A list of real labels, each element of shape (B, C, H, W) metric \u2013 metric to be calculated. Default value: ['PSNR','SSIM'] indexes ( List [ int ] ) \u2013 raw data index list Returns: dict ( dict ) \u2013 dictionary with average psnr and ssim Source code in uchiha\\datasets\\hsi_dehaze.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def evaluate ( self , preds : List [ np . ndarray ], targets : List [ np . ndarray ], metric , indexes : List [ int ]) -> dict : \"\"\" Calculates the similarity between the predicted data and the real data. PSNR and SSIM are supported by default. Args: preds: Model output list, each element shape is (B, C, H, W) targets: A list of real labels, each element of shape (B, C, H, W) metric: metric to be calculated. Default value: ['PSNR','SSIM'] indexes: raw data index list Returns: dict: dictionary with average psnr and ssim \"\"\" assert len ( preds ) == len ( targets ) == len ( indexes ), \"input list length must be the same\" assert metric is None or metric . upper () == 'PSNR' or metric . upper () == 'SSIM' , \\ 'only psnr and ssim are supported by default' logger = get_root_logger () logger . info ( 'start evaluating...' ) psnrs = [] ssims = [] for pred_batch , target_batch , index_batch in zip ( preds , targets , indexes ): B , C , H , W = pred_batch . shape for i in range ( B ): pred = pred_batch [ i ] # (C,H,W) target = target_batch [ i ] #convert to (H W C) format if C == 1 : pred_img = pred [ 0 ] target_img = target [ 0 ] else : pred_img = np . transpose ( pred , ( 1 , 2 , 0 )) target_img = np . transpose ( target , ( 1 , 2 , 0 )) # calculate psnr data_range = target_img . max () - target_img . min () psnr_val = psnr ( target_img , pred_img , data_range = data_range ) # calculate ssim multichannel = C > 1 ssim_val = ssim ( target_img , pred_img , multichannel = multichannel , channel_axis = 2 if multichannel else None , data_range = data_range ) psnrs . append ( psnr_val ) ssims . append ( ssim_val ) mean_psnr = np . mean ( psnrs ) mean_ssim = np . mean ( ssims ) logger . info ( f \"Mean PSNR: { mean_psnr : .2f } dB\" ) logger . info ( f \"Mean SSIM: { mean_ssim : .4f } \" ) return { 'psnr' : mean_psnr , 'ssim' : mean_ssim }","title":"Dataset"},{"location":"mkdocs/dataset.html#datasets","text":"","title":"Datasets"},{"location":"mkdocs/dataset.html#pipelines","text":"","title":"Pipelines"},{"location":"mkdocs/dataset.html#datasets.pipelines.compose.Compose","text":"Compose multiple transforms sequentially. Parameters: transforms ( Sequence [ dict | callable ] ) \u2013 Sequence of transform object or config dict to be composed. Source code in uchiha\\datasets\\pipelines\\compose.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @PIPELINES . register_module () class Compose : \"\"\" Compose multiple transforms sequentially. Args: transforms (Sequence[dict | callable]): Sequence of transform object or config dict to be composed. \"\"\" def __init__ ( self , transforms ): # assert isinstance(transforms, collections.abc.Sequence) self . transforms = [] for transform in transforms : if isinstance ( transform , dict ): transform = build_from_cfg ( transform , PIPELINES ) self . transforms . append ( transform ) elif callable ( transform ): self . transforms . append ( transform ) else : raise TypeError ( 'transform must be callable or a dict' ) def __call__ ( self , data ): \"\"\"Call function to apply transforms sequentially. Args: data (dict): A result dict contains the data to transform. Returns: data (dict): Transformed data. \"\"\" for t in self . transforms : data = t ( data ) if data is None : return None return data def __repr__ ( self ): format_string = self . __class__ . __name__ + '(' for t in self . transforms : str_ = t . __repr__ () if 'Compose(' in str_ : str_ = str_ . replace ( ' \\n ' , ' \\n ' ) format_string += ' \\n ' format_string += f ' { str_ } ' format_string += ' \\n )' return format_string","title":"Compose"},{"location":"mkdocs/dataset.html#datasets.pipelines.compose.Compose.__call__","text":"Call function to apply transforms sequentially. Parameters: data ( dict ) \u2013 A result dict contains the data to transform. Returns: data ( dict ) \u2013 Transformed data. Source code in uchiha\\datasets\\pipelines\\compose.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __call__ ( self , data ): \"\"\"Call function to apply transforms sequentially. Args: data (dict): A result dict contains the data to transform. Returns: data (dict): Transformed data. \"\"\" for t in self . transforms : data = t ( data ) if data is None : return None return data","title":"__call__"},{"location":"mkdocs/dataset.html#datasets.pipelines.crop.CenterCrop","text":"Source code in uchiha\\datasets\\pipelines\\crop.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @PIPELINES . register_module () class CenterCrop : def __init__ ( self , size ): \"\"\" Args: size (tuple): the target size of the crop height width \"\"\" self . size = size def __call__ ( self , data ): sample = data [ 'sample' ] target = data [ 'target' ] h , w = sample . shape [: 2 ] th , tw = self . size # If the image is smaller than the crop size, resize to the crop size first if h < th or w < tw : import cv2 sample = cv2 . resize ( sample , ( tw , th ), interpolation = cv2 . INTER_LINEAR ) target = cv2 . resize ( target , ( tw , th ), interpolation = cv2 . INTER_NEAREST ) data [ 'sample' ] = sample data [ 'target' ] = target return data # Calculate the starting coordinates of the center clipping i = ( h - th ) // 2 j = ( w - tw ) // 2 # crop if len ( sample . shape ) == 3 : sample_cropped = sample [ i : i + th , j : j + tw , :] else : sample_cropped = sample [ i : i + th , j : j + tw ] if len ( target . shape ) == 3 : target_cropped = target [ i : i + th , j : j + tw , :] else : target_cropped = target [ i : i + th , j : j + tw ] data [ 'sample' ] = sample_cropped data [ 'target' ] = target_cropped return data","title":"CenterCrop"},{"location":"mkdocs/dataset.html#datasets.pipelines.crop.CenterCrop.__init__","text":"Parameters: size ( tuple ) \u2013 the target size of the crop height width Source code in uchiha\\datasets\\pipelines\\crop.py 60 61 62 63 64 65 def __init__ ( self , size ): \"\"\" Args: size (tuple): the target size of the crop height width \"\"\" self . size = size","title":"__init__"},{"location":"mkdocs/dataset.html#datasets.pipelines.crop.RandomCrop","text":"Source code in uchiha\\datasets\\pipelines\\crop.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @PIPELINES . register_module () class RandomCrop : def __init__ ( self , crop_size , prob = 0.5 ): \"\"\" Args: crop_size (tuple): the target size of the crop height width prob (float): probability of triggering enhancement \"\"\" if isinstance ( crop_size , int ): crop_size = [ crop_size , crop_size ] self . size = crop_size self . p = prob def __call__ ( self , data ): if random . random () < self . p : sample = data [ 'sample' ] target = data [ 'target' ] h , w = sample . shape [: 2 ] th , tw = self . size # If the image is smaller than the crop size, resize to the crop size first if h < th or w < tw : sample = cv2 . resize ( sample , ( tw , th ), interpolation = cv2 . INTER_LINEAR ) target = cv2 . resize ( target , ( tw , th ), interpolation = cv2 . INTER_NEAREST ) data [ 'sample' ] = sample data [ 'target' ] = target return data # randomly select crop start point i = random . randint ( 0 , h - th ) j = random . randint ( 0 , w - tw ) # crop image if len ( sample . shape ) == 3 : sample_cropped = sample [ i : i + th , j : j + tw , :] else : sample_cropped = sample [ i : i + th , j : j + tw ] if len ( target . shape ) == 3 : target_cropped = target [ i : i + th , j : j + tw , :] else : target_cropped = target [ i : i + th , j : j + tw ] data [ 'sample' ] = sample_cropped data [ 'target' ] = target_cropped return data","title":"RandomCrop"},{"location":"mkdocs/dataset.html#datasets.pipelines.crop.RandomCrop.__init__","text":"Parameters: crop_size ( tuple ) \u2013 the target size of the crop height width prob ( float , default: 0.5 ) \u2013 probability of triggering enhancement Source code in uchiha\\datasets\\pipelines\\crop.py 10 11 12 13 14 15 16 17 18 19 def __init__ ( self , crop_size , prob = 0.5 ): \"\"\" Args: crop_size (tuple): the target size of the crop height width prob (float): probability of triggering enhancement \"\"\" if isinstance ( crop_size , int ): crop_size = [ crop_size , crop_size ] self . size = crop_size self . p = prob","title":"__init__"},{"location":"mkdocs/dataset.html#datasets.pipelines.flip.RandomFlip","text":"Source code in uchiha\\datasets\\pipelines\\flip.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @PIPELINES . register_module () class RandomFlip : def __init__ ( self , probs = None , directions = None ): \"\"\" Args: probs (list): The probability weights of the four flips, in the order: [Original probability, horizontal flip probability, vertical flip probability, diagonal flip probability] \"\"\" if directions is None : directions = [ 'original' , 'horizontal' , 'vertical' , 'diagonal' ] if probs is None : probs = [ 0.5 , 0.25 , 0.25 , 0.0 ] self . p = probs # normalized probability total = sum ( probs ) self . norm_p = [ x / total for x in probs ] # building a choice space self . directions = directions # Cumulative probability is used for random selection cum = 0.0 self . cum_probs = [] for probs in self . norm_p : cum += probs self . cum_probs . append ( cum ) def _apply_flip ( self , img , flip_type ): \"\"\"perform specific flip operations\"\"\" if flip_type == 'original' : return img elif flip_type == 'horizontal' : return img [:, :: - 1 , ... ] . copy () # HWC / HW elif flip_type == 'vertical' : return img [:: - 1 , :, ... ] . copy () elif flip_type == 'diagonal' : return img [:, :: - 1 , ... ][:: - 1 , :, ... ] . copy () else : raise ValueError ( f \"Unknown flip type: { flip_type } \" ) def __call__ ( self , data ): sample = data [ 'sample' ] target = data [ 'target' ] # use cumulative probability to select flip type rand_val = random . random () selected = self . directions [ - 1 ] for i , cp in enumerate ( self . cum_probs ): if rand_val < cp : selected = self . directions [ i ] break # flip sample = self . _apply_flip ( sample , selected ) target = self . _apply_flip ( target , selected ) data [ 'sample' ] = sample data [ 'target' ] = target return data","title":"RandomFlip"},{"location":"mkdocs/dataset.html#datasets.pipelines.flip.RandomFlip.__init__","text":"Args: probs (list): The probability weights of the four flips, in the order: [Original probability, horizontal flip probability, vertical flip probability, diagonal flip probability] Source code in uchiha\\datasets\\pipelines\\flip.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , probs = None , directions = None ): \"\"\" Args: probs (list): The probability weights of the four flips, in the order: [Original probability, horizontal flip probability, vertical flip probability, diagonal flip probability] \"\"\" if directions is None : directions = [ 'original' , 'horizontal' , 'vertical' , 'diagonal' ] if probs is None : probs = [ 0.5 , 0.25 , 0.25 , 0.0 ] self . p = probs # normalized probability total = sum ( probs ) self . norm_p = [ x / total for x in probs ] # building a choice space self . directions = directions # Cumulative probability is used for random selection cum = 0.0 self . cum_probs = [] for probs in self . norm_p : cum += probs self . cum_probs . append ( cum )","title":"__init__"},{"location":"mkdocs/dataset.html#datasets.pipelines.formatting.EasyToTensor","text":"convert data to tensor The default input data is in B H W C form, converting it to B C H W form Parameters: mode ( str , default: 'CHW' ) \u2013 data order form Source code in uchiha\\datasets\\pipelines\\formatting.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @PIPELINES . register_module () class EasyToTensor : \"\"\" convert data to tensor The default input data is in `B H W C` form, converting it to `B C H W` form Args: mode (str): data order form \"\"\" def __init__ ( self , mode = 'CHW' ): self . mode = mode def __call__ ( self , results ): \"\"\"Call function to convert the type of the data. Args: results (dict): Result dict from pipeline. Returns: results (dict): converted results \"\"\" assert 'sample' in results and 'target' in results , 'sample and target should be loaded to results' if self . mode == 'CHW' : results [ 'sample' ] = torch . Tensor ( results [ 'sample' ]) . permute ( 2 , 0 , 1 ) results [ 'target' ] = torch . Tensor ( results [ 'target' ]) . permute ( 2 , 0 , 1 ) else : results [ 'sample' ] = torch . Tensor ( results [ 'sample' ]) results [ 'target' ] = torch . Tensor ( results [ 'target' ]) return results","title":"EasyToTensor"},{"location":"mkdocs/dataset.html#datasets.pipelines.formatting.EasyToTensor.__call__","text":"Call function to convert the type of the data. Parameters: results ( dict ) \u2013 Result dict from pipeline. Returns: results ( dict ) \u2013 converted results Source code in uchiha\\datasets\\pipelines\\formatting.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def __call__ ( self , results ): \"\"\"Call function to convert the type of the data. Args: results (dict): Result dict from pipeline. Returns: results (dict): converted results \"\"\" assert 'sample' in results and 'target' in results , 'sample and target should be loaded to results' if self . mode == 'CHW' : results [ 'sample' ] = torch . Tensor ( results [ 'sample' ]) . permute ( 2 , 0 , 1 ) results [ 'target' ] = torch . Tensor ( results [ 'target' ]) . permute ( 2 , 0 , 1 ) else : results [ 'sample' ] = torch . Tensor ( results [ 'sample' ]) results [ 'target' ] = torch . Tensor ( results [ 'target' ]) return results","title":"__call__"},{"location":"mkdocs/dataset.html#datasets.pipelines.geometric.img_normalize","text":"Normalize an image with mean and std. Parameters: img ( ndarray ) \u2013 Image to be normalized. mean ( ndarray ) \u2013 The mean to be used for normalize. std ( ndarray ) \u2013 The std to be used for normalize. to_rgb ( bool , default: True ) \u2013 Whether to convert to rgb. scope ( str , default: 'spatial' ) \u2013 Normalized scope. Returns: ndarray \u2013 The normalized image. Source code in uchiha\\datasets\\pipelines\\geometric.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def img_normalize ( img , mean , std , to_rgb = True , scope = 'spatial' , mode = 'standard' ): \"\"\"Normalize an image with mean and std. Args: img (ndarray): Image to be normalized. mean (ndarray): The mean to be used for normalize. std (ndarray): The std to be used for normalize. to_rgb (bool): Whether to convert to rgb. scope (str): Normalized scope. Returns: ndarray: The normalized image. \"\"\" img = img . copy () . astype ( np . float32 ) return img_normalize_ ( img , mean , std , to_rgb , scope , mode )","title":"img_normalize"},{"location":"mkdocs/dataset.html#datasets.pipelines.geometric.img_normalize_","text":"Inplace normalize an image with mean and std. Parameters: img ( ndarray ) \u2013 Image to be normalized. mean ( ndarray ) \u2013 The mean to be used for normalize. std ( ndarray ) \u2013 The std to be used for normalize. to_rgb ( bool , default: True ) \u2013 Whether to convert to rgb. scope ( str , default: 'spatial' ) \u2013 Normalized scope. Returns: ndarray \u2013 The normalized image. Source code in uchiha\\datasets\\pipelines\\geometric.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def img_normalize_ ( img , mean , std , to_rgb = True , scope = 'spatial' , mode = 'standard' ): \"\"\"Inplace normalize an image with mean and std. Args: img (ndarray): Image to be normalized. mean (ndarray): The mean to be used for normalize. std (ndarray): The std to be used for normalize. to_rgb (bool): Whether to convert to rgb. scope (str): Normalized scope. Returns: ndarray: The normalized image. \"\"\" # cv2 inplace normalization does not accept uint8 assert img . dtype != np . uint8 channel = img . shape [ - 1 ] mean = np . float64 ( mean . reshape ( 1 , - 1 )) if mean else None stdinv = 1 / np . float64 ( std . reshape ( 1 , - 1 )) if std else None if to_rgb and channel == 3 : cv2 . cvtColor ( img , cv2 . COLOR_BGR2RGB , img ) # inplace if scope == 'spatial' : if not mean : mean = np . float64 ( np . mean ( img , axis = ( 0 , 1 )) . reshape ( 1 , - 1 )) if not std : std = np . float64 ( np . std ( img , axis = ( 0 , 1 )) . reshape ( 1 , - 1 )) mask = std != 0 stdinv = np . zeros_like ( std ) stdinv [ mask ] = 1 / std [ mask ] _min = np . float64 ( np . min ( img , axis = ( 0 , 1 )) . reshape ( 1 , - 1 )) _max = np . float64 ( np . max ( img , axis = ( 0 , 1 )) . reshape ( 1 , - 1 )) else : if not mean : mean = np . float32 ( np . mean ( img , axis = 2 ))[:, :, None ] mean = np . tile ( mean , ( 1 , 1 , channel )) if not std : std = np . float32 ( np . std ( img , axis = 2 ))[:, :, None ] std = np . tile ( std , ( 1 , 1 , channel )) stdinv = 1 / std _min = np . tile ( np . float32 ( np . min ( img , axis = 2 ))[:, :, None ], ( 1 , 1 , channel )) _max = np . tile ( np . float32 ( np . max ( img , axis = 2 ))[:, :, None ], ( 1 , 1 , channel )) if mode == 'standard' : cv2 . subtract ( img , mean , img ) # inplace cv2 . multiply ( img , stdinv , img ) # inplace elif mode == 'minmax' : cv2 . subtract ( img , _min , img ) # inplace cv2 . multiply ( img , 1 / ( _max - _min ), img ) # inplace else : raise NotImplementedError ( f \"normalize mode: { mode } is not supported yet\" ) return img","title":"img_normalize_"},{"location":"mkdocs/dataset.html#datasets.pipelines.geometric.impad","text":"Pad the given image to a certain shape or pad on all sides with specified padding mode and padding value. Parameters: img ( ndarray ) \u2013 Image to be padded. shape ( tuple [ int ] , default: None ) \u2013 Expected padding shape (h, w). Default: None. padding ( int or tuple [ int ] , default: None ) \u2013 Padding on each border. If a single int is provided this is used to pad all borders. If tuple of length 2 is provided this is the padding on top/bottom and left/right respectively. If a tuple of length 4 is provided this is the padding for the top, bottom, left and right borders respectively. Default: None. Note that shape and padding can not be both set. pad_val ( Number | Sequence [ Number ] , default: 0 ) \u2013 Values to be filled in padding areas when padding_mode is 'constant'. Default: 0. padding_mode ( str , default: 'constant' ) \u2013 Type of padding. Should be: constant, edge, reflect or symmetric. Default: constant. - constant: pads with a constant value, this value is specified with pad_val. - edge: pads with the last value at the edge of the image. - reflect: pads with reflection of image without repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]. - symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] Returns: ndarray ( ndarray ) \u2013 The padded image. Source code in uchiha\\datasets\\pipelines\\geometric.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def impad ( img : np . ndarray , * , shape : Optional [ Tuple [ int , int ]] = None , padding : Union [ int , tuple , None ] = None , pad_val : Union [ float , List ] = 0 , padding_mode : str = 'constant' ) -> np . ndarray : \"\"\"Pad the given image to a certain shape or pad on all sides with specified padding mode and padding value. Args: img (ndarray): Image to be padded. shape (tuple[int]): Expected padding shape (h, w). Default: None. padding (int or tuple[int]): Padding on each border. If a single int is provided this is used to pad all borders. If tuple of length 2 is provided this is the padding on top/bottom and left/right respectively. If a tuple of length 4 is provided this is the padding for the top, bottom, left and right borders respectively. Default: None. Note that `shape` and `padding` can not be both set. pad_val (Number | Sequence[Number]): Values to be filled in padding areas when padding_mode is 'constant'. Default: 0. padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric. Default: constant. - constant: pads with a constant value, this value is specified with pad_val. - edge: pads with the last value at the edge of the image. - reflect: pads with reflection of image without repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]. - symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] Returns: ndarray: The padded image. \"\"\" assert ( shape is not None ) ^ ( padding is not None ) if shape is not None : width = max ( shape [ 1 ] - img . shape [ 1 ], 0 ) height = max ( shape [ 0 ] - img . shape [ 0 ], 0 ) padding = ( height // 2 , math . ceil ( height / 2 ), width // 2 , math . ceil ( width / 2 )) # check pad_val if isinstance ( pad_val , tuple ): assert len ( pad_val ) == img . shape [ - 1 ] elif not isinstance ( pad_val , numbers . Number ): raise TypeError ( 'pad_val must be a int or a tuple. ' f 'But received { type ( pad_val ) } ' ) # check padding if isinstance ( padding , tuple ) and len ( padding ) in [ 2 , 4 ]: if len ( padding ) == 2 : padding = ( padding [ 0 ], padding [ 1 ], padding [ 0 ], padding [ 1 ]) elif isinstance ( padding , numbers . Number ): padding = ( padding , padding , padding , padding ) else : raise ValueError ( 'Padding must be a int or a 2, or 4 element tuple.' f 'But received { padding } ' ) # check padding mode assert padding_mode in [ 'constant' , 'edge' , 'reflect' , 'symmetric' ] border_type = { 'constant' : cv2 . BORDER_CONSTANT , 'edge' : cv2 . BORDER_REPLICATE , 'reflect' : cv2 . BORDER_REFLECT_101 , 'symmetric' : cv2 . BORDER_REFLECT } # check dim <= 512 if img . shape [ - 1 ] > 512 : img_patch1 = cv2 . copyMakeBorder ( img [:, :, : 512 ], padding [ 0 ], padding [ 1 ], padding [ 2 ], padding [ 3 ], border_type [ padding_mode ], value = pad_val ) img_patch2 = cv2 . copyMakeBorder ( img [:, :, 512 :], padding [ 0 ], padding [ 1 ], padding [ 2 ], padding [ 3 ], border_type [ padding_mode ], value = pad_val ) img = np . concatenate (( img_patch1 , img_patch2 ), axis =- 1 ) else : img = cv2 . copyMakeBorder ( img , padding [ 0 ], padding [ 1 ], padding [ 2 ], padding [ 3 ], border_type [ padding_mode ], value = pad_val ) return img","title":"impad"},{"location":"mkdocs/dataset.html#datasets.pipelines.geometric.impad_to_multiple","text":"Pad an image to ensure each edge to be multiple to some number. Parameters: img ( ndarray ) \u2013 Image to be padded. divisor ( int ) \u2013 Padded image edges will be multiple to divisor. pad_val ( Number | Sequence [ Number ] , default: 0 ) \u2013 Same as :func: impad . padding_mode ( str , default: 'constant' ) \u2013 refer to impad Returns: ndarray ( ndarray ) \u2013 The padded image. Source code in uchiha\\datasets\\pipelines\\geometric.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def impad_to_multiple ( img : np . ndarray , divisor : int , pad_val : Union [ float , List ] = 0 , padding_mode : str = 'constant' ) -> np . ndarray : \"\"\"Pad an image to ensure each edge to be multiple to some number. Args: img (ndarray): Image to be padded. divisor (int): Padded image edges will be multiple to divisor. pad_val (Number | Sequence[Number]): Same as :func:`impad`. padding_mode: refer to impad Returns: ndarray: The padded image. \"\"\" pad_h = int ( np . ceil ( img . shape [ 0 ] / divisor )) * divisor pad_w = int ( np . ceil ( img . shape [ 1 ] / divisor )) * divisor return impad ( img , shape = ( pad_h , pad_w ), pad_val = pad_val , padding_mode = padding_mode )","title":"impad_to_multiple"},{"location":"mkdocs/dataset.html#datasets.pipelines.normalize.Normalize","text":"Normalize the image. When the mean and standard deviation are given,the data will be normalized according to the given data, otherwise the mean and standard deviation of the input data will be calculated for normalization. There are two normalization modes: (1) standardize: (x-mean)/std (2) normalize(minmax): (x-min)/(max-min) There are two normalization scopes: (1) spatial: normalize the spatial data for each channel (2) channel: normalize the channel data for each pixel in space Added key is \"img_norm_cfg\". Parameters: mean ( sequence , default: None ) \u2013 Mean values of 3 channels. Default: None. std ( sequence , default: None ) \u2013 Std values of 3 channels. Default: None. to_rgb ( bool , default: True ) \u2013 Whether to convert the image from BGR to RGB. Default: true. mode ( str , default: 'standard' ) \u2013 normalization mode. scope ( str , default: 'spatial' ) \u2013 normalization scope Source code in uchiha\\datasets\\pipelines\\normalize.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @PIPELINES . register_module () class Normalize : \"\"\"Normalize the image. When the mean and standard deviation are given,the data will be normalized according to the given data, otherwise the mean and standard deviation of the input data will be calculated for normalization. There are two normalization modes: (1) standardize: (x-mean)/std (2) normalize(minmax): (x-min)/(max-min) There are two normalization scopes: (1) spatial: normalize the spatial data for each channel (2) channel: normalize the channel data for each pixel in space Added key is \"img_norm_cfg\". Args: mean (sequence): Mean values of 3 channels. Default: None. std (sequence): Std values of 3 channels. Default: None. to_rgb (bool): Whether to convert the image from BGR to RGB. Default: true. mode (str): normalization mode. scope (str): normalization scope \"\"\" def __init__ ( self , mean = None , std = None , to_rgb = True , scope = 'spatial' , mode = 'standard' ): self . mean = np . array ( mean , dtype = np . float32 ) if mean else None self . std = np . array ( std , dtype = np . float32 ) if std else None self . to_rgb = to_rgb self . scope = scope self . mode = mode def __call__ ( self , results ): \"\"\"Call function to normalize images. Args: results (dict): Result dict from pipeline. Returns: results (dict): Normalized results, 'img_norm_cfg' key is added into result dict. \"\"\" results [ 'sample' ] = img_normalize ( results [ 'sample' ], self . mean , self . std , self . to_rgb , self . scope , self . mode ) if self . mean and self . std : results [ 'norm_cfg' ] = dict ( mean = self . mean , std = self . std , to_rgb = self . to_rgb , scope = self . scope ) else : results [ 'norm_cfg' ] = dict ( mean = 'auto' , std = 'auto' , to_rgb = self . to_rgb , scope = self . scope ) return results def __repr__ ( self ): repr_str = self . __class__ . __name__ repr_str += f '(mean= { self . mean } , std= { self . std } , to_rgb= { self . to_rgb } )' return repr_str","title":"Normalize"},{"location":"mkdocs/dataset.html#datasets.pipelines.normalize.Normalize.__call__","text":"Call function to normalize images. Parameters: results ( dict ) \u2013 Result dict from pipeline. Returns: results ( dict ) \u2013 Normalized results, 'img_norm_cfg' key is added into result dict. Source code in uchiha\\datasets\\pipelines\\normalize.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __call__ ( self , results ): \"\"\"Call function to normalize images. Args: results (dict): Result dict from pipeline. Returns: results (dict): Normalized results, 'img_norm_cfg' key is added into result dict. \"\"\" results [ 'sample' ] = img_normalize ( results [ 'sample' ], self . mean , self . std , self . to_rgb , self . scope , self . mode ) if self . mean and self . std : results [ 'norm_cfg' ] = dict ( mean = self . mean , std = self . std , to_rgb = self . to_rgb , scope = self . scope ) else : results [ 'norm_cfg' ] = dict ( mean = 'auto' , std = 'auto' , to_rgb = self . to_rgb , scope = self . scope ) return results","title":"__call__"},{"location":"mkdocs/dataset.html#datasets.pipelines.pad.Pad","text":"Pad the image. There are two padding modes: (1) pad to a fixed size (2) pad to the minimum size that is divisible by some number. there are four ways to handle edges: (1) constant: pad with a constant -> the given pad_val (2) edge: pad with the last value at the edge of the image. (3) reflect: pads with reflection of image without repeating the last value on the edge. For example: padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]. (4) symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] Added key is 'pad_cfg'. Parameters: size ( tuple , default: None ) \u2013 Fixed padding size -> mode(1). size_divisor ( int , default: None ) \u2013 The divisor of padded size -> mode(2). pad_val ( dict , default: 0 ) \u2013 A dict for padding value, Default: 0 . mode ( str , default: 'constant' ) \u2013 the way to handle edges Source code in uchiha\\datasets\\pipelines\\pad.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @PIPELINES . register_module () class Pad : \"\"\" Pad the image. There are two padding modes: (1) pad to a fixed size (2) pad to the minimum size that is divisible by some number. there are four ways to handle edges: (1) constant: pad with a constant -> the given pad_val (2) edge: pad with the last value at the edge of the image. (3) reflect: pads with reflection of image without repeating the last value on the edge. For example: padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]. (4) symmetric: pads with reflection of image repeating the last value on the edge. For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] Added key is 'pad_cfg'. Args: size (tuple, optional): Fixed padding size -> mode(1). size_divisor (int, optional): The divisor of padded size -> mode(2). pad_val (dict, optional): A dict for padding value, Default: `0`. mode (str): the way to handle edges \"\"\" def __init__ ( self , size = None , size_divisor = None , pad_val = 0 , mode = 'constant' ): self . size = size self . size_divisor = size_divisor self . pad_val = pad_val self . mode = mode assert size is not None or size_divisor is not None , \\ 'only one of size and size_divisor should be valid' def _pad_img ( self , results ): \"\"\"Pad images according to ``self.size``.\"\"\" pad_val = self . pad_val if 'sample' in results : key = 'sample' else : key = None results [ 'pad_cfg' ] = dict ( ori_shape = results [ key ] . shape , pad_shape = None ) if self . size is not None : padded_img = impad ( results [ key ], shape = self . size , pad_val = pad_val , padding_mode = self . mode ) elif self . size_divisor is not None : padded_img = impad_to_multiple ( results [ key ], self . size_divisor , pad_val = pad_val , padding_mode = self . mode ) else : padded_img = None results [ key ] = padded_img results [ 'pad_cfg' ][ 'pad_shape' ] = padded_img . shape def __call__ ( self , results ): \"\"\"Call function to pad images. Args: results (dict): Result dict from loading pipeline. Returns: results (dict): Updated result dict,'pad_cfg' key is added into result dict. \"\"\" self . _pad_img ( results ) return results def __repr__ ( self ): repr_str = self . __class__ . __name__ repr_str += f '(size= { self . size } , ' repr_str += f 'size_divisor= { self . size_divisor } , ' repr_str += f 'pad_val= { self . pad_val } )' return repr_str","title":"Pad"},{"location":"mkdocs/dataset.html#datasets.pipelines.pad.Pad.__call__","text":"Call function to pad images. Parameters: results ( dict ) \u2013 Result dict from loading pipeline. Returns: results ( dict ) \u2013 Updated result dict,'pad_cfg' key is added into result dict. Source code in uchiha\\datasets\\pipelines\\pad.py 64 65 66 67 68 69 70 71 72 73 74 def __call__ ( self , results ): \"\"\"Call function to pad images. Args: results (dict): Result dict from loading pipeline. Returns: results (dict): Updated result dict,'pad_cfg' key is added into result dict. \"\"\" self . _pad_img ( results ) return results","title":"__call__"},{"location":"mkdocs/dataset.html#datasets.pipelines.rotation.RandomRotation","text":"Source code in uchiha\\datasets\\pipelines\\rotation.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 @PIPELINES . register_module () class RandomRotation : def __init__ ( self , angles = None , prob = 0.5 , interpolation = cv2 . INTER_LINEAR , border_mode = cv2 . BORDER_REFLECT_101 ): \"\"\" Args: angles (tuple): rotation angle range for example 30 30 prob (float): probability of triggering enhancement interpolation (int): interpolation method\uff08cv2.INTER_NEAREST / INTER_LINEAR / INTER_CUBIC\uff09 border_mode (int): boundary fill method\uff08cv2.BORDER_CONSTANT / BORDER_REPLICATE / BORDER_REFLECT_101\uff09 \"\"\" if angles is None : angles = [ - 90 , 90 ] self . angles = angles self . p = prob self . interpolation = interpolation self . border_mode = border_mode def __call__ ( self , data ): if random . random () < self . p : sample = data [ 'sample' ] target = data [ 'target' ] h , w = sample . shape [: 2 ] center = ( w / 2 , h / 2 ) # randomly generate rotation angle angle = random . choice ( self . angles ) if isinstance ( angle , int ): angle = float ( angle ) # compute the rotation matrix M = cv2 . getRotationMatrix2D ( center , angle , scale = 1.0 ) # applying affine transformation if len ( sample . shape ) == 3 : # HWC sample_rotated = cv2 . warpAffine ( sample , M , ( w , h ), flags = self . interpolation , borderMode = self . border_mode ) else : # HW sample_rotated = cv2 . warpAffine ( sample , M , ( w , h ), flags = self . interpolation , borderMode = self . border_mode ) if len ( target . shape ) == 3 : # HWC target_rotated = cv2 . warpAffine ( target , M , ( w , h ), flags = self . interpolation , borderMode = self . border_mode ) else : # HW target_rotated = cv2 . warpAffine ( target , M , ( w , h ), flags = self . interpolation , borderMode = self . border_mode ) data [ 'sample' ] = sample_rotated data [ 'target' ] = target_rotated return data","title":"RandomRotation"},{"location":"mkdocs/dataset.html#datasets.pipelines.rotation.RandomRotation.__init__","text":"Parameters: angles ( tuple , default: None ) \u2013 rotation angle range for example 30 30 prob ( float , default: 0.5 ) \u2013 probability of triggering enhancement interpolation ( int , default: INTER_LINEAR ) \u2013 interpolation method\uff08cv2.INTER_NEAREST / INTER_LINEAR / INTER_CUBIC\uff09 border_mode ( int , default: BORDER_REFLECT_101 ) \u2013 boundary fill method\uff08cv2.BORDER_CONSTANT / BORDER_REPLICATE / BORDER_REFLECT_101\uff09 Source code in uchiha\\datasets\\pipelines\\rotation.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def __init__ ( self , angles = None , prob = 0.5 , interpolation = cv2 . INTER_LINEAR , border_mode = cv2 . BORDER_REFLECT_101 ): \"\"\" Args: angles (tuple): rotation angle range for example 30 30 prob (float): probability of triggering enhancement interpolation (int): interpolation method\uff08cv2.INTER_NEAREST / INTER_LINEAR / INTER_CUBIC\uff09 border_mode (int): boundary fill method\uff08cv2.BORDER_CONSTANT / BORDER_REPLICATE / BORDER_REFLECT_101\uff09 \"\"\" if angles is None : angles = [ - 90 , 90 ] self . angles = angles self . p = prob self . interpolation = interpolation self . border_mode = border_mode","title":"__init__"},{"location":"mkdocs/dataset.html#customdataset","text":"","title":"CustomDataset"},{"location":"mkdocs/dataset.html#datasets.soil_pred.SoilDataset1d","text":"Bases: Dataset the dataset for hyperspectral sequence-data Parameters: data_root ( str ) \u2013 the root directory where the dataset is stored gt_path ( str ) \u2013 path of GT data elements ( str , default: None ) \u2013 Elements to be predicted, a comma-separated string is parsed into a list pipelines ( Sequence [ dict ] , default: None ) \u2013 Data processing flow: a sequence each element is different data processing configuration information Source code in uchiha\\datasets\\soil_pred.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 @DATASET . register_module () class SoilDataset1d ( Dataset ): \"\"\" the dataset for hyperspectral sequence-data Args: data_root (str): the root directory where the dataset is stored gt_path (str): path of GT data elements (str): Elements to be predicted, a comma-separated string is parsed into a list pipelines (Sequence[dict]): Data processing flow: a sequence each element is different data processing configuration information \"\"\" ELEMENTS = [ 'Zn' , 'Subs' ] def __init__ ( self , data_root , gt_path , elements = None , pipelines = None ): self . data_root = data_root self . spectral_data = read_pts ( data_root , is1d = True ) self . gt = read_txt ( gt_path ) self . ELEMENTS = self . get_elements ( elements ) self . pipelines = Compose ( pipelines ) if pipelines else None def __getitem__ ( self , index ): results = dict ( sample = self . spectral_data [ index ], target = self . gt [ index ], index = index ) return self . pipelines ( results ) if self . pipelines else results def __len__ ( self ): return len ( self . spectral_data ) def get_elements ( self , elements ): \"\"\" get the elements to be predicted Gets a list of containing elements based on the provided string separated by commas Args: elements (str): Elements to be predicted Returns: element_names (list): the list containing elements that need to be predicted \"\"\" if elements is None : return self . ELEMENTS if isinstance ( elements , str ): element_names = strings_to_list ( elements ) elif isinstance ( elements , ( tuple , list )): element_names = elements else : raise ValueError ( f 'Unsupported type { type ( elements ) } of elements.' ) return element_names def evaluate ( self , preds , targets = None , metric = 'MAE' ): \"\"\" evaluation of predicted values Args: preds (Tensor): output prediction of the model targets (Tensor): the true value of the predicted element metric (str): evaluation metric. Default: 'MAE' Returns: results (dict): evaluation results for each element \"\"\" elements = self . ELEMENTS if targets is None : targets = self . gt results = regression_eval ( preds , targets , elements , metric ) return results","title":"SoilDataset1d"},{"location":"mkdocs/dataset.html#datasets.soil_pred.SoilDataset1d.evaluate","text":"evaluation of predicted values Parameters: preds ( Tensor ) \u2013 output prediction of the model targets ( Tensor , default: None ) \u2013 the true value of the predicted element metric ( str , default: 'MAE' ) \u2013 evaluation metric. Default: 'MAE' Returns: results ( dict ) \u2013 evaluation results for each element Source code in uchiha\\datasets\\soil_pred.py 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def evaluate ( self , preds , targets = None , metric = 'MAE' ): \"\"\" evaluation of predicted values Args: preds (Tensor): output prediction of the model targets (Tensor): the true value of the predicted element metric (str): evaluation metric. Default: 'MAE' Returns: results (dict): evaluation results for each element \"\"\" elements = self . ELEMENTS if targets is None : targets = self . gt results = regression_eval ( preds , targets , elements , metric ) return results","title":"evaluate"},{"location":"mkdocs/dataset.html#datasets.soil_pred.SoilDataset1d.get_elements","text":"get the elements to be predicted Gets a list of containing elements based on the provided string separated by commas Parameters: elements ( str ) \u2013 Elements to be predicted Returns: element_names ( list ) \u2013 the list containing elements that need to be predicted Source code in uchiha\\datasets\\soil_pred.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 def get_elements ( self , elements ): \"\"\" get the elements to be predicted Gets a list of containing elements based on the provided string separated by commas Args: elements (str): Elements to be predicted Returns: element_names (list): the list containing elements that need to be predicted \"\"\" if elements is None : return self . ELEMENTS if isinstance ( elements , str ): element_names = strings_to_list ( elements ) elif isinstance ( elements , ( tuple , list )): element_names = elements else : raise ValueError ( f 'Unsupported type { type ( elements ) } of elements.' ) return element_names","title":"get_elements"},{"location":"mkdocs/dataset.html#datasets.soil_pred.SoilDataset2d","text":"Bases: Dataset the dataset for hyperspectral images Parameters: data_root ( str ) \u2013 the root directory where the dataset is stored gt_path ( str ) \u2013 path of GT data elements ( str , default: None ) \u2013 Elements to be predicted, a comma-separated string is parsed into a list pipelines ( Sequence [ dict ] , default: None ) \u2013 Data processing flow: a sequence each element is different data processing configuration information Source code in uchiha\\datasets\\soil_pred.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 @DATASET . register_module () class SoilDataset2d ( Dataset ): \"\"\" the dataset for hyperspectral images Args: data_root (str): the root directory where the dataset is stored gt_path (str): path of GT data elements (str): Elements to be predicted, a comma-separated string is parsed into a list pipelines (Sequence[dict]): Data processing flow: a sequence each element is different data processing configuration information \"\"\" ELEMENTS = [ 'Zn' , 'Others' ] def __init__ ( self , data_root , gt_path , elements = None , pipelines = None ): self . data_root = data_root self . spectral_data = read_npy ( data_root ) self . gt = read_txt ( gt_path ) self . ELEMENTS = self . get_elements ( elements ) # postprocess self . pipelines = Compose ( pipelines ) def __getitem__ ( self , index ): results = dict ( sample = self . spectral_data [ index ], target = self . gt [ index ], index = index ) return self . pipelines ( results ) def __len__ ( self ): return len ( self . spectral_data ) def get_elements ( self , elements ): \"\"\" get the elements to be predicted Gets a list of containing elements based on the provided string separated by commas Args: elements (str): Elements to be predicted Returns: element_names (list): the list containing elements that need to be predicted \"\"\" if elements is None : return self . ELEMENTS if isinstance ( elements , str ): element_names = strings_to_list ( elements ) elif isinstance ( elements , ( tuple , list )): element_names = elements else : raise ValueError ( f 'Unsupported type { type ( elements ) } of elements.' ) return element_names def evaluate ( self , preds , targets = None , metric = 'MAE' , indexes = None ): \"\"\" evaluation of predicted values Args: preds (Tensor): output prediction of the model targets (Tensor): the true value of the predicted element metric (str): evaluation metric. Default: 'MAE' indexes (int): index of original data Returns: results (dict): evaluation results for each element \"\"\" elements = self . ELEMENTS if targets is None : targets = self . gt results = regression_eval ( preds , targets , elements , metric ) return results","title":"SoilDataset2d"},{"location":"mkdocs/dataset.html#datasets.soil_pred.SoilDataset2d.evaluate","text":"evaluation of predicted values Parameters: preds ( Tensor ) \u2013 output prediction of the model targets ( Tensor , default: None ) \u2013 the true value of the predicted element metric ( str , default: 'MAE' ) \u2013 evaluation metric. Default: 'MAE' indexes ( int , default: None ) \u2013 index of original data Returns: results (dict): evaluation results for each element Source code in uchiha\\datasets\\soil_pred.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def evaluate ( self , preds , targets = None , metric = 'MAE' , indexes = None ): \"\"\" evaluation of predicted values Args: preds (Tensor): output prediction of the model targets (Tensor): the true value of the predicted element metric (str): evaluation metric. Default: 'MAE' indexes (int): index of original data Returns: results (dict): evaluation results for each element \"\"\" elements = self . ELEMENTS if targets is None : targets = self . gt results = regression_eval ( preds , targets , elements , metric ) return results","title":"evaluate"},{"location":"mkdocs/dataset.html#datasets.soil_pred.SoilDataset2d.get_elements","text":"get the elements to be predicted Gets a list of containing elements based on the provided string separated by commas Parameters: elements ( str ) \u2013 Elements to be predicted Returns: element_names ( list ) \u2013 the list containing elements that need to be predicted Source code in uchiha\\datasets\\soil_pred.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def get_elements ( self , elements ): \"\"\" get the elements to be predicted Gets a list of containing elements based on the provided string separated by commas Args: elements (str): Elements to be predicted Returns: element_names (list): the list containing elements that need to be predicted \"\"\" if elements is None : return self . ELEMENTS if isinstance ( elements , str ): element_names = strings_to_list ( elements ) elif isinstance ( elements , ( tuple , list )): element_names = elements else : raise ValueError ( f 'Unsupported type { type ( elements ) } of elements.' ) return element_names","title":"get_elements"},{"location":"mkdocs/dataset.html#datasets.soil_pred.compute_metric","text":"given metric, calculation result Parameters: pred ( ndarray ) \u2013 Prediction target ( ndarray ) \u2013 GT metric ( str , default: 'MAE' ) \u2013 Evaluation metric. Default: MAE. Returns: ndarray \u2013 Calculation results Source code in uchiha\\datasets\\soil_pred.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def compute_metric ( pred , target , metric = 'MAE' ): \"\"\" given metric, calculation result Args: pred (ndarray): Prediction target (ndarray): GT metric (str): Evaluation metric. Default: MAE. Returns: ndarray: Calculation results \"\"\" if metric == 'MAE' : return mean_absolute_error ( pred , target ) elif metric == 'R2' : return 1 - ( np . sum (( target - pred ) ** 2 , axis = 0 ) / np . sum (( target - np . mean ( target )) ** 2 , axis = 0 )) # return (np.sum((pred - np.mean(target)) ** 2) / np.sum((target - np.mean(target)) ** 2)) else : raise NotImplementedError ( f 'metric: { metric } is not supported yet' )","title":"compute_metric"},{"location":"mkdocs/dataset.html#datasets.soil_pred.print_metrics","text":"print metrics in the console Parameters: result ( dict ) \u2013 Evaluation results for each element . metric ( str ) \u2013 Evaluation metric. Source code in uchiha\\datasets\\soil_pred.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def print_metrics ( result , metric ): \"\"\" print metrics in the console Args: result (dict): Evaluation results for each element . metric (str): Evaluation metric. \"\"\" header = [ 'Element' , f ' { metric } ' ] table_data = [ header ] for key , value in result . items (): row_data = [ key , f ' { value : .6f } ' ] table_data . append ( row_data ) table = AsciiTable ( table_data ) table . inner_footing_row_border = True table . justify_columns [ 1 ] = 'center' logger = get_root_logger () print_log ( ' \\n ' + table . table , logger = logger )","title":"print_metrics"},{"location":"mkdocs/dataset.html#datasets.soil_pred.regression_eval","text":"evaluation of regression tasks Parameters: preds ( Tensor ) \u2013 Prediction of the model targets ( Tensor ) \u2013 GT data elements ( List [ str ] ) \u2013 Elements that need to be predicted metric ( str , default: 'MAE' ) \u2013 Evaluation metric. Default: MAE. Returns: dict \u2013 evaluation results for each element Source code in uchiha\\datasets\\soil_pred.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def regression_eval ( preds , targets , elements , metric = 'MAE' ): \"\"\" evaluation of regression tasks Args: preds (Tensor): Prediction of the model targets (Tensor): GT data elements (List[str]): Elements that need to be predicted metric (str): Evaluation metric. Default: MAE. Returns: dict: evaluation results for each element \"\"\" results = { element : 0.0 for element in elements } preds = np . vstack ( tensor2np ( preds )) targets = np . vstack ( tensor2np ( targets )) metrics = compute_metric ( preds , targets , metric ) if len ( results ) == 1 : results [ elements [ 0 ]] = metrics [ 0 ] else : for idx , element in enumerate ( elements ): results [ element ] = metrics [ idx ] print_metrics ( results , metric ) return results","title":"regression_eval"},{"location":"mkdocs/dataset.html#datasets.soil_pred.tensor2np","text":"tensor --> ndarray Parameters: x ( Tensor ) \u2013 Input tensor data Returns: ndarray \u2013 converted ndarray data Source code in uchiha\\datasets\\soil_pred.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def tensor2np ( x ): \"\"\" tensor --> ndarray Args: x (Tensor): Input tensor data Returns: ndarray: converted ndarray data \"\"\" if isinstance ( x , list ): for idx , data in enumerate ( x ): x [ idx ] = tensor2np ( data ) if isinstance ( x , torch . Tensor ): x = x . cpu () . numpy () return x","title":"tensor2np"},{"location":"mkdocs/dataset.html#datasets.hdr.HDF5MultiLevelRainHSIDataset","text":"Bases: Dataset Multi-level noise HSI data set based on HDF5 storage Parameters: h5_path \u2013 HDF5 file path split_file \u2013 txt path of the divided file pipelines \u2013 data preprocessing pipeline Source code in uchiha\\datasets\\hdr.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 @DATASET . register_module () class HDF5MultiLevelRainHSIDataset ( Dataset ): \"\"\"Multi-level noise HSI data set based on HDF5 storage Args: h5_path: HDF5 file path split_file: txt path of the divided file pipelines: data preprocessing pipeline \"\"\" NOISE_LEVELS = [ 'small' , 'medium' , 'heavy' , 'storm' ] def __init__ ( self , h5_path , split_file , pipelines = None ): self . h5_path = h5_path self . pipelines = Compose ( pipelines ) if pipelines else None # load partition file with open ( split_file , 'r' ) as f : self . sample_names = [ line . strip () for line in f if line . strip ()] # Initialize HDF5 connection (delayed until it is turned on for actual use) self . _h5 = None self . _validate_samples () @property def h5 ( self ): \"\"\"Delayed loading of HDF5 files (multi-process support)\"\"\" if self . _h5 is None : self . _h5 = h5py . File ( self . h5_path , 'r' , libver = 'latest' , swmr = True ) return self . _h5 def _validate_samples ( self ): \"\"\"verify all samples are present in hdf5\"\"\" # no suffix in hdf5 with h5py . File ( self . h5_path , 'r' ) as hf : missing = [] for name in self . sample_names : name = name . replace ( '.npy' , '' ) # verify gt if f 'gt/ { name } ' not in hf : missing . append ( f 'gt/ { name } ' ) # verify each noise level for level in self . NOISE_LEVELS : if f 'rain/ { level } / { name } ' not in hf : missing . append ( f 'rain/ { level } / { name } ' ) if missing : raise KeyError ( f \"Missing { len ( missing ) } samples in HDF5, e.g.: { missing [: 3 ] } \" ) def __len__ ( self ): return len ( self . sample_names ) * len ( self . NOISE_LEVELS ) def __getitem__ ( self , idx ): # Calculate the correspondence between sample name and noise level sample_idx = idx // len ( self . NOISE_LEVELS ) noise_idx = idx % len ( self . NOISE_LEVELS ) sample_name = self . sample_names [ sample_idx ] noise_level = self . NOISE_LEVELS [ noise_idx ] sample_name = sample_name . replace ( '.npy' , '' ) # load data from hdf5 memory mapped gt = self . h5 [ f 'gt/ { sample_name } ' ][:] lq = self . h5 [ f 'rain/ { noise_level } / { sample_name } ' ][:] results = { 'sample' : lq , 'target' : gt , 'index' : idx , 'noise_level' : noise_level } return self . pipelines ( results ) if self . pipelines else results def evaluate ( self , preds : List [ np . ndarray ], targets : List [ np . ndarray ], metric , indexes : List [ int ]) -> dict : \"\"\" Calculates the similarity between the predicted data and the real data. PSNR and SSIM are supported by default. Args: preds: Model output list, each element shape is (B, C, H, W) targets: A list of real labels, each element of shape (B, C, H, W) metric: metric to be calculated. Default value: ['PSNR','SSIM'] indexes: raw data index list Returns: dict: dictionary with average psnr and ssim \"\"\" assert len ( preds ) == len ( targets ) == len ( indexes ), \"input list length must be the same\" assert metric is None or metric . upper () == 'PSNR' or metric . upper () == 'SSIM' , \\ 'only psnr and ssim are supported by default' results = { 'global' : { 'psnr' : [], 'ssim' : []}, 'by_level' : { level : { 'psnr' : [], 'ssim' : []} for level in self . NOISE_LEVELS } } logger = get_root_logger () logger . info ( 'start evaluating...' ) for pred_batch , target_batch , index_batch in zip ( preds , targets , indexes ): B , C , H , W = pred_batch . shape for i in range ( B ): pred = pred_batch [ i ] # (C,H,W) target = target_batch [ i ] idx = index_batch [ i ] # acquire noise level noise_level = self . NOISE_LEVELS [ idx % len ( self . NOISE_LEVELS )] # convert to (H W C) format if C == 1 : pred_img = pred [ 0 ] target_img = target [ 0 ] else : pred_img = np . transpose ( pred , ( 1 , 2 , 0 )) target_img = np . transpose ( target , ( 1 , 2 , 0 )) # calculate psnr data_range = target_img . max () - target_img . min () psnr_val = psnr ( target_img , pred_img , data_range = data_range ) # calculate ssim multichannel = C > 1 ssim_val = ssim ( target_img , pred_img , multichannel = multichannel , channel_axis = 2 if multichannel else None , data_range = data_range ) # store results results [ 'global' ][ 'psnr' ] . append ( psnr_val ) results [ 'global' ][ 'ssim' ] . append ( ssim_val ) results [ 'by_level' ][ noise_level ][ 'psnr' ] . append ( psnr_val ) results [ 'by_level' ][ noise_level ][ 'ssim' ] . append ( ssim_val ) # computational statistics def calc_stats ( values ): return { 'mean' : np . mean ( values ), 'std' : np . std ( values ), 'min' : np . min ( values ), 'max' : np . max ( values ), 'count' : len ( values ) } stats = { 'global' : { 'psnr' : calc_stats ( results [ 'global' ][ 'psnr' ]), 'ssim' : calc_stats ( results [ 'global' ][ 'ssim' ]) }, 'by_level' : { level : { 'psnr' : calc_stats ( results [ 'by_level' ][ level ][ 'psnr' ]), 'ssim' : calc_stats ( results [ 'by_level' ][ level ][ 'ssim' ]) } for level in self . NOISE_LEVELS } } # log output logger . info ( f \" { ' Evaluation Results ' : =^60 } \" ) # Build hierarchical indicator tables table_data = [] for level in self . NOISE_LEVELS : table_data . append ([ level . upper (), f \" { stats [ 'by_level' ][ level ][ 'psnr' ][ 'mean' ] : .2f } \u00b1 { stats [ 'by_level' ][ level ][ 'psnr' ][ 'std' ] : .2f } \" , f \" { stats [ 'by_level' ][ level ][ 'psnr' ][ 'min' ] : .2f } - { stats [ 'by_level' ][ level ][ 'psnr' ][ 'max' ] : .2f } \" , f \" { stats [ 'by_level' ][ level ][ 'ssim' ][ 'mean' ] : .4f } \u00b1 { stats [ 'by_level' ][ level ][ 'ssim' ][ 'std' ] : .4f } \" , f \" { stats [ 'by_level' ][ level ][ 'ssim' ][ 'min' ] : .4f } - { stats [ 'by_level' ][ level ][ 'ssim' ][ 'max' ] : .4f } \" , stats [ 'by_level' ][ level ][ 'psnr' ][ 'count' ] ]) table_data . append ([ 'TOTAL' , f \" { stats [ 'global' ][ 'psnr' ][ 'mean' ] : .2f } \u00b1 { stats [ 'global' ][ 'psnr' ][ 'std' ] : .2f } \" , f \" { stats [ 'global' ][ 'psnr' ][ 'min' ] : .2f } - { stats [ 'global' ][ 'psnr' ][ 'max' ] : .2f } \" , f \" { stats [ 'global' ][ 'ssim' ][ 'mean' ] : .4f } \u00b1 { stats [ 'global' ][ 'ssim' ][ 'std' ] : .4f } \" , f \" { stats [ 'global' ][ 'ssim' ][ 'min' ] : .4f } - { stats [ 'global' ][ 'ssim' ][ 'max' ] : .4f } \" , stats [ 'global' ][ 'psnr' ][ 'count' ] ]) # log logger . info ( \" \\n \" + tabulate ( table_data , headers = [ \"Noise Level\" , \"PSNR (dB)\" , \"PSNR Range\" , \"SSIM\" , \"SSIM Range\" , \"Samples\" ], tablefmt = \"fancy_grid\" , floatfmt = ( \"\" , \".2f\" , \"\" , \".4f\" , \"\" , \"d\" ) )) logger . info ( f \"Mean PSNR: { stats [ 'global' ][ 'psnr' ][ 'mean' ] : .2f } dB\" ) logger . info ( f \"Mean SSIM: { stats [ 'global' ][ 'ssim' ][ 'mean' ] : .4f } \" ) logger . info ( \"=\" * 60 ) return { 'psnr' : stats [ 'global' ][ 'psnr' ][ 'mean' ], 'ssim' : stats [ 'global' ][ 'ssim' ][ 'mean' ] } def get_filename ( self , idx ): \"\"\"Get the original file name (without noise level information)\"\"\" sample_idx = idx // len ( self . NOISE_LEVELS ) return self . sample_names [ sample_idx ] def __del__ ( self ): \"\"\"ensure hdf5 connection is closed\"\"\" if self . _h5 is not None : self . _h5 . close ()","title":"HDF5MultiLevelRainHSIDataset"},{"location":"mkdocs/dataset.html#datasets.hdr.HDF5MultiLevelRainHSIDataset.h5","text":"Delayed loading of HDF5 files (multi-process support)","title":"h5"},{"location":"mkdocs/dataset.html#datasets.hdr.HDF5MultiLevelRainHSIDataset.__del__","text":"ensure hdf5 connection is closed Source code in uchiha\\datasets\\hdr.py 228 229 230 231 def __del__ ( self ): \"\"\"ensure hdf5 connection is closed\"\"\" if self . _h5 is not None : self . _h5 . close ()","title":"__del__"},{"location":"mkdocs/dataset.html#datasets.hdr.HDF5MultiLevelRainHSIDataset.evaluate","text":"Calculates the similarity between the predicted data and the real data. PSNR and SSIM are supported by default. Parameters: preds ( List [ ndarray ] ) \u2013 Model output list, each element shape is (B, C, H, W) targets ( List [ ndarray ] ) \u2013 A list of real labels, each element of shape (B, C, H, W) metric \u2013 metric to be calculated. Default value: ['PSNR','SSIM'] indexes ( List [ int ] ) \u2013 raw data index list Returns: dict ( dict ) \u2013 dictionary with average psnr and ssim Source code in uchiha\\datasets\\hdr.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def evaluate ( self , preds : List [ np . ndarray ], targets : List [ np . ndarray ], metric , indexes : List [ int ]) -> dict : \"\"\" Calculates the similarity between the predicted data and the real data. PSNR and SSIM are supported by default. Args: preds: Model output list, each element shape is (B, C, H, W) targets: A list of real labels, each element of shape (B, C, H, W) metric: metric to be calculated. Default value: ['PSNR','SSIM'] indexes: raw data index list Returns: dict: dictionary with average psnr and ssim \"\"\" assert len ( preds ) == len ( targets ) == len ( indexes ), \"input list length must be the same\" assert metric is None or metric . upper () == 'PSNR' or metric . upper () == 'SSIM' , \\ 'only psnr and ssim are supported by default' results = { 'global' : { 'psnr' : [], 'ssim' : []}, 'by_level' : { level : { 'psnr' : [], 'ssim' : []} for level in self . NOISE_LEVELS } } logger = get_root_logger () logger . info ( 'start evaluating...' ) for pred_batch , target_batch , index_batch in zip ( preds , targets , indexes ): B , C , H , W = pred_batch . shape for i in range ( B ): pred = pred_batch [ i ] # (C,H,W) target = target_batch [ i ] idx = index_batch [ i ] # acquire noise level noise_level = self . NOISE_LEVELS [ idx % len ( self . NOISE_LEVELS )] # convert to (H W C) format if C == 1 : pred_img = pred [ 0 ] target_img = target [ 0 ] else : pred_img = np . transpose ( pred , ( 1 , 2 , 0 )) target_img = np . transpose ( target , ( 1 , 2 , 0 )) # calculate psnr data_range = target_img . max () - target_img . min () psnr_val = psnr ( target_img , pred_img , data_range = data_range ) # calculate ssim multichannel = C > 1 ssim_val = ssim ( target_img , pred_img , multichannel = multichannel , channel_axis = 2 if multichannel else None , data_range = data_range ) # store results results [ 'global' ][ 'psnr' ] . append ( psnr_val ) results [ 'global' ][ 'ssim' ] . append ( ssim_val ) results [ 'by_level' ][ noise_level ][ 'psnr' ] . append ( psnr_val ) results [ 'by_level' ][ noise_level ][ 'ssim' ] . append ( ssim_val ) # computational statistics def calc_stats ( values ): return { 'mean' : np . mean ( values ), 'std' : np . std ( values ), 'min' : np . min ( values ), 'max' : np . max ( values ), 'count' : len ( values ) } stats = { 'global' : { 'psnr' : calc_stats ( results [ 'global' ][ 'psnr' ]), 'ssim' : calc_stats ( results [ 'global' ][ 'ssim' ]) }, 'by_level' : { level : { 'psnr' : calc_stats ( results [ 'by_level' ][ level ][ 'psnr' ]), 'ssim' : calc_stats ( results [ 'by_level' ][ level ][ 'ssim' ]) } for level in self . NOISE_LEVELS } } # log output logger . info ( f \" { ' Evaluation Results ' : =^60 } \" ) # Build hierarchical indicator tables table_data = [] for level in self . NOISE_LEVELS : table_data . append ([ level . upper (), f \" { stats [ 'by_level' ][ level ][ 'psnr' ][ 'mean' ] : .2f } \u00b1 { stats [ 'by_level' ][ level ][ 'psnr' ][ 'std' ] : .2f } \" , f \" { stats [ 'by_level' ][ level ][ 'psnr' ][ 'min' ] : .2f } - { stats [ 'by_level' ][ level ][ 'psnr' ][ 'max' ] : .2f } \" , f \" { stats [ 'by_level' ][ level ][ 'ssim' ][ 'mean' ] : .4f } \u00b1 { stats [ 'by_level' ][ level ][ 'ssim' ][ 'std' ] : .4f } \" , f \" { stats [ 'by_level' ][ level ][ 'ssim' ][ 'min' ] : .4f } - { stats [ 'by_level' ][ level ][ 'ssim' ][ 'max' ] : .4f } \" , stats [ 'by_level' ][ level ][ 'psnr' ][ 'count' ] ]) table_data . append ([ 'TOTAL' , f \" { stats [ 'global' ][ 'psnr' ][ 'mean' ] : .2f } \u00b1 { stats [ 'global' ][ 'psnr' ][ 'std' ] : .2f } \" , f \" { stats [ 'global' ][ 'psnr' ][ 'min' ] : .2f } - { stats [ 'global' ][ 'psnr' ][ 'max' ] : .2f } \" , f \" { stats [ 'global' ][ 'ssim' ][ 'mean' ] : .4f } \u00b1 { stats [ 'global' ][ 'ssim' ][ 'std' ] : .4f } \" , f \" { stats [ 'global' ][ 'ssim' ][ 'min' ] : .4f } - { stats [ 'global' ][ 'ssim' ][ 'max' ] : .4f } \" , stats [ 'global' ][ 'psnr' ][ 'count' ] ]) # log logger . info ( \" \\n \" + tabulate ( table_data , headers = [ \"Noise Level\" , \"PSNR (dB)\" , \"PSNR Range\" , \"SSIM\" , \"SSIM Range\" , \"Samples\" ], tablefmt = \"fancy_grid\" , floatfmt = ( \"\" , \".2f\" , \"\" , \".4f\" , \"\" , \"d\" ) )) logger . info ( f \"Mean PSNR: { stats [ 'global' ][ 'psnr' ][ 'mean' ] : .2f } dB\" ) logger . info ( f \"Mean SSIM: { stats [ 'global' ][ 'ssim' ][ 'mean' ] : .4f } \" ) logger . info ( \"=\" * 60 ) return { 'psnr' : stats [ 'global' ][ 'psnr' ][ 'mean' ], 'ssim' : stats [ 'global' ][ 'ssim' ][ 'mean' ] }","title":"evaluate"},{"location":"mkdocs/dataset.html#datasets.hdr.HDF5MultiLevelRainHSIDataset.get_filename","text":"Get the original file name (without noise level information) Source code in uchiha\\datasets\\hdr.py 223 224 225 226 def get_filename ( self , idx ): \"\"\"Get the original file name (without noise level information)\"\"\" sample_idx = idx // len ( self . NOISE_LEVELS ) return self . sample_names [ sample_idx ]","title":"get_filename"},{"location":"mkdocs/dataset.html#datasets.hdr.MultiLevelRainHSIDatasetV0","text":"Bases: Dataset Source code in uchiha\\datasets\\hdr.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 @DeprecationWarning @DATASET . register_module () class MultiLevelRainHSIDatasetV0 ( Dataset ): RAIN_TYPES = [ 'small' , 'medium' , 'heavy' , 'storm' ] # Four noise types are fixed def __init__ ( self , data_root : str , pipelines : Dict = None ): \"\"\" Args: data_root: The root directory of the dataset \"\"\" self . root = data_root self . pipelines = Compose ( pipelines ) if pipelines else None # Check the directory structure self . _validate_directory_structure () # Load all sample paths self . gt_paths = sorted ( glob . glob ( os . path . join ( data_root , 'gt' , '*.npy' ))) if not self . gt_paths : raise FileNotFoundError ( f \"No .npy files found in { os . path . join ( data_root , 'gt' ) } \" ) self . paths = self . _build_paths_index () def _validate_directory_structure ( self ): \"\"\"Verify that the directory structure is as expected\"\"\" required_dirs = [ 'gt' ] + [ os . path . join ( 'rain' , t ) for t in self . RAIN_TYPES ] for d in required_dirs : if not os . path . isdir ( os . path . join ( self . root , d )): raise RuntimeError ( f \"Missing required directory: { d } \" ) def _build_paths_index ( self ) -> List [ Dict [ str , str ]]: \"\"\"Build a list of sample index dictionaries\"\"\" paths = [] for gt_path in self . gt_paths : sample = { 'gt_path' : gt_path } # Obtain the corresponding noise file path filename , ext = os . path . basename ( gt_path ) . split ( '.' ) for rain_type in self . RAIN_TYPES : rain_path = os . path . join ( self . root , 'rain' , rain_type , f ' { filename } _ { rain_type } . { ext } ' ) if not os . path . exists ( rain_path ): raise FileNotFoundError ( f \"Missing corresponding rain file: { rain_path } \" ) sample [ 'lq_path' ] = rain_path paths . append ( sample . copy ()) return paths def __len__ ( self ) -> int : return len ( self . paths ) def __getitem__ ( self , idx : int ) -> Dict : sample = self . paths [ idx ] # \u52a0\u8f7d\u6570\u636e gt_path = sample [ 'gt_path' ] lq_path = sample [ 'lq_path' ] gt = np . load ( gt_path ) lq = np . load ( lq_path ) results = { 'sample' : lq , 'target' : gt , 'index' : idx } return self . pipelines ( results ) if self . pipelines else results def get_rain_types ( self ) -> List [ str ]: \"\"\"\u83b7\u53d6\u652f\u6301\u7684\u566a\u58f0\u7c7b\u578b\u5217\u8868\"\"\" return self . RAIN_TYPES . copy ()","title":"MultiLevelRainHSIDatasetV0"},{"location":"mkdocs/dataset.html#datasets.hdr.MultiLevelRainHSIDatasetV0.__init__","text":"Parameters: data_root ( str ) \u2013 The root directory of the dataset Source code in uchiha\\datasets\\hdr.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def __init__ ( self , data_root : str , pipelines : Dict = None ): \"\"\" Args: data_root: The root directory of the dataset \"\"\" self . root = data_root self . pipelines = Compose ( pipelines ) if pipelines else None # Check the directory structure self . _validate_directory_structure () # Load all sample paths self . gt_paths = sorted ( glob . glob ( os . path . join ( data_root , 'gt' , '*.npy' ))) if not self . gt_paths : raise FileNotFoundError ( f \"No .npy files found in { os . path . join ( data_root , 'gt' ) } \" ) self . paths = self . _build_paths_index ()","title":"__init__"},{"location":"mkdocs/dataset.html#datasets.hdr.MultiLevelRainHSIDatasetV0.get_rain_types","text":"\u83b7\u53d6\u652f\u6301\u7684\u566a\u58f0\u7c7b\u578b\u5217\u8868 Source code in uchiha\\datasets\\hdr.py 303 304 305 def get_rain_types ( self ) -> List [ str ]: \"\"\"\u83b7\u53d6\u652f\u6301\u7684\u566a\u58f0\u7c7b\u578b\u5217\u8868\"\"\" return self . RAIN_TYPES . copy ()","title":"get_rain_types"},{"location":"mkdocs/dataset.html#datasets.hsi_dehaze.HSIDehazeDataset","text":"Bases: Dataset Multi-level noise HSI data set based on HDF5 storage Parameters: gt_path \u2013 gt data directory lq_path \u2013 lq data directory loader_type \u2013 type of loader dataset_name \u2013 name of the dataset pipelines \u2013 data preprocessing pipeline Source code in uchiha\\datasets\\hsi_dehaze.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 @DATASET . register_module () class HSIDehazeDataset ( Dataset ): \"\"\"Multi-level noise HSI data set based on HDF5 storage Args: gt_path: gt data directory lq_path: lq data directory loader_type: type of loader dataset_name: name of the dataset pipelines: data preprocessing pipeline \"\"\" def __init__ ( self , gt_path = None , lq_path = None , loader_type = 'tiff' , dataset_name = 'HD' , pipelines = None ): super ( HSIDehazeDataset , self ) . __init__ () self . loader = partial ( get_loader , loader_type ) self . gt_path = glob . glob ( gt_path ) self . lq_path = glob . glob ( lq_path ) self . dataset_name = dataset_name self . pipelines = Compose ( pipelines ) if pipelines else None def __getitem__ ( self , index ): if self . dataset_name == 'HD' : lq , gt = load_hd ( self . loader , self . lq_path [ index ]) elif self . dataset_name == 'AVIRIS' : lq , gt = load_mat ( self . loader , self . gt_path [ index ], self . lq_path [ index ]) elif self . dataset_name == 'UAV' : lq , gt = load_mat ( self . loader , self . gt_path [ index ], self . lq_path [ index ]) else : raise ValueError ( 'Invalid dataset name' ) results = { 'sample' : lq , 'target' : gt , 'index' : index , } return self . pipelines ( results ) if self . pipelines else results def __len__ ( self ): return len ( self . lq_path ) def evaluate ( self , preds : List [ np . ndarray ], targets : List [ np . ndarray ], metric , indexes : List [ int ]) -> dict : \"\"\" Calculates the similarity between the predicted data and the real data. PSNR and SSIM are supported by default. Args: preds: Model output list, each element shape is (B, C, H, W) targets: A list of real labels, each element of shape (B, C, H, W) metric: metric to be calculated. Default value: ['PSNR','SSIM'] indexes: raw data index list Returns: dict: dictionary with average psnr and ssim \"\"\" assert len ( preds ) == len ( targets ) == len ( indexes ), \"input list length must be the same\" assert metric is None or metric . upper () == 'PSNR' or metric . upper () == 'SSIM' , \\ 'only psnr and ssim are supported by default' logger = get_root_logger () logger . info ( 'start evaluating...' ) psnrs = [] ssims = [] for pred_batch , target_batch , index_batch in zip ( preds , targets , indexes ): B , C , H , W = pred_batch . shape for i in range ( B ): pred = pred_batch [ i ] # (C,H,W) target = target_batch [ i ] #convert to (H W C) format if C == 1 : pred_img = pred [ 0 ] target_img = target [ 0 ] else : pred_img = np . transpose ( pred , ( 1 , 2 , 0 )) target_img = np . transpose ( target , ( 1 , 2 , 0 )) # calculate psnr data_range = target_img . max () - target_img . min () psnr_val = psnr ( target_img , pred_img , data_range = data_range ) # calculate ssim multichannel = C > 1 ssim_val = ssim ( target_img , pred_img , multichannel = multichannel , channel_axis = 2 if multichannel else None , data_range = data_range ) psnrs . append ( psnr_val ) ssims . append ( ssim_val ) mean_psnr = np . mean ( psnrs ) mean_ssim = np . mean ( ssims ) logger . info ( f \"Mean PSNR: { mean_psnr : .2f } dB\" ) logger . info ( f \"Mean SSIM: { mean_ssim : .4f } \" ) return { 'psnr' : mean_psnr , 'ssim' : mean_ssim }","title":"HSIDehazeDataset"},{"location":"mkdocs/dataset.html#datasets.hsi_dehaze.HSIDehazeDataset.evaluate","text":"Calculates the similarity between the predicted data and the real data. PSNR and SSIM are supported by default. Parameters: preds ( List [ ndarray ] ) \u2013 Model output list, each element shape is (B, C, H, W) targets ( List [ ndarray ] ) \u2013 A list of real labels, each element of shape (B, C, H, W) metric \u2013 metric to be calculated. Default value: ['PSNR','SSIM'] indexes ( List [ int ] ) \u2013 raw data index list Returns: dict ( dict ) \u2013 dictionary with average psnr and ssim Source code in uchiha\\datasets\\hsi_dehaze.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def evaluate ( self , preds : List [ np . ndarray ], targets : List [ np . ndarray ], metric , indexes : List [ int ]) -> dict : \"\"\" Calculates the similarity between the predicted data and the real data. PSNR and SSIM are supported by default. Args: preds: Model output list, each element shape is (B, C, H, W) targets: A list of real labels, each element of shape (B, C, H, W) metric: metric to be calculated. Default value: ['PSNR','SSIM'] indexes: raw data index list Returns: dict: dictionary with average psnr and ssim \"\"\" assert len ( preds ) == len ( targets ) == len ( indexes ), \"input list length must be the same\" assert metric is None or metric . upper () == 'PSNR' or metric . upper () == 'SSIM' , \\ 'only psnr and ssim are supported by default' logger = get_root_logger () logger . info ( 'start evaluating...' ) psnrs = [] ssims = [] for pred_batch , target_batch , index_batch in zip ( preds , targets , indexes ): B , C , H , W = pred_batch . shape for i in range ( B ): pred = pred_batch [ i ] # (C,H,W) target = target_batch [ i ] #convert to (H W C) format if C == 1 : pred_img = pred [ 0 ] target_img = target [ 0 ] else : pred_img = np . transpose ( pred , ( 1 , 2 , 0 )) target_img = np . transpose ( target , ( 1 , 2 , 0 )) # calculate psnr data_range = target_img . max () - target_img . min () psnr_val = psnr ( target_img , pred_img , data_range = data_range ) # calculate ssim multichannel = C > 1 ssim_val = ssim ( target_img , pred_img , multichannel = multichannel , channel_axis = 2 if multichannel else None , data_range = data_range ) psnrs . append ( psnr_val ) ssims . append ( ssim_val ) mean_psnr = np . mean ( psnrs ) mean_ssim = np . mean ( ssims ) logger . info ( f \"Mean PSNR: { mean_psnr : .2f } dB\" ) logger . info ( f \"Mean SSIM: { mean_ssim : .4f } \" ) return { 'psnr' : mean_psnr , 'ssim' : mean_ssim }","title":"evaluate"},{"location":"mkdocs/introduction.html","text":"Introduction \ud83c\udf1f \u6df1\u5ea6\u5b66\u4e60\u7b80\u6613\u5de5\u5177\u7bb1 | Uchiha DIY your model! \u5feb\u901f\u642d\u5efa\u3001\u81ea\u7531\u914d\u7f6e\u591a\u79cd\u7ecf\u5178\u4e0e\u521b\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7ed3\u6784\u3002 \ud83d\udcda \u76ee\u5f55 \u7b80\u4ecb Demo \u5b89\u88c5 \u4f7f\u7528 \u6570\u636e\u96c6 \u66f4\u591a\u6587\u6863 \ud83d\udcd6 \u7b80\u4ecb \u901a\u8fc7 \u914d\u7f6e\u6587\u4ef6 \uff0c\u6765\u81ea\u5b9a\u4e49\u4f60\u7684\u6a21\u578b\uff01 \u652f\u6301\u591a\u79cd\u57fa\u7840\u6a21\u578b\u7ed3\u6784\uff1a - \u4e32\u884c\u7ed3\u6784\uff08\u5982\u7ecf\u5178\u7684\u6df1\u5ea6 CNN\uff09 - \u5e76\u884c\u7ed3\u6784 - \u77e5\u540d\u7684 U-Net \u7ed3\u6784 \u7b49\u7b49 \ud83d\ude80 Demo python main.py --config ${ config file } \ud83d\udc49 \u914d\u7f6e\u6587\u4ef6\u8be6\u7ec6\u7528\u6cd5\uff1a \u914d\u7f6e\u8bb2\u89e3 \u2699\ufe0f \u5b89\u88c5 1\ufe0f\u20e3 \u514b\u9686\u4ed3\u5e93 git clone https://github.com/zhouruii/uchiha.git cd uchiha-main 2\ufe0f\u20e3 \u521b\u5efa\u865a\u62df\u73af\u5883 conda create -n your_env_name python = 3 .9 conda activate your_env_name 3\ufe0f\u20e3 \u5b89\u88c5 PyTorch \u26a0\ufe0f \u8bf7\u6839\u636e\u4f60\u7684 CUDA \u7248\u672c\u9009\u62e9\u5408\u9002\u7684 PyTorch\uff0c\u53c2\u8003 PyTorch \u5b98\u7f51 pip install torch == 1 .8.2 torchvision == 0 .9.2 torchaudio == 0 .8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111 4\ufe0f\u20e3 \u5b89\u88c5\u4f9d\u8d56 pip install -r requirements.txt \ud83c\udfc3\u200d\u2642\ufe0f \u4f7f\u7528 \u5e38\u7528\u53c2\u6570 \u53c2\u6570 \u63cf\u8ff0 --seed \u968f\u673a\u6570\u79cd\u5b50 --config \u8bad\u7ec3\u7a0b\u5e8f\u7684\u914d\u7f6e\u6587\u4ef6\uff08\u6838\u5fc3\uff09 --gpu_ids \u663e\u5361 ID\uff0c\u652f\u6301\u591a\u5361 --analyze_params \u53c2\u6570\u5206\u6790\u6df1\u5ea6\uff080=\u603b\u53c2\u6570\u91cf\uff09 \ud83d\udcc4 \u66f4\u591a\u8be6\u60c5\uff1a \u914d\u7f6e\u8bf4\u660e \ud83d\udea6 \u8bad\u7ec3\u793a\u4f8b python main.py --config ${ config file } \u591a\u5361\u8bad\u7ec3 + \u53c2\u6570\u9012\u5f52\u5206\u6790\uff1a python main.py --analyze_params 3 --gpu_ids 0 1 2 3 --config configs/baseline/Restormer.yaml \ud83d\udd0d \u6d4b\u8bd5\u793a\u4f8b python test.py --config ${ config file } --checkpoint ${ checkpoint file } \u793a\u4f8b\uff1a python test.py --config configs/baseline/Restormer.yaml --checkpoint your_checkpoint \ud83d\udcc2 \u6570\u636e\u96c6 \u6570\u636e\u96c6\u51c6\u5907\u4e0e\u7ec4\u7ec7\u8bf7\u53c2\u8003 \u6570\u636e\u51c6\u5907 \ud83d\udd17 \u66f4\u591a\u6587\u6863 \u914d\u7f6e\u6587\u4ef6\u8bf4\u660e \u6570\u636e\u96c6\u51c6\u5907 Have fun & happy training! \ud83d\ude80","title":"Introduction"},{"location":"mkdocs/introduction.html#introduction","text":"","title":"Introduction"},{"location":"mkdocs/introduction.html#uchiha","text":"DIY your model! \u5feb\u901f\u642d\u5efa\u3001\u81ea\u7531\u914d\u7f6e\u591a\u79cd\u7ecf\u5178\u4e0e\u521b\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7ed3\u6784\u3002","title":"\ud83c\udf1f \u6df1\u5ea6\u5b66\u4e60\u7b80\u6613\u5de5\u5177\u7bb1 | Uchiha"},{"location":"mkdocs/introduction.html#_1","text":"\u7b80\u4ecb Demo \u5b89\u88c5 \u4f7f\u7528 \u6570\u636e\u96c6 \u66f4\u591a\u6587\u6863","title":"\ud83d\udcda \u76ee\u5f55"},{"location":"mkdocs/introduction.html#_2","text":"\u901a\u8fc7 \u914d\u7f6e\u6587\u4ef6 \uff0c\u6765\u81ea\u5b9a\u4e49\u4f60\u7684\u6a21\u578b\uff01 \u652f\u6301\u591a\u79cd\u57fa\u7840\u6a21\u578b\u7ed3\u6784\uff1a - \u4e32\u884c\u7ed3\u6784\uff08\u5982\u7ecf\u5178\u7684\u6df1\u5ea6 CNN\uff09 - \u5e76\u884c\u7ed3\u6784 - \u77e5\u540d\u7684 U-Net \u7ed3\u6784 \u7b49\u7b49","title":"\ud83d\udcd6 \u7b80\u4ecb"},{"location":"mkdocs/introduction.html#demo","text":"python main.py --config ${ config file } \ud83d\udc49 \u914d\u7f6e\u6587\u4ef6\u8be6\u7ec6\u7528\u6cd5\uff1a \u914d\u7f6e\u8bb2\u89e3","title":"\ud83d\ude80 Demo"},{"location":"mkdocs/introduction.html#_3","text":"","title":"\u2699\ufe0f \u5b89\u88c5"},{"location":"mkdocs/introduction.html#1","text":"git clone https://github.com/zhouruii/uchiha.git cd uchiha-main","title":"1\ufe0f\u20e3 \u514b\u9686\u4ed3\u5e93"},{"location":"mkdocs/introduction.html#2","text":"conda create -n your_env_name python = 3 .9 conda activate your_env_name","title":"2\ufe0f\u20e3 \u521b\u5efa\u865a\u62df\u73af\u5883"},{"location":"mkdocs/introduction.html#3-pytorch","text":"\u26a0\ufe0f \u8bf7\u6839\u636e\u4f60\u7684 CUDA \u7248\u672c\u9009\u62e9\u5408\u9002\u7684 PyTorch\uff0c\u53c2\u8003 PyTorch \u5b98\u7f51 pip install torch == 1 .8.2 torchvision == 0 .9.2 torchaudio == 0 .8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111","title":"3\ufe0f\u20e3 \u5b89\u88c5 PyTorch"},{"location":"mkdocs/introduction.html#4","text":"pip install -r requirements.txt","title":"4\ufe0f\u20e3 \u5b89\u88c5\u4f9d\u8d56"},{"location":"mkdocs/introduction.html#_4","text":"","title":"\ud83c\udfc3\u200d\u2642\ufe0f \u4f7f\u7528"},{"location":"mkdocs/introduction.html#_5","text":"\u53c2\u6570 \u63cf\u8ff0 --seed \u968f\u673a\u6570\u79cd\u5b50 --config \u8bad\u7ec3\u7a0b\u5e8f\u7684\u914d\u7f6e\u6587\u4ef6\uff08\u6838\u5fc3\uff09 --gpu_ids \u663e\u5361 ID\uff0c\u652f\u6301\u591a\u5361 --analyze_params \u53c2\u6570\u5206\u6790\u6df1\u5ea6\uff080=\u603b\u53c2\u6570\u91cf\uff09 \ud83d\udcc4 \u66f4\u591a\u8be6\u60c5\uff1a \u914d\u7f6e\u8bf4\u660e","title":"\u5e38\u7528\u53c2\u6570"},{"location":"mkdocs/introduction.html#_6","text":"python main.py --config ${ config file } \u591a\u5361\u8bad\u7ec3 + \u53c2\u6570\u9012\u5f52\u5206\u6790\uff1a python main.py --analyze_params 3 --gpu_ids 0 1 2 3 --config configs/baseline/Restormer.yaml","title":"\ud83d\udea6 \u8bad\u7ec3\u793a\u4f8b"},{"location":"mkdocs/introduction.html#_7","text":"python test.py --config ${ config file } --checkpoint ${ checkpoint file } \u793a\u4f8b\uff1a python test.py --config configs/baseline/Restormer.yaml --checkpoint your_checkpoint","title":"\ud83d\udd0d \u6d4b\u8bd5\u793a\u4f8b"},{"location":"mkdocs/introduction.html#_8","text":"\u6570\u636e\u96c6\u51c6\u5907\u4e0e\u7ec4\u7ec7\u8bf7\u53c2\u8003 \u6570\u636e\u51c6\u5907","title":"\ud83d\udcc2 \u6570\u636e\u96c6"},{"location":"mkdocs/introduction.html#_9","text":"\u914d\u7f6e\u6587\u4ef6\u8bf4\u660e \u6570\u636e\u96c6\u51c6\u5907 Have fun & happy training! \ud83d\ude80","title":"\ud83d\udd17 \u66f4\u591a\u6587\u6863"},{"location":"mkdocs/model.html","text":"Models Stack Stack Bases: Module Stacked networks Parameters: preprocessor ( dict , default: None ) \u2013 Config information for building the preprocessor. Default: None. stacks ( List [ dict ] , default: None ) \u2013 The list containing config information for building stacks. Default: None postprocessor ( dict , default: None ) \u2013 Config information for building the postprocessor. Default: None. Source code in uchiha\\models\\stack\\base.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @MODEL . register_module () class Stack ( nn . Module ): \"\"\" Stacked networks Args: preprocessor (dict): Config information for building the preprocessor. Default: None. stacks (List[dict]): The list containing config information for building stacks. Default: None postprocessor (dict): Config information for building the postprocessor. Default: None. \"\"\" def __init__ ( self , preprocessor = None , stacks = None , postprocessor = None ): super () . __init__ () self . preprocessor : nn . Module = build_module ( preprocessor ) self . stacks : nn . ModuleList = _build_stacks ( stacks ) self . postprocessor : nn . Module = build_module ( postprocessor ) def forward ( self , x ): # preprocess out = self . preprocessor ( x ) if self . preprocessor else x # stacks for module in self . stacks : out = module ( out ) # postprocessor out = self . postprocessor ( out ) if self . postprocessor else out return out SimpleViT Bases: Stack The simplest Vision Transformer Parameters: embedding ( dict , default: None ) \u2013 Config information for building the embedding. Default: None. basemodule ( dict , default: None ) \u2013 Config information for building the basemodule. Default: None. head ( dict , default: None ) \u2013 Config information for building the head. Default: None. Source code in uchiha\\models\\stack\\vit.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @MODEL . register_module () class SimpleViT ( Stack ): \"\"\" The simplest Vision Transformer Args: embedding (dict): Config information for building the embedding. Default: None. basemodule (dict): Config information for building the basemodule. Default: None. head (dict): Config information for building the head. Default: None. \"\"\" def __init__ ( self , embedding = None , basemodule = None , head = None ): super () . __init__ ( stacks = [{ 'embedding' : embedding }, { 'basemodule' : basemodule }, { 'head' : head }]) self . embedding : nn . Module = self . stacks [ 0 ] self . basemodule : nn . Module = self . stacks [ 1 ] self . head : nn . Module = self . stacks [ 2 ] def forward_features ( self , x ): # embedding out = self . embedding ( x ) # Transformer out = self . basemodule ( out ) return out def forward ( self , x ): out = self . forward_features ( x ) out = self . head ( out ) return out SwinTransformer Bases: Stack Swin-Transformer Network Parameters: embedding ( dict , default: None ) \u2013 Config information for building the embedding. Default: None. basemodule ( dict , default: None ) \u2013 Config information for building the basemodule. Default: None. ape ( bool , default: False ) \u2013 Whether to use absolute position encoding. Default: False. head ( dict , default: None ) \u2013 Config information for building the head. Default: None. Source code in uchiha\\models\\stack\\swin_transformer.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 @MODEL . register_module () class SwinTransformer ( Stack ): \"\"\" Swin-Transformer Network Args: embedding (dict): Config information for building the embedding. Default: None. basemodule (dict): Config information for building the basemodule. Default: None. ape (bool): Whether to use absolute position encoding. Default: False. head (dict): Config information for building the head. Default: None. \"\"\" def __init__ ( self , embedding = None , ape = False , basemodule = None , head = None ): basemodule = strings_to_list ( basemodule ) super () . __init__ ( stacks = [{ 'embedding' : embedding }, { 'basemodule' : basemodule }, { 'head' : head }]) self . embedding : nn . Module = self . stacks [ 0 ] self . basemodule : nn . Module = self . stacks [ 1 ] self . head : nn . Module = self . stacks [ 2 ] # absolute position embedding self . ape = ape num_patches = self . embedding . num_patches embed_dim = self . embedding . embed_dim drop_rate = basemodule . get ( 'drop_rate' ) if self . ape : self . absolute_pos_embed = nn . Parameter ( torch . zeros ( 1 , num_patches , embed_dim )) trunc_normal_ ( self . absolute_pos_embed , std = .02 ) self . pos_drop = nn . Dropout ( p = drop_rate ) if hasattr ( self . basemodule , 'layers' ): self . layers = self . basemodule . layers self . num_layers = len ( self . layers ) else : self . layers = nn . ModuleList ([ self . basemodule ]) self . num_layers = 1 self . num_features = int ( embed_dim * 2 ** ( self . num_layers - 1 )) norm_layer = build_norm ( basemodule . get ( 'norm_layer' )) self . norm = norm_layer ( self . num_features ) self . apply ( self . _init_weights ) def forward_features ( self , x ): # embedding out = self . embedding ( x ) # ape if self . ape : out = out + self . absolute_pos_embed out = self . pos_drop ( out ) # core for layer in self . layers : out = layer ( out ) # norm out = self . norm ( out ) return out def forward ( self , x ): out = self . forward_features ( x ) if self . head : out = self . head ( out ) return out def _init_weights ( self , m ): if isinstance ( m , nn . Linear ): trunc_normal_ ( m . weight , std = .02 ) if isinstance ( m , nn . Linear ) and m . bias is not None : nn . init . constant_ ( m . bias , 0 ) elif isinstance ( m , nn . LayerNorm ): nn . init . constant_ ( m . bias , 0 ) nn . init . constant_ ( m . weight , 1.0 ) @torch . jit . ignore def no_weight_decay ( self ): return { 'absolute_pos_embed' } @torch . jit . ignore def no_weight_decay_keywords ( self ): return { 'relative_position_bias_table' } def flops ( self ): patches_resolution = self . embedding . patches_resolution num_classes = self . head . pred_num flops = 0 flops += self . embedding . flops () for i , layer in enumerate ( self . layers ): flops += layer . flops () flops += self . num_features * patches_resolution [ 0 ] * patches_resolution [ 1 ] // ( 2 ** self . num_layers ) flops += self . num_features * num_classes return flops ChannelTransformer Bases: Stack Channel Transformer Network Parameters: embedding ( dict , default: None ) \u2013 Config information for building the embedding. Default: None. basemodule ( dict , default: None ) \u2013 Config information for building the basemodule. Default: None. head ( dict , default: None ) \u2013 Config information for building the head. Default: None. Source code in uchiha\\models\\stack\\channel_transformer.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @MODEL . register_module () class ChannelTransformer ( Stack ): \"\"\" Channel Transformer Network Args: embedding (dict): Config information for building the embedding. Default: None. basemodule (dict): Config information for building the basemodule. Default: None. head (dict): Config information for building the head. Default: None. \"\"\" def __init__ ( self , embedding = None , basemodule = None , head = None ): basemodule = strings_to_list ( basemodule ) super () . __init__ ( stacks = [{ 'embedding' : embedding }, { 'basemodule' : basemodule }, { 'head' : head }]) self . embedding : nn . Module = self . stacks [ 0 ] self . layers : Union [ nn . ModuleList , nn . Module ] = self . stacks [ 1 ] self . head : nn . Module = self . stacks [ 2 ] if not isinstance ( self . layers , nn . ModuleList ): self . layers = nn . ModuleList ([ self . layers ]) def forward_features ( self , x ): # embedding out = self . embedding ( x ) # core for layer in self . layers : out = layer ( out ) return out def forward ( self , x ): out = self . forward_features ( x ) if self . head : out = self . head ( out ) return out Parallel Parallel Bases: Module parallel pipeline work It is divided into three parts, first preprocessing the data, then sending it to multiple pipelines (each pipeline is a Stack operation), and finally using post-processing to integrate the output of multiple pipelines. Parameters: preprocessor ( dict , default: None ) \u2013 Config information for building the preprocessor parallels ( List [ dict ] , default: None ) \u2013 A list of config information for building the parallel, each config information represents a Stack postprocessor ( dict , default: None ) \u2013 Config information for building the postprocessor Source code in uchiha\\models\\parallel\\base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @MODEL . register_module () class Parallel ( nn . Module ): \"\"\" parallel pipeline work It is divided into three parts, first preprocessing the data, then sending it to multiple pipelines (each pipeline is a `Stack` operation), and finally using post-processing to integrate the output of multiple pipelines. Args: preprocessor (dict): Config information for building the preprocessor parallels (List[dict]): A list of config information for building the parallel, each config information represents a `Stack` postprocessor (dict): Config information for building the postprocessor \"\"\" def __init__ ( self , preprocessor = None , parallels = None , postprocessor = None ): super () . __init__ () self . preprocessor : nn . Module = build_module ( preprocessor ) self . workflows = nn . ModuleList () for workflow in parallels : self . workflows . append ( build_model ( workflow )) self . postprocessor : nn . Module = build_module ( postprocessor ) def forward ( self , x ): if self . preprocessor : x_parallel = self . preprocessor ( x ) else : x_parallel = x # core y_parallel = [] for idx , x in enumerate ( x_parallel ): y = self . workflows [ idx ]( x ) y_parallel . append ( y ) if self . postprocessor : out = self . postprocessor ( y_parallel ) else : out = y_parallel return out","title":"Model"},{"location":"mkdocs/model.html#models","text":"","title":"Models"},{"location":"mkdocs/model.html#stack","text":"","title":"Stack"},{"location":"mkdocs/model.html#models.stack.base.Stack","text":"Bases: Module Stacked networks Parameters: preprocessor ( dict , default: None ) \u2013 Config information for building the preprocessor. Default: None. stacks ( List [ dict ] , default: None ) \u2013 The list containing config information for building stacks. Default: None postprocessor ( dict , default: None ) \u2013 Config information for building the postprocessor. Default: None. Source code in uchiha\\models\\stack\\base.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @MODEL . register_module () class Stack ( nn . Module ): \"\"\" Stacked networks Args: preprocessor (dict): Config information for building the preprocessor. Default: None. stacks (List[dict]): The list containing config information for building stacks. Default: None postprocessor (dict): Config information for building the postprocessor. Default: None. \"\"\" def __init__ ( self , preprocessor = None , stacks = None , postprocessor = None ): super () . __init__ () self . preprocessor : nn . Module = build_module ( preprocessor ) self . stacks : nn . ModuleList = _build_stacks ( stacks ) self . postprocessor : nn . Module = build_module ( postprocessor ) def forward ( self , x ): # preprocess out = self . preprocessor ( x ) if self . preprocessor else x # stacks for module in self . stacks : out = module ( out ) # postprocessor out = self . postprocessor ( out ) if self . postprocessor else out return out","title":"Stack"},{"location":"mkdocs/model.html#models.stack.vit.SimpleViT","text":"Bases: Stack The simplest Vision Transformer Parameters: embedding ( dict , default: None ) \u2013 Config information for building the embedding. Default: None. basemodule ( dict , default: None ) \u2013 Config information for building the basemodule. Default: None. head ( dict , default: None ) \u2013 Config information for building the head. Default: None. Source code in uchiha\\models\\stack\\vit.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @MODEL . register_module () class SimpleViT ( Stack ): \"\"\" The simplest Vision Transformer Args: embedding (dict): Config information for building the embedding. Default: None. basemodule (dict): Config information for building the basemodule. Default: None. head (dict): Config information for building the head. Default: None. \"\"\" def __init__ ( self , embedding = None , basemodule = None , head = None ): super () . __init__ ( stacks = [{ 'embedding' : embedding }, { 'basemodule' : basemodule }, { 'head' : head }]) self . embedding : nn . Module = self . stacks [ 0 ] self . basemodule : nn . Module = self . stacks [ 1 ] self . head : nn . Module = self . stacks [ 2 ] def forward_features ( self , x ): # embedding out = self . embedding ( x ) # Transformer out = self . basemodule ( out ) return out def forward ( self , x ): out = self . forward_features ( x ) out = self . head ( out ) return out","title":"SimpleViT"},{"location":"mkdocs/model.html#models.stack.swin_transformer.SwinTransformer","text":"Bases: Stack Swin-Transformer Network Parameters: embedding ( dict , default: None ) \u2013 Config information for building the embedding. Default: None. basemodule ( dict , default: None ) \u2013 Config information for building the basemodule. Default: None. ape ( bool , default: False ) \u2013 Whether to use absolute position encoding. Default: False. head ( dict , default: None ) \u2013 Config information for building the head. Default: None. Source code in uchiha\\models\\stack\\swin_transformer.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 @MODEL . register_module () class SwinTransformer ( Stack ): \"\"\" Swin-Transformer Network Args: embedding (dict): Config information for building the embedding. Default: None. basemodule (dict): Config information for building the basemodule. Default: None. ape (bool): Whether to use absolute position encoding. Default: False. head (dict): Config information for building the head. Default: None. \"\"\" def __init__ ( self , embedding = None , ape = False , basemodule = None , head = None ): basemodule = strings_to_list ( basemodule ) super () . __init__ ( stacks = [{ 'embedding' : embedding }, { 'basemodule' : basemodule }, { 'head' : head }]) self . embedding : nn . Module = self . stacks [ 0 ] self . basemodule : nn . Module = self . stacks [ 1 ] self . head : nn . Module = self . stacks [ 2 ] # absolute position embedding self . ape = ape num_patches = self . embedding . num_patches embed_dim = self . embedding . embed_dim drop_rate = basemodule . get ( 'drop_rate' ) if self . ape : self . absolute_pos_embed = nn . Parameter ( torch . zeros ( 1 , num_patches , embed_dim )) trunc_normal_ ( self . absolute_pos_embed , std = .02 ) self . pos_drop = nn . Dropout ( p = drop_rate ) if hasattr ( self . basemodule , 'layers' ): self . layers = self . basemodule . layers self . num_layers = len ( self . layers ) else : self . layers = nn . ModuleList ([ self . basemodule ]) self . num_layers = 1 self . num_features = int ( embed_dim * 2 ** ( self . num_layers - 1 )) norm_layer = build_norm ( basemodule . get ( 'norm_layer' )) self . norm = norm_layer ( self . num_features ) self . apply ( self . _init_weights ) def forward_features ( self , x ): # embedding out = self . embedding ( x ) # ape if self . ape : out = out + self . absolute_pos_embed out = self . pos_drop ( out ) # core for layer in self . layers : out = layer ( out ) # norm out = self . norm ( out ) return out def forward ( self , x ): out = self . forward_features ( x ) if self . head : out = self . head ( out ) return out def _init_weights ( self , m ): if isinstance ( m , nn . Linear ): trunc_normal_ ( m . weight , std = .02 ) if isinstance ( m , nn . Linear ) and m . bias is not None : nn . init . constant_ ( m . bias , 0 ) elif isinstance ( m , nn . LayerNorm ): nn . init . constant_ ( m . bias , 0 ) nn . init . constant_ ( m . weight , 1.0 ) @torch . jit . ignore def no_weight_decay ( self ): return { 'absolute_pos_embed' } @torch . jit . ignore def no_weight_decay_keywords ( self ): return { 'relative_position_bias_table' } def flops ( self ): patches_resolution = self . embedding . patches_resolution num_classes = self . head . pred_num flops = 0 flops += self . embedding . flops () for i , layer in enumerate ( self . layers ): flops += layer . flops () flops += self . num_features * patches_resolution [ 0 ] * patches_resolution [ 1 ] // ( 2 ** self . num_layers ) flops += self . num_features * num_classes return flops","title":"SwinTransformer"},{"location":"mkdocs/model.html#models.stack.channel_transformer.ChannelTransformer","text":"Bases: Stack Channel Transformer Network Parameters: embedding ( dict , default: None ) \u2013 Config information for building the embedding. Default: None. basemodule ( dict , default: None ) \u2013 Config information for building the basemodule. Default: None. head ( dict , default: None ) \u2013 Config information for building the head. Default: None. Source code in uchiha\\models\\stack\\channel_transformer.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @MODEL . register_module () class ChannelTransformer ( Stack ): \"\"\" Channel Transformer Network Args: embedding (dict): Config information for building the embedding. Default: None. basemodule (dict): Config information for building the basemodule. Default: None. head (dict): Config information for building the head. Default: None. \"\"\" def __init__ ( self , embedding = None , basemodule = None , head = None ): basemodule = strings_to_list ( basemodule ) super () . __init__ ( stacks = [{ 'embedding' : embedding }, { 'basemodule' : basemodule }, { 'head' : head }]) self . embedding : nn . Module = self . stacks [ 0 ] self . layers : Union [ nn . ModuleList , nn . Module ] = self . stacks [ 1 ] self . head : nn . Module = self . stacks [ 2 ] if not isinstance ( self . layers , nn . ModuleList ): self . layers = nn . ModuleList ([ self . layers ]) def forward_features ( self , x ): # embedding out = self . embedding ( x ) # core for layer in self . layers : out = layer ( out ) return out def forward ( self , x ): out = self . forward_features ( x ) if self . head : out = self . head ( out ) return out","title":"ChannelTransformer"},{"location":"mkdocs/model.html#parallel","text":"","title":"Parallel"},{"location":"mkdocs/model.html#models.parallel.base.Parallel","text":"Bases: Module parallel pipeline work It is divided into three parts, first preprocessing the data, then sending it to multiple pipelines (each pipeline is a Stack operation), and finally using post-processing to integrate the output of multiple pipelines. Parameters: preprocessor ( dict , default: None ) \u2013 Config information for building the preprocessor parallels ( List [ dict ] , default: None ) \u2013 A list of config information for building the parallel, each config information represents a Stack postprocessor ( dict , default: None ) \u2013 Config information for building the postprocessor Source code in uchiha\\models\\parallel\\base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @MODEL . register_module () class Parallel ( nn . Module ): \"\"\" parallel pipeline work It is divided into three parts, first preprocessing the data, then sending it to multiple pipelines (each pipeline is a `Stack` operation), and finally using post-processing to integrate the output of multiple pipelines. Args: preprocessor (dict): Config information for building the preprocessor parallels (List[dict]): A list of config information for building the parallel, each config information represents a `Stack` postprocessor (dict): Config information for building the postprocessor \"\"\" def __init__ ( self , preprocessor = None , parallels = None , postprocessor = None ): super () . __init__ () self . preprocessor : nn . Module = build_module ( preprocessor ) self . workflows = nn . ModuleList () for workflow in parallels : self . workflows . append ( build_model ( workflow )) self . postprocessor : nn . Module = build_module ( postprocessor ) def forward ( self , x ): if self . preprocessor : x_parallel = self . preprocessor ( x ) else : x_parallel = x # core y_parallel = [] for idx , x in enumerate ( x_parallel ): y = self . workflows [ idx ]( x ) y_parallel . append ( y ) if self . postprocessor : out = self . postprocessor ( y_parallel ) else : out = y_parallel return out","title":"Parallel"},{"location":"mkdocs/module.html","text":"Modules Atom Downsample DownsampleConv Bases: Module Conv with stride > 1 to downsample image Parameters: in_channel ( int , default: 512 ) \u2013 Number of input channels. Default: 512 out_channel ( int , default: 1024 ) \u2013 Number of output channels. Default: 1024 kernel_size ( int , default: 3 ) \u2013 Kernel size for Conv . Default: 3 stride ( int , default: 2 ) \u2013 Stride for Conv . Default: 2 padding ( int , default: 1 ) \u2013 Padding for Conv . Default: 1 Source code in uchiha\\models\\modules\\downsample.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @MODULE . register_module () class DownsampleConv ( nn . Module ): \"\"\" `Conv` with stride > 1 to downsample image Args: in_channel (int): Number of input channels. Default: 512 out_channel (int): Number of output channels. Default: 1024 kernel_size (int): Kernel size for `Conv`. Default: 3 stride (int): Stride for `Conv`. Default: 2 padding (int): Padding for `Conv`. Default: 1 \"\"\" def __init__ ( self , in_channel = 512 , out_channel = 1024 , kernel_size = 3 , stride = 2 , padding = 1 ): super () . __init__ () self . downsample = nn . Conv2d ( in_channel , out_channel , kernel_size , stride , padding ) self . activate = nn . GELU () def forward ( self , x ): if len ( x . shape ) == 3 : B , L , C = x . shape H , W = int ( math . sqrt ( L )), int ( math . sqrt ( L )) x = x . view ( B , H , W , C ) . permute ( 0 , 3 , 1 , 2 ) out = self . activate ( self . downsample ( x )) return out . flatten ( 2 ) . transpose ( 1 , 2 ) else : return self . activate ( self . downsample ( x )) PatchMerging Bases: Module Patch Merging Layer. Parameters: input_resolution ( tuple [ int ] ) \u2013 Resolution of input feature. in_channel ( int ) \u2013 Number of input channels. norm_layer ( Module , default: LayerNorm ) \u2013 Normalization layer. Default: nn.LayerNorm Source code in uchiha\\models\\modules\\downsample.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 @MODULE . register_module () class PatchMerging ( nn . Module ): r \"\"\" Patch Merging Layer. Args: input_resolution (tuple[int]): Resolution of input feature. in_channel (int): Number of input channels. norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm \"\"\" def __init__ ( self , input_resolution , in_channel , norm_layer = nn . LayerNorm ): super () . __init__ () self . input_resolution = to_2tuple ( input_resolution ) self . dim = in_channel self . reduction = nn . Linear ( 4 * in_channel , 2 * in_channel , bias = False ) self . norm = norm_layer ( 4 * in_channel ) def forward ( self , x ): H , W = self . input_resolution B , L , C = x . shape assert L == H * W , \"input feature has wrong size\" assert H % 2 == 0 and W % 2 == 0 , f \"x size ( { H } * { W } ) are not even.\" x = x . view ( B , H , W , C ) x0 = x [:, 0 :: 2 , 0 :: 2 , :] # B H/2 W/2 C x1 = x [:, 1 :: 2 , 0 :: 2 , :] # B H/2 W/2 C x2 = x [:, 0 :: 2 , 1 :: 2 , :] # B H/2 W/2 C x3 = x [:, 1 :: 2 , 1 :: 2 , :] # B H/2 W/2 C x = torch . cat ([ x0 , x1 , x2 , x3 ], - 1 ) # B H/2 W/2 4*C x = x . view ( B , - 1 , 4 * C ) # B H/2*W/2 4*C x = self . norm ( x ) x = self . reduction ( x ) return x def extra_repr ( self ) -> str : return f \"dim= { self . dim } , input_resolution= { self . input_resolution } , depth= { self . depth } \" def flops ( self ): flops = 0 for blk in self . blocks : flops += blk . flops () if self . downsample is not None : flops += self . downsample . flops () return flops PixelUnShuffle Bases: Module PixelUnShuffle to image and sequence After PixelUnShuffle , a Conv / Linear layer is used to map the data to the output dimension. Parameters: factor ( int , default: 2 ) \u2013 downsample factor in_channel ( int , default: 512 ) \u2013 Number of input channels. out_channel ( int , default: 1024 ) \u2013 Number of output channels. Source code in uchiha\\models\\modules\\downsample.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @MODULE . register_module () class PixelUnShuffle ( nn . Module ): \"\"\" `PixelUnShuffle` to image and sequence After `PixelUnShuffle`, a `Conv`/`Linear` layer is used to map the data to the output dimension. Args: factor (int): downsample factor in_channel (int): Number of input channels. out_channel (int): Number of output channels. \"\"\" def __init__ ( self , factor = 2 , in_channel = 512 , out_channel = 1024 ): super () . __init__ () self . downsample = nn . PixelUnshuffle ( downscale_factor = factor ) self . fc = nn . Linear ( in_channel * 4 , out_channel ) self . norm = nn . LayerNorm ( out_channel ) def forward ( self , x ): B , L , C = x . shape H , W = int ( math . sqrt ( L )), int ( math . sqrt ( L )) downsample = self . downsample ( x . view ( B , C , H , W )) fc = self . fc ( downsample . flatten ( 2 ) . transpose ( 1 , 2 )) return self . norm ( fc ) Upsample PixelShuffle Bases: Module PixelShuffle to image and sequence After PixelShuffle , a Conv/Linear layer id used to map the data to the output dimension. Parameters: factor ( int , default: 2 ) \u2013 upsample factor in_channel ( int , default: 1024 ) \u2013 Number of input channels. out_channel ( int , default: 512 ) \u2013 Number of output channels. Source code in uchiha\\models\\modules\\upsample.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @MODULE . register_module () class PixelShuffle ( nn . Module ): \"\"\" `PixelShuffle` to image and sequence After `PixelShuffle`, a Conv/Linear layer id used to map the data to the output dimension. Args: factor (int): upsample factor in_channel (int): Number of input channels. out_channel (int): Number of output channels. \"\"\" def __init__ ( self , factor = 2 , in_channel = 1024 , out_channel = 512 ): super () . __init__ () self . upsample = nn . PixelShuffle ( upscale_factor = factor ) self . fc = nn . Linear ( in_channel // 4 , out_channel ) def forward ( self , x ): B , L , C = x . shape H , W = int ( math . sqrt ( L )), int ( math . sqrt ( L )) upsample = self . upsample ( x . view ( B , C , H , W )) fc = self . fc ( upsample . flatten ( 2 ) . transpose ( 1 , 2 )) return fc Embedding PatchEmbedding Bases: Module Image to Patch Embedding (B, C, H, W) --> (B, L, _C) L = (H * W) / (patch_size ** 2) _C = embed_dim Parameters: img_size ( int , default: 4 ) \u2013 Image size. Default: 4. patch_size ( int , default: 1 ) \u2013 Patch token size. Default: 1. in_channel ( int , default: 330 ) \u2013 Number of input image channels. Default: 330. embed_dim ( int , default: 512 ) \u2013 Number of Conv projection output channels. Default: 512. norm_layer ( Module , default: None ) \u2013 Normalization layer. Default: None sequence ( boolean , default: True ) \u2013 if True, output shape: (B, L, C) else (B, C, H, W) Source code in uchiha\\models\\modules\\embedding.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @MODULE . register_module () class PatchEmbedding ( nn . Module ): r \"\"\" Image to Patch Embedding (B, C, H, W) --> (B, L, _C) L = (H * W) / (patch_size ** 2) _C = embed_dim Args: img_size (int): Image size. Default: 4. patch_size (int): Patch token size. Default: 1. in_channel (int): Number of input image channels. Default: 330. embed_dim (int): Number of Conv projection output channels. Default: 512. norm_layer (nn.Module, optional): Normalization layer. Default: None sequence (boolean): if True, output shape: (B, L, C) else (B, C, H, W) \"\"\" def __init__ ( self , img_size = 4 , patch_size = 1 , in_channel = 330 , embed_dim = 512 , norm_layer = None , sequence = True ): super () . __init__ () assert img_size % patch_size == 0 , \\ f 'img_size: { img_size } cannot be divided by patch_size: { patch_size } ' self . img_size : tuple = to_2tuple ( img_size ) self . patch_size : tuple = to_2tuple ( patch_size ) patches_resolution = [ self . img_size [ 0 ] // self . patch_size [ 0 ], self . img_size [ 1 ] // self . patch_size [ 1 ]] self . patches_resolution = patches_resolution self . num_patches = patches_resolution [ 0 ] * patches_resolution [ 1 ] self . sequence = sequence self . in_channel = in_channel self . embed_dim = embed_dim self . proj = nn . Sequential ( nn . Conv2d ( in_channel , embed_dim , kernel_size = patch_size , stride = patch_size , bias = False ) ) norm_layer = build_norm ( norm_layer ) self . norm = norm_layer ( embed_dim ) def forward ( self , x ): B , C , H , W = x . shape assert H == self . img_size [ 0 ] and W == self . img_size [ 1 ], \\ f \"Input image size ( { H } * { W } ) doesn't match model ( { self . img_size [ 0 ] } * { self . img_size [ 1 ] } ).\" x = self . proj ( x ) x = x . flatten ( 2 ) . transpose ( 1 , 2 ) # B Ph*Pw C if self . norm is not None : x = self . norm ( x ) if not self . sequence : x = x . transpose ( 1 , 2 ) . view ( B , self . embed_dim , * self . patches_resolution ) return x def flops ( self ): Ho , Wo = self . patches_resolution flops = Ho * Wo * self . embed_dim * self . in_chans * ( self . patch_size [ 0 ] * self . patch_size [ 1 ]) if self . norm is not None : flops += Ho * Wo * self . embed_dim return flops TokenEmbedding Bases: Module Sequence to Patch Embedding (B, L, W) --> (B, L, _C) _C = embed_dim Parameters: in_channel ( int ) \u2013 Number of input image channels. embed_dim ( int ) \u2013 Number of linear projection output channels. norm_layer ( Module ) \u2013 Normalization layer. Source code in uchiha\\models\\modules\\embedding.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 @MODULE . register_module () class TokenEmbedding ( nn . Module ): r \"\"\" Sequence to Patch Embedding (B, L, W) --> (B, L, _C) _C = embed_dim Args: in_channel (int): Number of input image channels. embed_dim (int): Number of linear projection output channels. norm_layer (nn.Module, optional): Normalization layer. \"\"\" def __init__ ( self , in_channel , embed_dim , norm_layer ): super () . __init__ () self . in_channel = in_channel self . embed_dim = embed_dim self . proj = nn . Sequential ( nn . Linear ( in_channel , embed_dim , bias = False ), nn . GELU (), nn . Linear ( embed_dim , embed_dim , bias = False ), nn . GELU () ) if norm_layer is not None : if norm_layer == 'nn.LayerNorm' : self . norm = nn . LayerNorm ( embed_dim ) else : self . norm = norm_layer ( embed_dim ) else : self . norm = None def forward ( self , x ): if len ( x . shape ) == 4 : x = x . flatten ( 2 ) . transpose ( 1 , 2 ) # B C H W -> B L C x = self . proj ( x ) if self . norm is not None : x = self . norm ( x ) return x Head FCHead Bases: Module Fully Connected Head Parameters: embed_dim ( int ) \u2013 Number of input channels. Default: 512. pred_num ( int ) \u2013 Number of prediction mode ( str , default: 'sequence' ) \u2013 If set to 'sequence', apply predictions to (B, L, C), else (B, C, H, W) post_process ( str , default: None ) \u2013 processor after Linear Source code in uchiha\\models\\modules\\head.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @MODULE . register_module () class FCHead ( nn . Module ): \"\"\" Fully Connected Head Args: embed_dim (int): Number of input channels. Default: 512. pred_num (int): Number of prediction mode (str): If set to 'sequence', apply predictions to (B, L, C), else (B, C, H, W) post_process (str): processor after `Linear ` \"\"\" def __init__ ( self , embed_dim , pred_num , mode = 'sequence' , post_process = None ): super () . __init__ () if mode == 'sequence' : self . pooling = nn . AdaptiveAvgPool1d ( 1 ) else : self . pooling = nn . AdaptiveAvgPool2d ( 1 ) self . head = nn . Linear ( embed_dim , pred_num ) if post_process == 'RELU' : self . activate = nn . ReLU () else : self . activate = nn . Identity () def forward ( self , x ): if len ( x . shape ) == 3 : # B,L,C = x.shape pooling = self . pooling ( x . transpose ( 1 , 2 )) . squeeze ( 2 ) else : # B,C,H,W = x.shape pooling = self . pooling ( x ) . squeeze ( - 1 ) . squeeze ( - 1 ) out = self . head ( pooling ) return self . activate ( out ) Pre-process DWT1d Bases: Module 1D Discrete Wavelet Transform Parameters: scales ( int , default: 1 ) \u2013 Number of scales (number of transform performed) wave ( str , default: 'haar' ) \u2013 The type of wavelet padding ( str , default: 'zero' ) \u2013 data padding mode before transformation origin ( bool , default: False ) \u2013 Whether to return original data. Source code in uchiha\\models\\modules\\preprocessor.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @MODULE . register_module () class DWT1d ( nn . Module ): \"\"\" 1D Discrete Wavelet Transform Args: scales (int): Number of scales (number of transform performed) wave (str): The type of wavelet padding (str): data padding mode before transformation origin (bool): Whether to return original data. \"\"\" def __init__ ( self , scales = 1 , wave = 'haar' , padding = 'zero' , origin = False ): super () . __init__ () self . wavelet_transform = DWT1DForward ( J = scales , wave = wave , mode = padding ) self . origin = origin def forward ( self , x ): # x.shape:B, C, H, W if len ( x . shape ) == 4 : x = x . flatten ( 2 ) . transpose ( 1 , 2 ) L , H = self . wavelet_transform ( x ) parallel = [ sequence_to_image ( L )] for idx , h in enumerate ( H ): parallel . append ( sequence_to_image ( h )) if self . origin : return x , parallel else : return parallel DWT2d Bases: Module 2D Discrete Wavelet Transform Parameters: scales ( int , default: 1 ) \u2013 Number of scales (number of transform performed) wave ( str , default: 'haar' ) \u2013 The type of wavelet padding ( str , default: 'zero' ) \u2013 data padding mode before transformation Source code in uchiha\\models\\modules\\preprocessor.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @MODULE . register_module () class DWT2d ( nn . Module ): \"\"\" 2D Discrete Wavelet Transform Args: scales (int): Number of scales (number of transform performed) wave (str): The type of wavelet padding (str): data padding mode before transformation \"\"\" def __init__ ( self , scales = 1 , wave = 'haar' , padding = 'zero' ): super () . __init__ () self . wavelet_transform = DWTForward ( J = scales , wave = wave , mode = padding ) def forward ( self , x ): # x.shape:B, C, H, W out = self . wavelet_transform ( x ) LL , H = out LH , HL , HH = H [ 0 ][:, :, 0 , :, :], H [ 0 ][:, :, 1 , :, :], H [ 0 ][:, :, 2 , :, :] return torch . cat (( LL , LH , HL , HH ), dim = 1 ) GroupRCP Bases: Module Group-based Residue Channel Prior Parameters: split_bands ( List , default: None ) \u2013 \u5212\u5206\u7684\u6ce2\u6bb5\u7d22\u5f15\uff0c\u5728\u5176\u4e24\u4fa7\u8fdb\u884c\u6b8b\u5dee\u8fd0\u7b97 Source code in uchiha\\models\\modules\\preprocessor.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 @MODULE . register_module () class GroupRCP ( nn . Module ): \"\"\" Group-based Residue Channel Prior Args: split_bands (List): \u5212\u5206\u7684\u6ce2\u6bb5\u7d22\u5f15\uff0c\u5728\u5176\u4e24\u4fa7\u8fdb\u884c\u6b8b\u5dee\u8fd0\u7b97 \"\"\" def __init__ ( self , split_bands = None , strategy = None ): super () . __init__ () self . split_bands = split_bands if strategy is None : self . strategy = [ 'sorted' , 'serial' ] else : self . strategy = strategy def forward ( self , x ): # x.shape (B, C, H, W) rcp = [] for s in self . strategy : if s == 'sorted' : rcp . append ( get_sorted_rcp ( x , self . split_bands )) elif s == 'serial' : rcp . append ( get_serial_rcp ( x , self . split_bands )) elif s == 'sorted_pw' : rcp . append ( get_sorted_rcp_pw ( x , self . split_bands )) else : raise ValueError ( f \"Invalid strategy: { s } \" ) rcp = torch . cat ( rcp , dim = 1 ) return rcp get_sorted_rcp_pw ( image , split_bands ) \u8f93\u5165: CHW torch.Tensor \u529f\u80fd: \u6392\u5e8f + \u5207\u5206\u70b9\u5dee\u503c \u8f93\u51fa: HWN torch.Tensor Source code in uchiha\\models\\modules\\preprocessor.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def get_sorted_rcp_pw ( image : torch . Tensor , split_bands : list ) -> torch . Tensor : \"\"\" \u8f93\u5165: CHW torch.Tensor \u529f\u80fd: \u6392\u5e8f + \u5207\u5206\u70b9\u5dee\u503c \u8f93\u51fa: HWN torch.Tensor \"\"\" c , h , w = image . shape flattened = image . permute ( 1 , 2 , 0 ) . reshape ( - 1 , c ) # L x C sorted_flattened , _ = torch . sort ( flattened , dim = 1 ) results = [] for split_idx in split_bands : if not ( 0 < split_idx < c ): raise ValueError ( f \"\u5207\u5206\u70b9 { split_idx } \u8d85\u51fa\u901a\u9053\u8303\u56f4 (1~ { c - 1 } )\" ) front = sorted_flattened [:, : split_idx ] back = sorted_flattened [:, split_idx :] diff = front . mean ( dim = 1 ) - back . mean ( dim = 1 ) results . append ( diff ) output = torch . stack ( results , dim = 1 ) # L x N return output . reshape ( h , w , - 1 ) # HWN Post-process IDWT1d Bases: Module 1D Inverse Discrete Wavelet Transform Parameters: wave ( str , default: 'haar' ) \u2013 The type of wavelet in_channel ( int , default: 1024 ) \u2013 Number of input channels. out_channel ( int , default: 1 ) \u2013 Number of output channels. Source code in uchiha\\models\\modules\\postprocessor.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @MODULE . register_module () class IDWT1d ( nn . Module ): \"\"\" 1D Inverse Discrete Wavelet Transform Args: wave (str): The type of wavelet in_channel (int): Number of input channels. out_channel (int): Number of output channels. \"\"\" def __init__ ( self , wave = 'haar' , in_channel = 1024 , out_channel = 1 ): super () . __init__ () self . IDWT = DWT1DInverse ( wave = wave ) self . pooling = nn . AdaptiveAvgPool1d ( 1 ) self . fc = nn . Linear ( in_channel , out_channel ) def forward ( self , x ): L , H = x H = ( H ,) out = self . IDWT (( L , H )) pooling = self . pooling ( out . transpose ( 1 , 2 )) . squeeze ( 2 ) return self . fc ( pooling ) IDWT2d Bases: Module 2D Inverse Discrete Wavelet Transform Parameters: wave ( str , default: 'haar' ) \u2013 The type of wavelet Source code in uchiha\\models\\modules\\postprocessor.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @MODULE . register_module () class IDWT2d ( nn . Module ): \"\"\" 2D Inverse Discrete Wavelet Transform Args: wave (str): The type of wavelet \"\"\" def __init__ ( self , wave = 'haar' ): super () . __init__ () self . IDWT = DWTInverse ( wave = wave ) def forward ( self , x ): # x.shape:B, C, H, W out = self . wavlet_transform ( x ) LL , H = out LH , HL , HH = H [ 0 ][:, :, 0 , :, :], H [ 0 ][:, :, 1 , :, :], H [ 0 ][:, :, 2 , :, :] return torch . cat (( LL , LH , HL , HH ), dim = 1 ) WeightedSum Bases: Module assign weights to multiple inputs and accumulate If a list is provided, it will be weighted according to the value of the list, if not, the mean weight will be assigned initially and updated with backward propagation. Parameters: weights \u2013 weights of each input Source code in uchiha\\models\\modules\\postprocessor.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 @MODULE . register_module () class WeightedSum ( nn . Module ): \"\"\" assign weights to multiple inputs and accumulate If a list is provided, it will be weighted according to the value of the list, if not, the mean weight will be assigned initially and updated with backward propagation. Args: weights (): weights of each input \"\"\" def __init__ ( self , weights ): super () . __init__ () if isinstance ( weights , list ): self . weights = weights else : self . weights = nn . ParameterList ([ nn . Parameter ( torch . tensor ( 1 / weights )) for _ in range ( weights )]) def forward ( self , x : List [ Tensor ]): result = torch . zeros_like ( x [ 0 ]) for idx , parallel in enumerate ( x ): result += self . weights [ idx ] * parallel return result Fusion CatPWConvFusion Bases: Module Concat followed by PW-Conv Parameters: in_channels ( int ) \u2013 Number of input channels. out_channels ( int , default: None ) \u2013 Number of output channels. Source code in uchiha\\models\\modules\\fusion.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @MODULE . register_module () class CatPWConvFusion ( nn . Module ): \"\"\" Concat followed by PW-Conv Args: in_channels (int): Number of input channels. out_channels (int): Number of output channels. \"\"\" def __init__ ( self , in_channels , out_channels = None ): super () . __init__ () self . in_channels = in_channels if out_channels is None : out_channels = in_channels // 2 self . fusion = nn . Conv2d ( in_channels , out_channels , 1 ) def forward ( self , x , y ): # x y.shape: (B, C, H, W ) cat = torch . cat ([ x , y ], dim = 1 ) out = self . fusion ( cat ) # B H*W C return out ResNet BasicResidualBlock Bases: Module basic residual block (RB) Parameters: inplanes ( int ) \u2013 number of input channels planes ( int ) \u2013 number of output channels stride ( int , default: 1 ) \u2013 the stride for the convolution operation. Default: 1 downsample ( Module , default: None ) \u2013 operations performed when residual is applied. Default: None groups ( int , default: 1 ) \u2013 the groups for the convolution operation. Default: 1 base_width ( int , default: 64 ) \u2013 base width of image. Default: 64 dilation ( int , default: 1 ) \u2013 the dilation for the convolution operation. Default: 1 norm_layer ( Module , default: None ) \u2013 normalization layer after convolution layer. Default: None Source code in uchiha\\models\\modules\\basic_resnet.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 @MODULE . register_module () class BasicResidualBlock ( nn . Module ): \"\"\" basic residual block (RB) Args: inplanes (int): number of input channels planes (int): number of output channels stride (int): the stride for the convolution operation. Default: 1 downsample (nn.Module): operations performed when residual is applied. Default: None groups (int): the groups for the convolution operation. Default: 1 base_width (int): base width of image. Default: 64 dilation (int): the dilation for the convolution operation. Default: 1 norm_layer (nn.Module): normalization layer after convolution layer. Default: None \"\"\" expansion : int = 1 def __init__ ( self , inplanes : int , planes : int , stride : int = 1 , downsample : Optional [ nn . Module ] = None , groups : int = 1 , base_width : int = 64 , dilation : int = 1 , norm_layer : Optional [ Callable [ ... , nn . Module ]] = None ) -> None : super ( BasicResidualBlock , self ) . __init__ () if norm_layer is None : norm_layer = nn . BatchNorm2d if groups != 1 or base_width != 64 : raise ValueError ( 'BasicBlock only supports groups=1 and base_width=64' ) if dilation > 1 : raise NotImplementedError ( \"Dilation > 1 not supported in BasicBlock\" ) # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self . conv1 = conv3x3 ( inplanes , planes , stride ) self . bn1 = norm_layer ( planes ) self . relu = nn . ReLU ( inplace = True ) self . conv2 = conv3x3 ( planes , planes ) self . bn2 = norm_layer ( planes ) self . downsample = downsample self . stride = stride def forward ( self , x : Tensor ) -> Tensor : identity = x out = self . conv1 ( x ) out = self . bn1 ( out ) out = self . relu ( out ) out = self . conv2 ( out ) out = self . bn2 ( out ) if self . downsample is not None : identity = self . downsample ( x ) out += identity out = self . relu ( out ) return out ResidualBottleneck Bases: Module basic residual bottleneck Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2) while original implementation places the stride at the first 1x1 convolution(self.conv1) according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385. This variant is also known as ResNet V1.5 and improves accuracy according to https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch. Parameters: inplanes ( int ) \u2013 number of input channels planes ( int ) \u2013 number of output channels stride ( int , default: 1 ) \u2013 the stride for the convolution operation. Default: 1 downsample ( Module , default: None ) \u2013 operations performed when residual is applied. Default: None groups ( int , default: 1 ) \u2013 the groups for the convolution operation. Default: 1 base_width ( int , default: 64 ) \u2013 base width of image. Default: 64 dilation ( int , default: 1 ) \u2013 the dilation for the convolution operation. Default: 1 norm_layer ( Module , default: None ) \u2013 normalization layer after convolution layer. Default: None Source code in uchiha\\models\\modules\\basic_resnet.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 @MODULE . register_module () class ResidualBottleneck ( nn . Module ): \"\"\" basic residual bottleneck Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2) while original implementation places the stride at the first 1x1 convolution(self.conv1) according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385. This variant is also known as ResNet V1.5 and improves accuracy according to https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch. Args: inplanes (int): number of input channels planes (int): number of output channels stride (int): the stride for the convolution operation. Default: 1 downsample (nn.Module): operations performed when residual is applied. Default: None groups (int): the groups for the convolution operation. Default: 1 base_width (int): base width of image. Default: 64 dilation (int): the dilation for the convolution operation. Default: 1 norm_layer (nn.Module): normalization layer after convolution layer. Default: None \"\"\" expansion : int = 4 def __init__ ( self , inplanes : int , planes : int , stride : int = 1 , downsample : Optional [ nn . Module ] = None , groups : int = 1 , base_width : int = 64 , dilation : int = 1 , norm_layer : Optional [ Callable [ ... , nn . Module ]] = None ) -> None : super ( ResidualBottleneck , self ) . __init__ () if norm_layer is None : norm_layer = nn . BatchNorm2d width = int ( planes * ( base_width / 64. )) * groups # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self . conv1 = conv1x1 ( inplanes , width ) self . bn1 = norm_layer ( width ) self . conv2 = conv3x3 ( width , width , stride , groups , dilation ) self . bn2 = norm_layer ( width ) self . conv3 = conv1x1 ( width , planes * self . expansion ) self . bn3 = norm_layer ( planes * self . expansion ) self . relu = nn . ReLU ( inplace = True ) self . downsample = downsample self . stride = stride def forward ( self , x : Tensor ) -> Tensor : identity = x out = self . conv1 ( x ) out = self . bn1 ( out ) out = self . relu ( out ) out = self . conv2 ( out ) out = self . bn2 ( out ) out = self . relu ( out ) out = self . conv3 ( out ) out = self . bn3 ( out ) if self . downsample is not None : identity = self . downsample ( x ) out += identity out = self . relu ( out ) return out ViT SimpleVisionTransformerLayer Bases: Module Vision Transformer (the simplest implementation) Parameters: dim ( int ) \u2013 input dimension depth ( int ) \u2013 number of stacked basic-transformer-blocks num_heads ( int ) \u2013 number of heads in Multi-Head Attention sequence_length ( int ) \u2013 The length of the sequence after changing the shape to (B ,L, C). qkv_bias ( bool ) \u2013 If True, add a learnable bias to query, key, value. Default: True mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP. Default: 4.0. cls ( bool , default: False ) \u2013 Class token will be used if set to True. pos ( bool , default: False ) \u2013 Position encoding will be used if set to True. dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 @MODULE . register_module () class SimpleVisionTransformerLayer ( nn . Module ): \"\"\" Vision Transformer (the simplest implementation) Args: dim (int): input dimension depth (int): number of stacked basic-transformer-blocks num_heads (int): number of heads in `Multi-Head Attention` sequence_length (int): The length of the sequence after changing the shape to (B ,L, C). qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True mlp_ratio (float): ratio of hidden layer to input channel in MLP. Default: 4.0. cls (bool): Class token will be used if set to True. pos (bool): Position encoding will be used if set to True. dropout (float): the rate of `Dropout` layer. Default: 0.0 \"\"\" def __init__ ( self , dim , depth , num_heads , sequence_length , qkv_bias , mlp_ratio = 4. , cls = False , pos = False , dropout = 0. ): super () . __init__ () self . sequence_length = sequence_length self . pos = pos self . cls = cls if cls : self . cls_token = nn . Parameter ( torch . zeros ( 1 , 1 , dim )) self . sequence_length += 1 if pos : self . pos_embed = nn . Parameter ( torch . zeros ( 1 , self . sequence_length , dim )) self . pos_drop = nn . Dropout ( dropout ) mlp_dim = int ( dim * mlp_ratio ) self . blocks = nn . Sequential ( * [ Block ( dim , num_heads , qkv_bias , mlp_dim , dropout ) for _ in range ( depth ) ]) self . norm = nn . LayerNorm ( dim ) if self . pos : nn . init . trunc_normal_ ( self . pos_embed , std = 0.02 ) if self . cls : nn . init . trunc_normal_ ( self . cls_token , std = 0.02 ) self . apply ( _init_vit_weights ) def forward ( self , x ): if self . cls : cls_token = self . cls_token . expand ( x . shape [ 0 ], - 1 , - 1 ) x = torch . cat (( cls_token , x ), dim = 1 ) if self . pos : x = x + self . pos_embed x = self . pos_drop ( x ) x = self . blocks ( x ) x = self . norm ( x ) return x [:, 0 ] . unsqueeze ( 1 ) if self . cls else x Atom Attention Bases: Module self attention module Parameters: dim ( int ) \u2013 input dimension num_heads ( int , default: 8 ) \u2013 number of heads in Multi-Head Attention qkv_bias ( bool , default: False ) \u2013 If True, add a learnable bias to query, key, value. Default: True dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer after qkv calculation. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class Attention ( nn . Module ): \"\"\" self attention module Args: dim (int): input dimension num_heads (int): number of heads in `Multi-Head Attention` qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True dropout (float): the rate of `Dropout` layer after qkv calculation. Default: 0.0 \"\"\" def __init__ ( self , dim , num_heads = 8 , qkv_bias = False , dropout = 0. ): super () . __init__ () self . num_heads = num_heads head_dim = dim // num_heads self . scale = head_dim ** - 0.5 self . qkv = nn . Linear ( dim , dim * 3 , bias = qkv_bias ) self . attn_drop = nn . Dropout ( dropout ) self . proj = nn . Linear ( dim , dim ) self . proj_drop = nn . Dropout ( dropout ) def forward ( self , x ): B , N , C = x . shape qkv = self . qkv ( x ) . reshape ( B , N , 3 , self . num_heads , C // self . num_heads ) . permute ( 2 , 0 , 3 , 1 , 4 ) q , k , v = qkv [ 0 ], qkv [ 1 ], qkv [ 2 ] attn = ( q @ k . transpose ( - 2 , - 1 )) * self . scale attn = attn . softmax ( dim =- 1 ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B , N , C ) x = self . proj ( x ) x = self . proj_drop ( x ) return x Block Bases: Module basic Transformer block Parameters: dim ( int ) \u2013 input dimension num_heads ( int ) \u2013 number of heads in Multi-Head Attention qkv_bias ( bool ) \u2013 If True, add a learnable bias to query, key, value. Default: True mlp_dim ( int ) \u2013 dimension of hidden layers in FeedForward dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class Block ( nn . Module ): \"\"\" basic Transformer block Args: dim (int): input dimension num_heads (int): number of heads in `Multi-Head Attention` qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True mlp_dim (int): dimension of hidden layers in FeedForward dropout (float): the rate of `Dropout` layer. Default: 0.0 \"\"\" def __init__ ( self , dim , num_heads , qkv_bias , mlp_dim , dropout = 0. ): super () . __init__ () self . norm1 = nn . LayerNorm ( dim ) self . attn = Attention ( dim , num_heads , qkv_bias , dropout ) self . drop = nn . Dropout ( dropout ) self . norm2 = nn . LayerNorm ( dim ) self . mlp = FeedForward ( dim , mlp_dim , dropout ) def forward ( self , x ): x = x + self . drop ( self . attn ( self . norm1 ( x ))) x = x + self . drop ( self . mlp ( self . norm2 ( x ))) return x FeedForward Bases: Module Feedforward-Network Parameters: dim ( int ) \u2013 input dimension hidden_dim ( int ) \u2013 dimension of hidden layers dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class FeedForward ( nn . Module ): \"\"\" Feedforward-Network Args: dim (int): input dimension hidden_dim (int): dimension of hidden layers dropout (float): the rate of `Dropout` layer. Default: 0.0 \"\"\" def __init__ ( self , dim , hidden_dim , dropout = 0. ): super () . __init__ () self . net = nn . Sequential ( # nn.LayerNorm(dim), nn . Linear ( dim , hidden_dim ), nn . GELU (), nn . Dropout ( dropout ), nn . Linear ( hidden_dim , dim ), nn . Dropout ( dropout ) ) def forward ( self , x ): return self . net ( x ) _init_vit_weights ( m ) ViT weight initialization Parameters: m ( Module ) \u2013 modules that need to initialize weights Source code in uchiha\\models\\modules\\basic_transformer.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def _init_vit_weights ( m ): \"\"\" ViT weight initialization Args: m (nn.Module): modules that need to initialize weights \"\"\" if isinstance ( m , nn . Linear ): nn . init . trunc_normal_ ( m . weight , std = .01 ) if m . bias is not None : nn . init . zeros_ ( m . bias ) elif isinstance ( m , nn . Conv2d ): nn . init . kaiming_normal_ ( m . weight , mode = \"fan_out\" ) if m . bias is not None : nn . init . zeros_ ( m . bias ) elif isinstance ( m , nn . LayerNorm ): nn . init . zeros_ ( m . bias ) nn . init . ones_ ( m . weight ) CBAM BasicCAM Bases: Module basic Channel Attention Module(CAM) Parameters: in_channel ( int ) \u2013 number of input channels out_channel ( int ) \u2013 number of output channels stride ( int , default: 1 ) \u2013 the stride for the convolution operation. Default: 1 downsample ( dict , default: None ) \u2013 operations performed when residual is applied. Default: None Source code in uchiha\\models\\modules\\cbam.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 @MODULE . register_module () class BasicCAM ( nn . Module ): \"\"\" basic Channel Attention Module(CAM) Args: in_channel (int): number of input channels out_channel (int): number of output channels stride (int): the stride for the convolution operation. Default: 1 downsample (dict): operations performed when residual is applied. Default: None \"\"\" expansion = 1 def __init__ ( self , in_channel , out_channel , stride = 1 , downsample = None ): super ( BasicCAM , self ) . __init__ () self . conv1 = conv3x3 ( in_channel , out_channel , stride ) self . bn1 = nn . BatchNorm2d ( out_channel ) self . relu = nn . ReLU ( inplace = True ) self . conv2 = conv3x3 ( out_channel , out_channel ) self . bn2 = nn . BatchNorm2d ( out_channel ) self . ca = CAM ( out_channel ) self . downsample_opt = build_module ( downsample ) self . stride = stride def forward ( self , x ): residual = x out = self . conv1 ( x ) out = self . bn1 ( out ) out = self . relu ( out ) out = self . conv2 ( out ) out = self . bn2 ( out ) out = self . ca ( out ) * out if self . downsample_opt is not None : residual = self . downsample_opt ( x ) out += residual out = self . relu ( out ) return out BasicSAM Bases: Module basic Spatial Attention Module(SAM) Parameters: in_channel ( int ) \u2013 number of input channels out_channel ( int ) \u2013 number of output channels stride ( int , default: 1 ) \u2013 the stride for the convolution operation. Default: 1 downsample ( dict , default: None ) \u2013 operations performed when residual is applied. Default: None Source code in uchiha\\models\\modules\\cbam.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 @MODULE . register_module () class BasicSAM ( nn . Module ): \"\"\" basic Spatial Attention Module(SAM) Args: in_channel (int): number of input channels out_channel (int): number of output channels stride (int): the stride for the convolution operation. Default: 1 downsample (dict): operations performed when residual is applied. Default: None \"\"\" expansion = 1 def __init__ ( self , in_channel , out_channel , stride = 1 , downsample = None ): super ( BasicSAM , self ) . __init__ () self . conv1 = conv3x3 ( in_channel , out_channel , stride ) self . bn1 = nn . BatchNorm2d ( out_channel ) self . relu = nn . ReLU ( inplace = True ) self . conv2 = conv3x3 ( out_channel , out_channel ) self . bn2 = nn . BatchNorm2d ( out_channel ) self . sa = SAM () self . downsample_opt = build_module ( downsample ) self . stride = stride def forward ( self , x ): residual = x out = self . conv1 ( x ) out = self . bn1 ( out ) out = self . relu ( out ) out = self . conv2 ( out ) out = self . bn2 ( out ) out = self . sa ( out ) * out if self . downsample_opt is not None : residual = self . downsample_opt ( x ) out += residual out = self . relu ( out ) return out BasicCBAM Bases: Module Conv*2 ==> CAM ==> SAM ==> Parameters: inplanes ( int ) \u2013 number of input channels planes ( int ) \u2013 number of output channels stride ( int , default: 1 ) \u2013 the stride for the convolution operation. Default: 1 downsample ( Module , default: None ) \u2013 operations performed when residual is applied. Default: None Source code in uchiha\\models\\modules\\cbam.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 class BasicCBAM ( nn . Module ): \"\"\" Conv*2 ==> CAM ==> SAM ==> Args: inplanes (int): number of input channels planes (int): number of output channels stride (int): the stride for the convolution operation. Default: 1 downsample (nn.Module): operations performed when residual is applied. Default: None \"\"\" expansion = 1 def __init__ ( self , inplanes , planes , stride = 1 , downsample = None ): super ( BasicCBAM , self ) . __init__ () self . conv1 = conv3x3 ( inplanes , planes , stride ) self . bn1 = nn . BatchNorm2d ( planes ) self . relu = nn . ReLU ( inplace = True ) self . conv2 = conv3x3 ( planes , planes ) self . bn2 = nn . BatchNorm2d ( planes ) self . ca = CAM ( planes ) self . sa = SAM () self . downsample = downsample self . stride = stride def forward ( self , x ): residual = x out = self . conv1 ( x ) out = self . bn1 ( out ) out = self . relu ( out ) out = self . conv2 ( out ) out = self . bn2 ( out ) out = self . ca ( out ) * out out = self . sa ( out ) * out if self . downsample is not None : residual = self . downsample ( x ) out += residual out = self . relu ( out ) return out CBAMBottleneck Bases: Module Conv*3 ==> CAM ==> SAM ==> the actual number of output channels is expanded by expansion Parameters: inplanes ( int ) \u2013 number of input channels planes ( int ) \u2013 number of output channels stride ( int , default: 1 ) \u2013 the stride for the convolution operation. Default: 1 downsample ( Module , default: None ) \u2013 operations performed when residual is applied. Default: None Source code in uchiha\\models\\modules\\cbam.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 class CBAMBottleneck ( nn . Module ): \"\"\" Conv*3 ==> CAM ==> SAM ==> the actual number of output channels is expanded by expansion Args: inplanes (int): number of input channels planes (int): number of output channels stride (int): the stride for the convolution operation. Default: 1 downsample (nn.Module): operations performed when residual is applied. Default: None \"\"\" expansion = 4 def __init__ ( self , inplanes , planes , stride = 1 , downsample = None ): super ( CBAMBottleneck , self ) . __init__ () self . conv1 = nn . Conv2d ( inplanes , planes , kernel_size = 1 , bias = False ) self . bn1 = nn . BatchNorm2d ( planes ) self . conv2 = nn . Conv2d ( planes , planes , kernel_size = 3 , stride = stride , padding = 1 , bias = False ) self . bn2 = nn . BatchNorm2d ( planes ) self . conv3 = nn . Conv2d ( planes , planes * self . expansion , kernel_size = 1 , bias = False ) self . bn3 = nn . BatchNorm2d ( planes * self . expansion ) self . relu = nn . ReLU ( inplace = True ) self . ca = CAM ( planes * self . expansion ) self . sa = SAM () self . downsample = downsample self . stride = stride def forward ( self , x ): residual = x out = self . conv1 ( x ) out = self . bn1 ( out ) out = self . relu ( out ) out = self . conv2 ( out ) out = self . bn2 ( out ) out = self . relu ( out ) out = self . conv3 ( out ) out = self . bn3 ( out ) out = self . ca ( out ) * out out = self . sa ( out ) * out if self . downsample is not None : residual = self . downsample ( x ) out += residual out = self . relu ( out ) return out Atom Attention Bases: Module self attention module Parameters: dim ( int ) \u2013 input dimension num_heads ( int , default: 8 ) \u2013 number of heads in Multi-Head Attention qkv_bias ( bool , default: False ) \u2013 If True, add a learnable bias to query, key, value. Default: True dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer after qkv calculation. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class Attention ( nn . Module ): \"\"\" self attention module Args: dim (int): input dimension num_heads (int): number of heads in `Multi-Head Attention` qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True dropout (float): the rate of `Dropout` layer after qkv calculation. Default: 0.0 \"\"\" def __init__ ( self , dim , num_heads = 8 , qkv_bias = False , dropout = 0. ): super () . __init__ () self . num_heads = num_heads head_dim = dim // num_heads self . scale = head_dim ** - 0.5 self . qkv = nn . Linear ( dim , dim * 3 , bias = qkv_bias ) self . attn_drop = nn . Dropout ( dropout ) self . proj = nn . Linear ( dim , dim ) self . proj_drop = nn . Dropout ( dropout ) def forward ( self , x ): B , N , C = x . shape qkv = self . qkv ( x ) . reshape ( B , N , 3 , self . num_heads , C // self . num_heads ) . permute ( 2 , 0 , 3 , 1 , 4 ) q , k , v = qkv [ 0 ], qkv [ 1 ], qkv [ 2 ] attn = ( q @ k . transpose ( - 2 , - 1 )) * self . scale attn = attn . softmax ( dim =- 1 ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B , N , C ) x = self . proj ( x ) x = self . proj_drop ( x ) return x Block Bases: Module basic Transformer block Parameters: dim ( int ) \u2013 input dimension num_heads ( int ) \u2013 number of heads in Multi-Head Attention qkv_bias ( bool ) \u2013 If True, add a learnable bias to query, key, value. Default: True mlp_dim ( int ) \u2013 dimension of hidden layers in FeedForward dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class Block ( nn . Module ): \"\"\" basic Transformer block Args: dim (int): input dimension num_heads (int): number of heads in `Multi-Head Attention` qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True mlp_dim (int): dimension of hidden layers in FeedForward dropout (float): the rate of `Dropout` layer. Default: 0.0 \"\"\" def __init__ ( self , dim , num_heads , qkv_bias , mlp_dim , dropout = 0. ): super () . __init__ () self . norm1 = nn . LayerNorm ( dim ) self . attn = Attention ( dim , num_heads , qkv_bias , dropout ) self . drop = nn . Dropout ( dropout ) self . norm2 = nn . LayerNorm ( dim ) self . mlp = FeedForward ( dim , mlp_dim , dropout ) def forward ( self , x ): x = x + self . drop ( self . attn ( self . norm1 ( x ))) x = x + self . drop ( self . mlp ( self . norm2 ( x ))) return x FeedForward Bases: Module Feedforward-Network Parameters: dim ( int ) \u2013 input dimension hidden_dim ( int ) \u2013 dimension of hidden layers dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class FeedForward ( nn . Module ): \"\"\" Feedforward-Network Args: dim (int): input dimension hidden_dim (int): dimension of hidden layers dropout (float): the rate of `Dropout` layer. Default: 0.0 \"\"\" def __init__ ( self , dim , hidden_dim , dropout = 0. ): super () . __init__ () self . net = nn . Sequential ( # nn.LayerNorm(dim), nn . Linear ( dim , hidden_dim ), nn . GELU (), nn . Dropout ( dropout ), nn . Linear ( hidden_dim , dim ), nn . Dropout ( dropout ) ) def forward ( self , x ): return self . net ( x ) SimpleVisionTransformerLayer Bases: Module Vision Transformer (the simplest implementation) Parameters: dim ( int ) \u2013 input dimension depth ( int ) \u2013 number of stacked basic-transformer-blocks num_heads ( int ) \u2013 number of heads in Multi-Head Attention sequence_length ( int ) \u2013 The length of the sequence after changing the shape to (B ,L, C). qkv_bias ( bool ) \u2013 If True, add a learnable bias to query, key, value. Default: True mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP. Default: 4.0. cls ( bool , default: False ) \u2013 Class token will be used if set to True. pos ( bool , default: False ) \u2013 Position encoding will be used if set to True. dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 @MODULE . register_module () class SimpleVisionTransformerLayer ( nn . Module ): \"\"\" Vision Transformer (the simplest implementation) Args: dim (int): input dimension depth (int): number of stacked basic-transformer-blocks num_heads (int): number of heads in `Multi-Head Attention` sequence_length (int): The length of the sequence after changing the shape to (B ,L, C). qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True mlp_ratio (float): ratio of hidden layer to input channel in MLP. Default: 4.0. cls (bool): Class token will be used if set to True. pos (bool): Position encoding will be used if set to True. dropout (float): the rate of `Dropout` layer. Default: 0.0 \"\"\" def __init__ ( self , dim , depth , num_heads , sequence_length , qkv_bias , mlp_ratio = 4. , cls = False , pos = False , dropout = 0. ): super () . __init__ () self . sequence_length = sequence_length self . pos = pos self . cls = cls if cls : self . cls_token = nn . Parameter ( torch . zeros ( 1 , 1 , dim )) self . sequence_length += 1 if pos : self . pos_embed = nn . Parameter ( torch . zeros ( 1 , self . sequence_length , dim )) self . pos_drop = nn . Dropout ( dropout ) mlp_dim = int ( dim * mlp_ratio ) self . blocks = nn . Sequential ( * [ Block ( dim , num_heads , qkv_bias , mlp_dim , dropout ) for _ in range ( depth ) ]) self . norm = nn . LayerNorm ( dim ) if self . pos : nn . init . trunc_normal_ ( self . pos_embed , std = 0.02 ) if self . cls : nn . init . trunc_normal_ ( self . cls_token , std = 0.02 ) self . apply ( _init_vit_weights ) def forward ( self , x ): if self . cls : cls_token = self . cls_token . expand ( x . shape [ 0 ], - 1 , - 1 ) x = torch . cat (( cls_token , x ), dim = 1 ) if self . pos : x = x + self . pos_embed x = self . pos_drop ( x ) x = self . blocks ( x ) x = self . norm ( x ) return x [:, 0 ] . unsqueeze ( 1 ) if self . cls else x _init_vit_weights ( m ) ViT weight initialization Parameters: m ( Module ) \u2013 modules that need to initialize weights Source code in uchiha\\models\\modules\\basic_transformer.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def _init_vit_weights ( m ): \"\"\" ViT weight initialization Args: m (nn.Module): modules that need to initialize weights \"\"\" if isinstance ( m , nn . Linear ): nn . init . trunc_normal_ ( m . weight , std = .01 ) if m . bias is not None : nn . init . zeros_ ( m . bias ) elif isinstance ( m , nn . Conv2d ): nn . init . kaiming_normal_ ( m . weight , mode = \"fan_out\" ) if m . bias is not None : nn . init . zeros_ ( m . bias ) elif isinstance ( m , nn . LayerNorm ): nn . init . zeros_ ( m . bias ) nn . init . ones_ ( m . weight ) Swin Transformer SwinTransformerLayer Bases: Module A basic Swin Transformer layer for one stage. Parameters: dim ( int ) \u2013 Number of input channels. input_resolution ( tuple [ int ] | int ) \u2013 Input resolution. depth ( int ) \u2013 Number of blocks. num_heads ( int ) \u2013 Number of attention heads. window_size ( int ) \u2013 Local window size. mlp_ratio ( float , default: 4.0 ) \u2013 Ratio of mlp hidden dim to embedding dim. qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set. drop_rate ( float , default: 0.0 ) \u2013 Dropout rate. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Attention dropout rate. Default: 0.0 drop_path ( float | tuple [ float ] , default: 0.0 ) \u2013 Stochastic depth rate. Default: 0.0 act_layer ( Module | str , default: GELU ) \u2013 activation function in MLP. norm_layer ( Module , default: LayerNorm ) \u2013 Normalization layer. Default: nn.LayerNorm downsample ( Module | None , default: None ) \u2013 Downsample layer at the end of the layer. Default: None Source code in uchiha\\models\\modules\\swin_transformer.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @MODULE . register_module () class SwinTransformerLayer ( nn . Module ): \"\"\" A basic Swin Transformer layer for one stage. Args: dim (int): Number of input channels. input_resolution (tuple[int] | int): Input resolution. depth (int): Number of blocks. num_heads (int): Number of attention heads. window_size (int): Local window size. mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set. drop_rate (float, optional): Dropout rate. Default: 0.0 attn_drop (float, optional): Attention dropout rate. Default: 0.0 drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0 act_layer (nn.Module | str): activation function in MLP. norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None \"\"\" def __init__ ( self , dim , input_resolution , depth , num_heads , window_size , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop_rate = 0. , attn_drop = 0. , drop_path = 0. , act_layer = nn . GELU , norm_layer = nn . LayerNorm , downsample = None ): super () . __init__ () self . dim = dim self . input_resolution = to_2tuple ( input_resolution ) self . depth = depth self . downsample = build_module ( downsample ) if isinstance ( norm_layer , str ): norm_layer = build_norm ( norm_layer ) if isinstance ( act_layer , str ): act_layer = build_act ( act_layer ) # build blocks self . blocks = nn . ModuleList ([ SwinTransformerBlock ( dim = dim , input_resolution = self . input_resolution , num_heads = num_heads , window_size = window_size , shift_size = 0 if ( i % 2 == 0 ) else window_size // 2 , mlp_ratio = mlp_ratio , qkv_bias = qkv_bias , qk_scale = qk_scale , drop = drop_rate , attn_drop = attn_drop , drop_path = drop_path [ i ] if isinstance ( drop_path , list ) else drop_path , act_layer = act_layer , norm_layer = norm_layer , ) for i in range ( depth )]) def forward ( self , x ): for blk in self . blocks : x = blk ( x ) out = self . downsample ( x ) if self . downsample else x return out def extra_repr ( self ) -> str : return f \"dim= { self . dim } , input_resolution= { self . input_resolution } , depth= { self . depth } \" def flops ( self ): flops = 0 for blk in self . blocks : flops += blk . flops () if self . downsample is not None : flops += self . downsample . flops () return flops SwinTransformerLayers Bases: Module Collection of Channel-Transformer-Layers Parameters: dims ( List [ int ] ) \u2013 Number of input channels. input_resolutions ( List [ int | Tuple ( int )] ) \u2013 spatial resolution of input features depths ( List [ int ] ) \u2013 number of stacked channel-transformer-blocks num_heads ( List [ int ] ) \u2013 Number of heads in Multi-Head Attention mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set drop_rate \u2013 Dropout ratio of output of Attention. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 drop_path_rate ( float , default: 0.1 ) \u2013 Rate required to generate drop path, it will be called by torch.linspace(0, drop_path_rate, depth) designed to increase the probability of DropPath as the depth increases act_layer ( Module | str , default: GELU ) \u2013 activation function in MLP. norm_layer ( Module | str , default: LayerNorm ) \u2013 normalization layer before Attention and MLP. Default: None Source code in uchiha\\models\\modules\\swin_transformer.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 @MODULE . register_module () class SwinTransformerLayers ( nn . Module ): \"\"\" Collection of Channel-Transformer-Layers Args: dims (List[int]): Number of input channels. input_resolutions (List[int | Tuple(int)]): spatial resolution of input features depths (List[int]): number of stacked channel-transformer-blocks num_heads (List[int]): Number of heads in `Multi-Head Attention` mlp_ratio (float): ratio of hidden layer to input channel in MLP qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set drop_rate (): Dropout ratio of output of Attention. Default: 0.0 attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 drop_path_rate (float): Rate required to generate drop path, it will be called by `torch.linspace(0, drop_path_rate, depth)` designed to increase the probability of DropPath as the depth increases act_layer (nn.Module | str): activation function in MLP. norm_layer (nn.Module | str): normalization layer before Attention and MLP. Default: None \"\"\" def __init__ ( self , dims , input_resolutions , depths , num_heads , window_sizes , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop_rate = 0. , attn_drop = 0. , drop_path_rate = 0.1 , act_layer = nn . GELU , norm_layer = nn . LayerNorm , downsamples = None ): super () . __init__ () self . dims = dims self . input_resolutions = input_resolutions self . depths = depths self . num_heads = num_heads self . window_sizes = window_sizes self . num_layers = len ( dims ) self . downsamples = cfg_decomposition ( downsamples ) if len ( self . downsamples ) < self . num_layers : self . downsamples . extend ([ None ] * ( self . num_layers - len ( self . downsamples ))) # stochastic depth self . dpr = [ x . item () for x in torch . linspace ( 0 , drop_path_rate , sum ( depths ))] # build layers self . layers = nn . ModuleList () for i in range ( self . num_layers ): dim = self . dims [ i ] input_resolution = self . input_resolutions [ i ] num_heads = self . num_heads [ i ] window_size = self . window_sizes [ i ] depth = self . depths [ i ] downsample = self . downsamples [ i ] drop_path = self . dpr [ sum ( depths [: i ]): sum ( depths [: i + 1 ])] # build layer layer = SwinTransformerLayer ( dim = dim , input_resolution = input_resolution , depth = depth , num_heads = num_heads , window_size = window_size , mlp_ratio = mlp_ratio , qkv_bias = qkv_bias , qk_scale = qk_scale , drop_rate = drop_rate , attn_drop = attn_drop , drop_path = drop_path , act_layer = act_layer , norm_layer = norm_layer , downsample = downsample ) self . layers . append ( layer ) Atom SwinTransformerBlock Bases: Module Swin Transformer Block. Parameters: dim ( int ) \u2013 Number of input channels. input_resolution ( tuple [ int ] ) \u2013 Input resolution. num_heads ( int ) \u2013 Number of attention heads. window_size ( int , default: 7 ) \u2013 Window size. shift_size ( int , default: 0 ) \u2013 Shift size for SW-MSA. mlp_ratio ( float , default: 4.0 ) \u2013 Ratio of mlp hidden dim to embedding dim. qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set. drop ( float , default: 0.0 ) \u2013 Dropout rate. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Attention dropout rate. Default: 0.0 drop_path ( float , default: 0.0 ) \u2013 Stochastic depth rate. Default: 0.0 act_layer ( Module , default: GELU ) \u2013 Activation layer. Default: nn.GELU norm_layer ( Module , default: LayerNorm ) \u2013 Normalization layer. Default: nn.LayerNorm Source code in uchiha\\models\\modules\\swin_transformer.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 class SwinTransformerBlock ( nn . Module ): r \"\"\" Swin Transformer Block. Args: dim (int): Number of input channels. input_resolution (tuple[int]): Input resolution. num_heads (int): Number of attention heads. window_size (int): Window size. shift_size (int): Shift size for SW-MSA. mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set. drop (float, optional): Dropout rate. Default: 0.0 attn_drop (float, optional): Attention dropout rate. Default: 0.0 drop_path (float, optional): Stochastic depth rate. Default: 0.0 act_layer (nn.Module, optional): Activation layer. Default: nn.GELU norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm \"\"\" def __init__ ( self , dim , input_resolution , num_heads , window_size = 7 , shift_size = 0 , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop = 0. , attn_drop = 0. , drop_path = 0. , act_layer = nn . GELU , norm_layer = nn . LayerNorm ): super () . __init__ () self . dim = dim self . input_resolution = input_resolution self . num_heads = num_heads self . window_size = window_size self . shift_size = shift_size self . mlp_ratio = mlp_ratio if min ( self . input_resolution ) <= self . window_size : # if window size is larger than input resolution, we don't partition windows self . shift_size = 0 self . window_size = min ( self . input_resolution ) assert 0 <= self . shift_size < self . window_size , \"shift_size must in 0-window_size\" self . norm1 = norm_layer ( dim ) self . attn = WindowAttention ( dim , window_size = to_2tuple ( self . window_size ), num_heads = num_heads , qkv_bias = qkv_bias , qk_scale = qk_scale , attn_drop = attn_drop , proj_drop = drop ) self . drop_path = DropPath ( drop_path ) if drop_path > 0. else nn . Identity () self . norm2 = norm_layer ( dim ) mlp_hidden_dim = int ( dim * mlp_ratio ) self . mlp = MLP ( in_features = dim , hidden_features = mlp_hidden_dim , act_layer = act_layer , drop = drop ) if self . shift_size > 0 : # calculate attention mask for SW-MSA H , W = self . input_resolution img_mask = torch . zeros (( 1 , H , W , 1 )) # 1 H W 1 h_slices = ( slice ( 0 , - self . window_size ), slice ( - self . window_size , - self . shift_size ), slice ( - self . shift_size , None )) w_slices = ( slice ( 0 , - self . window_size ), slice ( - self . window_size , - self . shift_size ), slice ( - self . shift_size , None )) cnt = 0 for h in h_slices : for w in w_slices : img_mask [:, h , w , :] = cnt cnt += 1 mask_windows = window_partition ( img_mask , self . window_size ) # nW, window_size, window_size, 1 mask_windows = mask_windows . view ( - 1 , self . window_size * self . window_size ) attn_mask = mask_windows . unsqueeze ( 1 ) - mask_windows . unsqueeze ( 2 ) attn_mask = attn_mask . masked_fill ( attn_mask != 0 , float ( - 100.0 )) . masked_fill ( attn_mask == 0 , float ( 0.0 )) else : attn_mask = None self . register_buffer ( \"attn_mask\" , attn_mask ) def forward ( self , x ): H , W = self . input_resolution B , L , C = x . shape assert L == H * W , \"input feature has wrong size\" shortcut = x x = self . norm1 ( x ) x = x . view ( B , H , W , C ) # cyclic shift if self . shift_size > 0 : shifted_x = torch . roll ( x , shifts = ( - self . shift_size , - self . shift_size ), dims = ( 1 , 2 )) # partition windows x_windows = window_partition ( shifted_x , self . window_size ) # nW*B, window_size, window_size, C else : shifted_x = x # partition windows x_windows = window_partition ( shifted_x , self . window_size ) # nW*B, window_size, window_size, C x_windows = x_windows . view ( - 1 , self . window_size * self . window_size , C ) # nW*B, window_size*window_size, C # W-MSA/SW-MSA attn_windows = self . attn ( x_windows , mask = self . attn_mask ) # nW*B, window_size*window_size, C # merge windows attn_windows = attn_windows . view ( - 1 , self . window_size , self . window_size , C ) # reverse cyclic shift if self . shift_size > 0 : shifted_x = window_reverse ( attn_windows , self . window_size , H , W ) # B H' W' C x = torch . roll ( shifted_x , shifts = ( self . shift_size , self . shift_size ), dims = ( 1 , 2 )) else : shifted_x = window_reverse ( attn_windows , self . window_size , H , W ) # B H' W' C x = shifted_x x = x . view ( B , H * W , C ) x = shortcut + self . drop_path ( x ) # FFN x = x + self . drop_path ( self . mlp ( self . norm2 ( x ))) return x def extra_repr ( self ) -> str : return f \"dim= { self . dim } , input_resolution= { self . input_resolution } , num_heads= { self . num_heads } , \" \\ f \"window_size= { self . window_size } , shift_size= { self . shift_size } , mlp_ratio= { self . mlp_ratio } \" def flops ( self ): flops = 0 H , W = self . input_resolution # norm1 flops += self . dim * H * W # W-MSA/SW-MSA nW = H * W / self . window_size / self . window_size flops += nW * self . attn . flops ( self . window_size * self . window_size ) # mlp flops += 2 * H * W * self . dim * self . dim * self . mlp_ratio # norm2 flops += self . dim * H * W return flops WindowAttention Bases: Module Window based multi-head self attention (W-MSA) module with relative position bias. It supports both of shifted and non-shifted window. Parameters: dim ( int ) \u2013 Number of input channels. window_size ( tuple [ int ] ) \u2013 The height and width of the window. num_heads ( int ) \u2013 Number of attention heads. qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 proj_drop ( float , default: 0.0 ) \u2013 Dropout ratio of output. Default: 0.0 Source code in uchiha\\models\\modules\\swin_transformer.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class WindowAttention ( nn . Module ): r \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias. It supports both of shifted and non-shifted window. Args: dim (int): Number of input channels. window_size (tuple[int]): The height and width of the window. num_heads (int): Number of attention heads. qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 proj_drop (float, optional): Dropout ratio of output. Default: 0.0 \"\"\" def __init__ ( self , dim , window_size , num_heads , qkv_bias = True , qk_scale = None , attn_drop = 0. , proj_drop = 0. ): super () . __init__ () self . dim = dim self . window_size = window_size # Wh, Ww self . num_heads = num_heads head_dim = dim // num_heads self . scale = qk_scale or head_dim ** - 0.5 # define a parameter table of relative position bias self . relative_position_bias_table = nn . Parameter ( torch . zeros (( 2 * window_size [ 0 ] - 1 ) * ( 2 * window_size [ 1 ] - 1 ), num_heads )) # 2*Wh-1 * 2*Ww-1, nH # get pair-wise relative position index for each token inside the window coords_h = torch . arange ( self . window_size [ 0 ]) coords_w = torch . arange ( self . window_size [ 1 ]) coords = torch . stack ( torch . meshgrid ([ coords_h , coords_w ])) # 2, Wh, Ww coords_flatten = torch . flatten ( coords , 1 ) # 2, Wh*Ww relative_coords = coords_flatten [:, :, None ] - coords_flatten [:, None , :] # 2, Wh*Ww, Wh*Ww relative_coords = relative_coords . permute ( 1 , 2 , 0 ) . contiguous () # Wh*Ww, Wh*Ww, 2 relative_coords [:, :, 0 ] += self . window_size [ 0 ] - 1 # shift to start from 0 relative_coords [:, :, 1 ] += self . window_size [ 1 ] - 1 relative_coords [:, :, 0 ] *= 2 * self . window_size [ 1 ] - 1 relative_position_index = relative_coords . sum ( - 1 ) # Wh*Ww, Wh*Ww self . register_buffer ( \"relative_position_index\" , relative_position_index ) self . qkv = nn . Linear ( dim , dim * 3 , bias = qkv_bias ) self . attn_drop = nn . Dropout ( attn_drop ) self . proj = nn . Linear ( dim , dim ) self . proj_drop = nn . Dropout ( proj_drop ) trunc_normal_ ( self . relative_position_bias_table , std = .02 ) self . softmax = nn . Softmax ( dim =- 1 ) def forward ( self , x , mask = None ): \"\"\" Args: x (Tensor):: input features with shape of (num_windows*B, N, C) mask (Tensor):: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None \"\"\" B_ , N , C = x . shape qkv = self . qkv ( x ) . reshape ( B_ , N , 3 , self . num_heads , C // self . num_heads ) . permute ( 2 , 0 , 3 , 1 , 4 ) q , k , v = qkv [ 0 ], qkv [ 1 ], qkv [ 2 ] # make torchscript happy (cannot use tensor as tuple) q = q * self . scale attn = ( q @ k . transpose ( - 2 , - 1 )) relative_position_bias = self . relative_position_bias_table [ self . relative_position_index . view ( - 1 )] . view ( self . window_size [ 0 ] * self . window_size [ 1 ], self . window_size [ 0 ] * self . window_size [ 1 ], - 1 ) # Wh*Ww,Wh*Ww,nH relative_position_bias = relative_position_bias . permute ( 2 , 0 , 1 ) . contiguous () # nH, Wh*Ww, Wh*Ww attn = attn + relative_position_bias . unsqueeze ( 0 ) if mask is not None : nW = mask . shape [ 0 ] attn = attn . view ( B_ // nW , nW , self . num_heads , N , N ) + mask . unsqueeze ( 1 ) . unsqueeze ( 0 ) attn = attn . view ( - 1 , self . num_heads , N , N ) attn = self . softmax ( attn ) else : attn = self . softmax ( attn ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B_ , N , C ) x = self . proj ( x ) x = self . proj_drop ( x ) return x def extra_repr ( self ) -> str : return f 'dim= { self . dim } , window_size= { self . window_size } , num_heads= { self . num_heads } ' def flops ( self , N ): # calculate flops for 1 window with token length of N flops = 0 # qkv = self.qkv(x) flops += N * self . dim * 3 * self . dim # attn = (q @ k.transpose(-2, -1)) flops += self . num_heads * N * ( self . dim // self . num_heads ) * N # x = (attn @ v) flops += self . num_heads * N * N * ( self . dim // self . num_heads ) # x = self.proj(x) flops += N * self . dim * self . dim return flops forward ( x , mask = None ) Parameters: x ( Tensor ) \u2013 : input features with shape of (num_windows*B, N, C) mask ( Tensor , default: None ) \u2013 : (0/-inf) mask with shape of (num_windows, Wh Ww, Wh Ww) or None Source code in uchiha\\models\\modules\\swin_transformer.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def forward ( self , x , mask = None ): \"\"\" Args: x (Tensor):: input features with shape of (num_windows*B, N, C) mask (Tensor):: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None \"\"\" B_ , N , C = x . shape qkv = self . qkv ( x ) . reshape ( B_ , N , 3 , self . num_heads , C // self . num_heads ) . permute ( 2 , 0 , 3 , 1 , 4 ) q , k , v = qkv [ 0 ], qkv [ 1 ], qkv [ 2 ] # make torchscript happy (cannot use tensor as tuple) q = q * self . scale attn = ( q @ k . transpose ( - 2 , - 1 )) relative_position_bias = self . relative_position_bias_table [ self . relative_position_index . view ( - 1 )] . view ( self . window_size [ 0 ] * self . window_size [ 1 ], self . window_size [ 0 ] * self . window_size [ 1 ], - 1 ) # Wh*Ww,Wh*Ww,nH relative_position_bias = relative_position_bias . permute ( 2 , 0 , 1 ) . contiguous () # nH, Wh*Ww, Wh*Ww attn = attn + relative_position_bias . unsqueeze ( 0 ) if mask is not None : nW = mask . shape [ 0 ] attn = attn . view ( B_ // nW , nW , self . num_heads , N , N ) + mask . unsqueeze ( 1 ) . unsqueeze ( 0 ) attn = attn . view ( - 1 , self . num_heads , N , N ) attn = self . softmax ( attn ) else : attn = self . softmax ( attn ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B_ , N , C ) x = self . proj ( x ) x = self . proj_drop ( x ) return x window_partition ( x , window_size ) images --> window patches Parameters: x ( Tensor ) \u2013 (B, H, W, C) window_size ( int ) \u2013 window size Returns: windows ( Tensor ) \u2013 (num_windows*B, window_size, window_size, C) Source code in uchiha\\models\\modules\\swin_transformer.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def window_partition ( x , window_size ): \"\"\" images --> window patches Args: x (Tensor): (B, H, W, C) window_size (int): window size Returns: windows (Tensor): (num_windows*B, window_size, window_size, C) \"\"\" B , H , W , C = x . shape x = x . view ( B , H // window_size , window_size , W // window_size , window_size , C ) windows = x . permute ( 0 , 1 , 3 , 2 , 4 , 5 ) . contiguous () . view ( - 1 , window_size , window_size , C ) return windows window_reverse ( windows , window_size , H , W ) window patches --> images Parameters: windows ( Tensor ) \u2013 : (num_windows*B, window_size, window_size, C) window_size ( int ) \u2013 Window size H ( int ) \u2013 Height of image W ( int ) \u2013 Width of image Returns: x ( Tensor ) \u2013 : (B, H, W, C) Source code in uchiha\\models\\modules\\swin_transformer.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def window_reverse ( windows , window_size , H , W ): \"\"\" window patches --> images Args: windows (Tensor):: (num_windows*B, window_size, window_size, C) window_size (int): Window size H (int): Height of image W (int): Width of image Returns: x (Tensor):: (B, H, W, C) \"\"\" B = int ( windows . shape [ 0 ] / ( H * W / window_size / window_size )) x = windows . view ( B , H // window_size , W // window_size , window_size , window_size , - 1 ) x = x . permute ( 0 , 1 , 3 , 2 , 4 , 5 ) . contiguous () . view ( B , H , W , - 1 ) return x Channel Transformer ChannelTransformerLayer Bases: Module Stacked Channel-Transformer-Block Parameters: dim ( int ) \u2013 Number of input channels. input_resolution ( Tuple ( int ) | int ) \u2013 spatial resolution of input features depth ( int ) \u2013 number of stacked channel-transformer-blocks num_heads ( int ) \u2013 Number of heads in Multi-Head Attention mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set drop_rate \u2013 Dropout ratio of output of Attention. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 drop_path ( List | float , default: 0.0 ) \u2013 The probability of DropPath of each ChannelTransformer Block . act_layer ( Module | str , default: GELU ) \u2013 activation function in MLP. norm_layer ( Module | str , default: LayerNorm ) \u2013 normalization layer before Attention and MLP. Default: None Source code in uchiha\\models\\modules\\channel_transformer.py 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 @MODULE . register_module () class ChannelTransformerLayer ( nn . Module ): \"\"\" Stacked Channel-Transformer-Block Args: dim (int): Number of input channels. input_resolution (Tuple(int) | int): spatial resolution of input features depth (int): number of stacked channel-transformer-blocks num_heads (int): Number of heads in `Multi-Head Attention` mlp_ratio (float): ratio of hidden layer to input channel in MLP qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set drop_rate (): Dropout ratio of output of Attention. Default: 0.0 attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 drop_path (List | float): The probability of DropPath of each `ChannelTransformer Block`. act_layer (nn.Module | str): activation function in MLP. norm_layer (nn.Module | str): normalization layer before Attention and MLP. Default: None \"\"\" def __init__ ( self , dim , input_resolution , depth , num_heads , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop_rate = 0. , attn_drop = 0. , drop_path = 0. , act_layer = nn . GELU , norm_layer = nn . LayerNorm , downsample = None ): super () . __init__ () self . dim = dim self . input_resolution = to_2tuple ( input_resolution ) if isinstance ( input_resolution , int ) else input_resolution self . depth = depth self . downsample = build_module ( downsample ) if isinstance ( norm_layer , str ): norm_layer = build_norm ( norm_layer ) if isinstance ( act_layer , str ): act_layer = build_act ( act_layer ) # build blocks self . blocks = nn . ModuleList ([ ChannelTransformerBlock ( dim = dim , input_resolution = self . input_resolution , num_heads = num_heads , mlp_ratio = mlp_ratio , qkv_bias = qkv_bias , qk_scale = qk_scale , drop = drop_rate , attn_drop = attn_drop , drop_path = drop_path [ i ] if isinstance ( drop_path , list ) else drop_path , act_layer = act_layer , norm_layer = norm_layer ) for i in range ( depth )]) def forward ( self , x ): for blk in self . blocks : x = blk ( x ) out = self . downsample ( x ) if self . downsample else x return out ChannelTransformerLayers Bases: Module Collection of Channel-Transformer-Layers Parameters: dims ( List [ int ] ) \u2013 Number of input channels. input_resolutions ( List [ int | Tuple ( int )] ) \u2013 spatial resolution of input features depths ( List [ int ] ) \u2013 number of stacked channel-transformer-blocks num_heads ( List [ int ] ) \u2013 Number of heads in Multi-Head Attention mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set drop_rate \u2013 Dropout ratio of output of Attention. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 drop_path_rate ( float , default: 0.1 ) \u2013 Rate required to generate drop path, it will be called by torch.linspace(0, drop_path_rate, depth) designed to increase the probability of DropPath as the depth increases act_layer ( Module | str , default: GELU ) \u2013 activation function in MLP. norm_layer ( Module | str , default: LayerNorm ) \u2013 normalization layer before Attention and MLP. Default: None Source code in uchiha\\models\\modules\\channel_transformer.py 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 @MODULE . register_module () class ChannelTransformerLayers ( nn . Module ): \"\"\" Collection of Channel-Transformer-Layers Args: dims (List[int]): Number of input channels. input_resolutions (List[int | Tuple(int)]): spatial resolution of input features depths (List[int]): number of stacked channel-transformer-blocks num_heads (List[int]): Number of heads in `Multi-Head Attention` mlp_ratio (float): ratio of hidden layer to input channel in MLP qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set drop_rate (): Dropout ratio of output of Attention. Default: 0.0 attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 drop_path_rate (float): Rate required to generate drop path, it will be called by `torch.linspace(0, drop_path_rate, depth)` designed to increase the probability of DropPath as the depth increases act_layer (nn.Module | str): activation function in MLP. norm_layer (nn.Module | str): normalization layer before Attention and MLP. Default: None \"\"\" def __init__ ( self , dims , input_resolutions , depths , num_heads , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop_rate = 0. , attn_drop = 0. , drop_path_rate = 0.1 , act_layer = nn . GELU , norm_layer = nn . LayerNorm , downsamples = None ): super () . __init__ () self . dims = dims self . input_resolutions = input_resolutions self . depths = depths self . num_heads = num_heads self . num_layers = len ( dims ) self . downsamples = cfg_decomposition ( downsamples ) if len ( self . downsamples ) < self . num_layers : self . downsamples . extend ([ None ] * ( self . num_layers - len ( self . downsamples ))) # stochastic depth self . dpr = [ x . item () for x in torch . linspace ( 0 , drop_path_rate , sum ( depths ))] # build layers self . layers = nn . ModuleList () for i in range ( self . num_layers ): dim = self . dims [ i ] input_resolution = self . input_resolutions [ i ] num_heads = self . num_heads [ i ] depth = self . depths [ i ] downsample = self . downsamples [ i ] drop_path = self . dpr [ sum ( depths [: i ]): sum ( depths [: i + 1 ])] # build layer layer = ChannelTransformerLayer ( dim = dim , input_resolution = input_resolution , depth = depth , num_heads = num_heads , mlp_ratio = mlp_ratio , qkv_bias = qkv_bias , qk_scale = qk_scale , drop_rate = drop_rate , attn_drop = attn_drop , drop_path = drop_path , act_layer = act_layer , norm_layer = norm_layer , downsample = downsample ) self . layers . append ( layer ) UnetChannelTransformerLayers Bases: Module Collection of Channel-Transformer-Layers in the shape of unet Parameters: dims ( List [ int ] ) \u2013 Number of input channels. input_resolutions ( List [ int ] ) \u2013 spatial resolution of input features depths ( List [ int ] ) \u2013 number of stacked channel-transformer-blocks num_heads ( List [ int ] ) \u2013 Number of heads in Multi-Head Attention mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set drop_rate \u2013 Dropout ratio of output of Attention. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 drop_path_rate ( float , default: 0.1 ) \u2013 Rate required to generate drop path, it will be called by torch.linspace(0, drop_path_rate, depth) designed to increase the probability of DropPath as the depth increases act_layer ( Module | str , default: GELU ) \u2013 activation function in MLP. norm_layer ( Module | str , default: LayerNorm ) \u2013 normalization layer before Attention and MLP. Default: None Source code in uchiha\\models\\modules\\channel_transformer.py 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 @MODULE . register_module () class UnetChannelTransformerLayers ( nn . Module ): \"\"\" Collection of Channel-Transformer-Layers in the shape of unet Args: dims (List[int]): Number of input channels. input_resolutions (List[int]): spatial resolution of input features depths (List[int]): number of stacked channel-transformer-blocks num_heads (List[int]): Number of heads in `Multi-Head Attention` mlp_ratio (float): ratio of hidden layer to input channel in MLP qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set drop_rate (): Dropout ratio of output of Attention. Default: 0.0 attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 drop_path_rate (float): Rate required to generate drop path, it will be called by `torch.linspace(0, drop_path_rate, depth)` designed to increase the probability of DropPath as the depth increases act_layer (nn.Module | str): activation function in MLP. norm_layer (nn.Module | str): normalization layer before Attention and MLP. Default: None \"\"\" def __init__ ( self , dims , input_resolutions , depths , num_heads , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop_rate = 0. , attn_drop = 0. , drop_path_rate = 0.1 , act_layer = nn . GELU , norm_layer = nn . LayerNorm , downsamples = None ): super () . __init__ () self . dims = dims self . input_resolutions = input_resolutions self . depths = depths self . num_heads = num_heads self . num_layers = len ( dims ) // 2 self . downsamples = cfg_decomposition ( downsamples ) if len ( self . downsamples ) < self . num_layers * 2 : self . downsamples . extend ([ None ] * ( self . num_layers * 2 - len ( self . downsamples ))) # stochastic depth self . dpr = [ x . item () for x in torch . linspace ( 0 , drop_path_rate , sum ( depths [: len ( depths ) // 2 ]))] self . dec_dpr = self . dpr [:: - 1 ] # build layers self . layers = nn . ModuleList () for i in range ( self . num_layers * 2 ): dim = self . dims [ i ] input_resolution = self . input_resolutions [ i ] depth = self . depths [ i ] num_heads = self . num_heads [ i ] downsample = self . downsamples [ i ] drop_path = self . dpr [ sum ( depths [: i ]): sum ( depths [: i + 1 ])] if i >= self . num_layers : drop_path = self . dec_dpr [ sum ( depths [ self . num_layers : i ]): sum ( depths [ self . num_layers : i + 1 ])] # build layer layer = ChannelTransformerLayer ( dim = dim , input_resolution = input_resolution , depth = depth , num_heads = num_heads , mlp_ratio = mlp_ratio , qkv_bias = qkv_bias , qk_scale = qk_scale , drop_rate = drop_rate , attn_drop = attn_drop , drop_path = drop_path , act_layer = act_layer , norm_layer = norm_layer , downsample = downsample ) self . layers . append ( layer ) Atom ChannelAttention Bases: Module attention operations at the channel draw inspiration from Window based multi-head self attention (W-MSA) Parameters: dim ( int ) \u2013 Number of input channels. num_heads ( int ) \u2013 Number of heads in Multi-Head Attention qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 proj_drop ( float , default: 0.0 ) \u2013 Dropout ratio of output. Default: 0.0 Source code in uchiha\\models\\modules\\channel_transformer.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class ChannelAttention ( nn . Module ): r \"\"\" attention operations at the channel draw inspiration from Window based multi-head self attention (W-MSA) Args: dim (int): Number of input channels. num_heads (int): Number of heads in `Multi-Head Attention` qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 proj_drop (float, optional): Dropout ratio of output. Default: 0.0 \"\"\" def __init__ ( self , dim , num_heads , qkv_bias = True , qk_scale = None , attn_drop = 0. , proj_drop = 0. ): super () . __init__ () self . dim = dim self . num_heads = num_heads head_dim = dim // num_heads self . scale = qk_scale or head_dim ** - 0.5 self . qkv = nn . Linear ( dim , dim * 3 , bias = qkv_bias ) self . v = nn . Identity () self . attn_drop = nn . Dropout ( attn_drop ) self . proj = nn . Linear ( dim , dim ) self . proj_drop = nn . Dropout ( proj_drop ) self . softmax = nn . Softmax ( dim =- 1 ) def forward ( self , x ): \"\"\" Args: x (Tensor):: input features with shape of (num_windows*B, N, C) \"\"\" B_ , N , C = x . shape qkv = self . qkv ( x ) qkv = qkv . reshape ( B_ , N , 3 , C ) . permute ( 2 , 0 , 1 , 3 ) q = qkv [ 0 ] . reshape ( B_ , N , self . num_heads , C // self . num_heads ) . permute ( 0 , 2 , 1 , 3 ) k = qkv [ 1 ] . reshape ( B_ , N , self . num_heads , C // self . num_heads ) . permute ( 0 , 2 , 1 , 3 ) v = qkv [ 2 ] v = self . v ( v ) v = v . reshape ( B_ , N , self . num_heads , C // self . num_heads ) . permute ( 0 , 2 , 1 , 3 ) q = q . transpose ( - 2 , - 1 ) k = k . transpose ( - 2 , - 1 ) v = v . transpose ( - 2 , - 1 ) q = F . normalize ( q , dim =- 1 , p = 2 ) k = F . normalize ( k , dim =- 1 , p = 2 ) attn = ( k @ q . transpose ( - 2 , - 1 )) attn = attn * self . scale attn = attn . softmax ( dim =- 1 ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B_ , N , C ) x = self . proj ( x ) x = self . proj_drop ( x ) return x forward ( x ) Parameters: x ( Tensor ) \u2013 : input features with shape of (num_windows*B, N, C) Source code in uchiha\\models\\modules\\channel_transformer.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def forward ( self , x ): \"\"\" Args: x (Tensor):: input features with shape of (num_windows*B, N, C) \"\"\" B_ , N , C = x . shape qkv = self . qkv ( x ) qkv = qkv . reshape ( B_ , N , 3 , C ) . permute ( 2 , 0 , 1 , 3 ) q = qkv [ 0 ] . reshape ( B_ , N , self . num_heads , C // self . num_heads ) . permute ( 0 , 2 , 1 , 3 ) k = qkv [ 1 ] . reshape ( B_ , N , self . num_heads , C // self . num_heads ) . permute ( 0 , 2 , 1 , 3 ) v = qkv [ 2 ] v = self . v ( v ) v = v . reshape ( B_ , N , self . num_heads , C // self . num_heads ) . permute ( 0 , 2 , 1 , 3 ) q = q . transpose ( - 2 , - 1 ) k = k . transpose ( - 2 , - 1 ) v = v . transpose ( - 2 , - 1 ) q = F . normalize ( q , dim =- 1 , p = 2 ) k = F . normalize ( k , dim =- 1 , p = 2 ) attn = ( k @ q . transpose ( - 2 , - 1 )) attn = attn * self . scale attn = attn . softmax ( dim =- 1 ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B_ , N , C ) x = self . proj ( x ) x = self . proj_drop ( x ) return x ChannelAttentionV2 Bases: Module Channel Attention V2: Linear in spatial Parameters: sequence ( int ) \u2013 length of input sequences. factor ( float ) \u2013 Control the spatial Linear qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 proj_drop ( float , default: 0.0 ) \u2013 Dropout ratio of output. Default: 0.0 Source code in uchiha\\models\\modules\\channel_transformer.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class ChannelAttentionV2 ( nn . Module ): r \"\"\" Channel Attention V2: Linear in spatial Args: sequence (int): length of input sequences. factor (float): Control the spatial `Linear` qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 proj_drop (float, optional): Dropout ratio of output. Default: 0.0 \"\"\" def __init__ ( self , sequence , factor , qkv_bias = True , qk_scale = None , attn_drop = 0. , proj_drop = 0. ): super () . __init__ () self . sequence = sequence self . factor = factor self . scale = qk_scale or sequence ** - 0.5 self . inner_len = int ( self . sequence * self . factor ) self . qk = nn . Linear ( sequence , self . inner_len * 2 , bias = qkv_bias ) self . v = nn . Linear ( sequence , sequence , bias = qkv_bias ) self . attn_drop = nn . Dropout ( attn_drop ) self . proj = nn . Linear ( sequence , sequence ) self . proj_drop = nn . Dropout ( proj_drop ) self . softmax = nn . Softmax ( dim =- 1 ) def forward ( self , x ): \"\"\" Args: x (Tensor):: input features with shape of (B, N, C) \"\"\" B_ , C , N = x . shape N_ = int ( N * self . factor ) qk = self . qk ( x ) qk = qk . reshape ( B_ , C , 2 , N_ ) . permute ( 2 , 0 , 1 , 3 ) q = qk [ 0 ] k = qk [ 1 ] v = self . v ( x ) q = F . normalize ( q , dim =- 1 , p = 2 ) k = F . normalize ( k , dim =- 1 , p = 2 ) attn = ( k @ q . transpose ( - 2 , - 1 )) attn = attn * self . scale attn = attn . softmax ( dim =- 1 ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B_ , C , N ) x = self . proj ( x ) x = self . proj_drop ( x ) return x forward ( x ) Parameters: x ( Tensor ) \u2013 : input features with shape of (B, N, C) Source code in uchiha\\models\\modules\\channel_transformer.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def forward ( self , x ): \"\"\" Args: x (Tensor):: input features with shape of (B, N, C) \"\"\" B_ , C , N = x . shape N_ = int ( N * self . factor ) qk = self . qk ( x ) qk = qk . reshape ( B_ , C , 2 , N_ ) . permute ( 2 , 0 , 1 , 3 ) q = qk [ 0 ] k = qk [ 1 ] v = self . v ( x ) q = F . normalize ( q , dim =- 1 , p = 2 ) k = F . normalize ( k , dim =- 1 , p = 2 ) attn = ( k @ q . transpose ( - 2 , - 1 )) attn = attn * self . scale attn = attn . softmax ( dim =- 1 ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B_ , C , N ) x = self . proj ( x ) x = self . proj_drop ( x ) return x ChannelTransformerBlock Bases: Module The block containing Channel Attention, norm and MLP DropPath: Randomly dropout the entire path, usually at the network structure level, dropout a computational path, such as the residual path of the network or some sub-module in the Transformer. Parameters: dim ( int ) \u2013 Number of input channels. input_resolution ( Tuple(int ) \u2013 spatial resolution of input features num_heads ( int ) \u2013 Number of heads in Multi-Head Attention mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set drop ( float , default: 0.0 ) \u2013 Dropout ratio of output of AttentionBlock. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 drop_path ( float , default: 0.0 ) \u2013 Dropout ratio of entire path act_layer ( Module , default: GELU ) \u2013 activation function norm_layer ( Module , default: LayerNorm ) \u2013 normalization layer before Attention and MLP. Default: None Source code in uchiha\\models\\modules\\channel_transformer.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 class ChannelTransformerBlock ( nn . Module ): \"\"\" The block containing Channel Attention, norm and MLP DropPath: Randomly dropout the entire path, usually at the network structure level, dropout a computational path, such as the residual path of the network or some sub-module in the Transformer. Args: dim (int): Number of input channels. input_resolution (Tuple(int)): spatial resolution of input features num_heads (int): Number of heads in `Multi-Head Attention` mlp_ratio (float): ratio of hidden layer to input channel in MLP qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set drop (float): Dropout ratio of output of AttentionBlock. Default: 0.0 attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 drop_path (float): Dropout ratio of entire path act_layer (nn.Module): activation function norm_layer (nn.Module): normalization layer before Attention and MLP. Default: None \"\"\" def __init__ ( self , dim , input_resolution , num_heads , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop = 0. , attn_drop = 0. , drop_path = 0. , act_layer = nn . GELU , norm_layer = nn . LayerNorm ): super () . __init__ () self . dim = dim self . input_resolution = input_resolution self . num_heads = num_heads self . mlp_ratio = mlp_ratio self . norm1 = norm_layer ( dim ) self . attn_C = ChannelAttention ( dim , num_heads = num_heads , qkv_bias = qkv_bias , qk_scale = qk_scale , attn_drop = attn_drop , proj_drop = drop ) self . drop_path : nn . Module = DropPath ( drop_path ) if drop_path > 0. else nn . Identity () self . norm2 = norm_layer ( dim ) mlp_hidden_dim = int ( dim * mlp_ratio ) self . mlp = MLP ( in_features = dim , hidden_features = mlp_hidden_dim , act_layer = act_layer , drop = drop ) def forward ( self , x ): H , W = self . input_resolution B , L , C = x . shape assert L == H * W , \"input feature has wrong size\" shortcut = x x = self . norm1 ( x ) # W-MSA/SW-MSA x = self . attn_C ( x ) # FFN x = shortcut + self . drop_path ( x ) x = x + self . drop_path ( self . mlp ( self . norm2 ( x ))) return x ChannelTransformerBlockV2 Bases: Module The block containing Channel Attention, norm and MLP DropPath: Randomly dropout the entire path, usually at the network structure level, dropout a computational path, such as the residual path of the network or some sub-module in the Transformer. Parameters: sequence ( int ) \u2013 length of input sequences. input_resolution ( Tuple(int ) \u2013 spatial resolution of input features num_heads ( int ) \u2013 Number of heads in Multi-Head Attention factor ( float ) \u2013 Control the spatial Linear mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set drop ( float , default: 0.0 ) \u2013 Dropout ratio of output of AttentionBlock. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 drop_path ( float , default: 0.0 ) \u2013 Dropout ratio of entire path act_layer ( Module , default: GELU ) \u2013 activation function norm_layer ( Module , default: LayerNorm ) \u2013 normalization layer before Attention and MLP. Default: None Source code in uchiha\\models\\modules\\channel_transformer.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 class ChannelTransformerBlockV2 ( nn . Module ): \"\"\" The block containing Channel Attention, norm and MLP DropPath: Randomly dropout the entire path, usually at the network structure level, dropout a computational path, such as the residual path of the network or some sub-module in the Transformer. Args: sequence (int): length of input sequences. input_resolution (Tuple(int)): spatial resolution of input features num_heads (int): Number of heads in `Multi-Head Attention` factor (float): Control the spatial `Linear` mlp_ratio (float): ratio of hidden layer to input channel in MLP qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set drop (float): Dropout ratio of output of AttentionBlock. Default: 0.0 attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 drop_path (float): Dropout ratio of entire path act_layer (nn.Module): activation function norm_layer (nn.Module): normalization layer before Attention and MLP. Default: None \"\"\" def __init__ ( self , sequence , input_resolution , factor , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop = 0. , attn_drop = 0. , drop_path = 0. , act_layer = nn . GELU , norm_layer = nn . LayerNorm ): super () . __init__ () self . sequence = sequence self . input_resolution = input_resolution self . mlp_ratio = mlp_ratio self . norm1 = norm_layer ( sequence ) self . attn_C = ChannelAttentionV2 ( sequence , factor = factor , qkv_bias = qkv_bias , qk_scale = qk_scale , attn_drop = attn_drop , proj_drop = drop ) self . drop_path : nn . Module = DropPath ( drop_path ) if drop_path > 0. else nn . Identity () self . norm2 = norm_layer ( sequence ) mlp_hidden_dim = int ( sequence * mlp_ratio ) self . mlp = MLP ( in_features = sequence , hidden_features = mlp_hidden_dim , act_layer = act_layer , drop = drop ) def forward ( self , x ): H , W = self . input_resolution B , L , C = x . shape assert L == H * W , \"input feature has wrong size\" x = x . permute ( 0 , 2 , 1 ) shortcut = x x = self . norm1 ( x ) # W-MSA/SW-MSA x = self . attn_C ( x ) # FFN x = shortcut + self . drop_path ( x ) x = x + self . drop_path ( self . mlp ( self . norm2 ( x ))) return x . permute ( 0 , 2 , 1 )","title":"Module"},{"location":"mkdocs/module.html#modules","text":"","title":"Modules"},{"location":"mkdocs/module.html#atom","text":"","title":"Atom"},{"location":"mkdocs/module.html#downsample","text":"","title":"Downsample"},{"location":"mkdocs/module.html#models.modules.downsample.DownsampleConv","text":"Bases: Module Conv with stride > 1 to downsample image Parameters: in_channel ( int , default: 512 ) \u2013 Number of input channels. Default: 512 out_channel ( int , default: 1024 ) \u2013 Number of output channels. Default: 1024 kernel_size ( int , default: 3 ) \u2013 Kernel size for Conv . Default: 3 stride ( int , default: 2 ) \u2013 Stride for Conv . Default: 2 padding ( int , default: 1 ) \u2013 Padding for Conv . Default: 1 Source code in uchiha\\models\\modules\\downsample.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @MODULE . register_module () class DownsampleConv ( nn . Module ): \"\"\" `Conv` with stride > 1 to downsample image Args: in_channel (int): Number of input channels. Default: 512 out_channel (int): Number of output channels. Default: 1024 kernel_size (int): Kernel size for `Conv`. Default: 3 stride (int): Stride for `Conv`. Default: 2 padding (int): Padding for `Conv`. Default: 1 \"\"\" def __init__ ( self , in_channel = 512 , out_channel = 1024 , kernel_size = 3 , stride = 2 , padding = 1 ): super () . __init__ () self . downsample = nn . Conv2d ( in_channel , out_channel , kernel_size , stride , padding ) self . activate = nn . GELU () def forward ( self , x ): if len ( x . shape ) == 3 : B , L , C = x . shape H , W = int ( math . sqrt ( L )), int ( math . sqrt ( L )) x = x . view ( B , H , W , C ) . permute ( 0 , 3 , 1 , 2 ) out = self . activate ( self . downsample ( x )) return out . flatten ( 2 ) . transpose ( 1 , 2 ) else : return self . activate ( self . downsample ( x ))","title":"DownsampleConv"},{"location":"mkdocs/module.html#models.modules.downsample.PatchMerging","text":"Bases: Module Patch Merging Layer. Parameters: input_resolution ( tuple [ int ] ) \u2013 Resolution of input feature. in_channel ( int ) \u2013 Number of input channels. norm_layer ( Module , default: LayerNorm ) \u2013 Normalization layer. Default: nn.LayerNorm Source code in uchiha\\models\\modules\\downsample.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 @MODULE . register_module () class PatchMerging ( nn . Module ): r \"\"\" Patch Merging Layer. Args: input_resolution (tuple[int]): Resolution of input feature. in_channel (int): Number of input channels. norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm \"\"\" def __init__ ( self , input_resolution , in_channel , norm_layer = nn . LayerNorm ): super () . __init__ () self . input_resolution = to_2tuple ( input_resolution ) self . dim = in_channel self . reduction = nn . Linear ( 4 * in_channel , 2 * in_channel , bias = False ) self . norm = norm_layer ( 4 * in_channel ) def forward ( self , x ): H , W = self . input_resolution B , L , C = x . shape assert L == H * W , \"input feature has wrong size\" assert H % 2 == 0 and W % 2 == 0 , f \"x size ( { H } * { W } ) are not even.\" x = x . view ( B , H , W , C ) x0 = x [:, 0 :: 2 , 0 :: 2 , :] # B H/2 W/2 C x1 = x [:, 1 :: 2 , 0 :: 2 , :] # B H/2 W/2 C x2 = x [:, 0 :: 2 , 1 :: 2 , :] # B H/2 W/2 C x3 = x [:, 1 :: 2 , 1 :: 2 , :] # B H/2 W/2 C x = torch . cat ([ x0 , x1 , x2 , x3 ], - 1 ) # B H/2 W/2 4*C x = x . view ( B , - 1 , 4 * C ) # B H/2*W/2 4*C x = self . norm ( x ) x = self . reduction ( x ) return x def extra_repr ( self ) -> str : return f \"dim= { self . dim } , input_resolution= { self . input_resolution } , depth= { self . depth } \" def flops ( self ): flops = 0 for blk in self . blocks : flops += blk . flops () if self . downsample is not None : flops += self . downsample . flops () return flops","title":"PatchMerging"},{"location":"mkdocs/module.html#models.modules.downsample.PixelUnShuffle","text":"Bases: Module PixelUnShuffle to image and sequence After PixelUnShuffle , a Conv / Linear layer is used to map the data to the output dimension. Parameters: factor ( int , default: 2 ) \u2013 downsample factor in_channel ( int , default: 512 ) \u2013 Number of input channels. out_channel ( int , default: 1024 ) \u2013 Number of output channels. Source code in uchiha\\models\\modules\\downsample.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @MODULE . register_module () class PixelUnShuffle ( nn . Module ): \"\"\" `PixelUnShuffle` to image and sequence After `PixelUnShuffle`, a `Conv`/`Linear` layer is used to map the data to the output dimension. Args: factor (int): downsample factor in_channel (int): Number of input channels. out_channel (int): Number of output channels. \"\"\" def __init__ ( self , factor = 2 , in_channel = 512 , out_channel = 1024 ): super () . __init__ () self . downsample = nn . PixelUnshuffle ( downscale_factor = factor ) self . fc = nn . Linear ( in_channel * 4 , out_channel ) self . norm = nn . LayerNorm ( out_channel ) def forward ( self , x ): B , L , C = x . shape H , W = int ( math . sqrt ( L )), int ( math . sqrt ( L )) downsample = self . downsample ( x . view ( B , C , H , W )) fc = self . fc ( downsample . flatten ( 2 ) . transpose ( 1 , 2 )) return self . norm ( fc )","title":"PixelUnShuffle"},{"location":"mkdocs/module.html#upsample","text":"","title":"Upsample"},{"location":"mkdocs/module.html#models.modules.upsample.PixelShuffle","text":"Bases: Module PixelShuffle to image and sequence After PixelShuffle , a Conv/Linear layer id used to map the data to the output dimension. Parameters: factor ( int , default: 2 ) \u2013 upsample factor in_channel ( int , default: 1024 ) \u2013 Number of input channels. out_channel ( int , default: 512 ) \u2013 Number of output channels. Source code in uchiha\\models\\modules\\upsample.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @MODULE . register_module () class PixelShuffle ( nn . Module ): \"\"\" `PixelShuffle` to image and sequence After `PixelShuffle`, a Conv/Linear layer id used to map the data to the output dimension. Args: factor (int): upsample factor in_channel (int): Number of input channels. out_channel (int): Number of output channels. \"\"\" def __init__ ( self , factor = 2 , in_channel = 1024 , out_channel = 512 ): super () . __init__ () self . upsample = nn . PixelShuffle ( upscale_factor = factor ) self . fc = nn . Linear ( in_channel // 4 , out_channel ) def forward ( self , x ): B , L , C = x . shape H , W = int ( math . sqrt ( L )), int ( math . sqrt ( L )) upsample = self . upsample ( x . view ( B , C , H , W )) fc = self . fc ( upsample . flatten ( 2 ) . transpose ( 1 , 2 )) return fc","title":"PixelShuffle"},{"location":"mkdocs/module.html#embedding","text":"","title":"Embedding"},{"location":"mkdocs/module.html#models.modules.embedding.PatchEmbedding","text":"Bases: Module Image to Patch Embedding (B, C, H, W) --> (B, L, _C) L = (H * W) / (patch_size ** 2) _C = embed_dim Parameters: img_size ( int , default: 4 ) \u2013 Image size. Default: 4. patch_size ( int , default: 1 ) \u2013 Patch token size. Default: 1. in_channel ( int , default: 330 ) \u2013 Number of input image channels. Default: 330. embed_dim ( int , default: 512 ) \u2013 Number of Conv projection output channels. Default: 512. norm_layer ( Module , default: None ) \u2013 Normalization layer. Default: None sequence ( boolean , default: True ) \u2013 if True, output shape: (B, L, C) else (B, C, H, W) Source code in uchiha\\models\\modules\\embedding.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @MODULE . register_module () class PatchEmbedding ( nn . Module ): r \"\"\" Image to Patch Embedding (B, C, H, W) --> (B, L, _C) L = (H * W) / (patch_size ** 2) _C = embed_dim Args: img_size (int): Image size. Default: 4. patch_size (int): Patch token size. Default: 1. in_channel (int): Number of input image channels. Default: 330. embed_dim (int): Number of Conv projection output channels. Default: 512. norm_layer (nn.Module, optional): Normalization layer. Default: None sequence (boolean): if True, output shape: (B, L, C) else (B, C, H, W) \"\"\" def __init__ ( self , img_size = 4 , patch_size = 1 , in_channel = 330 , embed_dim = 512 , norm_layer = None , sequence = True ): super () . __init__ () assert img_size % patch_size == 0 , \\ f 'img_size: { img_size } cannot be divided by patch_size: { patch_size } ' self . img_size : tuple = to_2tuple ( img_size ) self . patch_size : tuple = to_2tuple ( patch_size ) patches_resolution = [ self . img_size [ 0 ] // self . patch_size [ 0 ], self . img_size [ 1 ] // self . patch_size [ 1 ]] self . patches_resolution = patches_resolution self . num_patches = patches_resolution [ 0 ] * patches_resolution [ 1 ] self . sequence = sequence self . in_channel = in_channel self . embed_dim = embed_dim self . proj = nn . Sequential ( nn . Conv2d ( in_channel , embed_dim , kernel_size = patch_size , stride = patch_size , bias = False ) ) norm_layer = build_norm ( norm_layer ) self . norm = norm_layer ( embed_dim ) def forward ( self , x ): B , C , H , W = x . shape assert H == self . img_size [ 0 ] and W == self . img_size [ 1 ], \\ f \"Input image size ( { H } * { W } ) doesn't match model ( { self . img_size [ 0 ] } * { self . img_size [ 1 ] } ).\" x = self . proj ( x ) x = x . flatten ( 2 ) . transpose ( 1 , 2 ) # B Ph*Pw C if self . norm is not None : x = self . norm ( x ) if not self . sequence : x = x . transpose ( 1 , 2 ) . view ( B , self . embed_dim , * self . patches_resolution ) return x def flops ( self ): Ho , Wo = self . patches_resolution flops = Ho * Wo * self . embed_dim * self . in_chans * ( self . patch_size [ 0 ] * self . patch_size [ 1 ]) if self . norm is not None : flops += Ho * Wo * self . embed_dim return flops","title":"PatchEmbedding"},{"location":"mkdocs/module.html#models.modules.embedding.TokenEmbedding","text":"Bases: Module Sequence to Patch Embedding (B, L, W) --> (B, L, _C) _C = embed_dim Parameters: in_channel ( int ) \u2013 Number of input image channels. embed_dim ( int ) \u2013 Number of linear projection output channels. norm_layer ( Module ) \u2013 Normalization layer. Source code in uchiha\\models\\modules\\embedding.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 @MODULE . register_module () class TokenEmbedding ( nn . Module ): r \"\"\" Sequence to Patch Embedding (B, L, W) --> (B, L, _C) _C = embed_dim Args: in_channel (int): Number of input image channels. embed_dim (int): Number of linear projection output channels. norm_layer (nn.Module, optional): Normalization layer. \"\"\" def __init__ ( self , in_channel , embed_dim , norm_layer ): super () . __init__ () self . in_channel = in_channel self . embed_dim = embed_dim self . proj = nn . Sequential ( nn . Linear ( in_channel , embed_dim , bias = False ), nn . GELU (), nn . Linear ( embed_dim , embed_dim , bias = False ), nn . GELU () ) if norm_layer is not None : if norm_layer == 'nn.LayerNorm' : self . norm = nn . LayerNorm ( embed_dim ) else : self . norm = norm_layer ( embed_dim ) else : self . norm = None def forward ( self , x ): if len ( x . shape ) == 4 : x = x . flatten ( 2 ) . transpose ( 1 , 2 ) # B C H W -> B L C x = self . proj ( x ) if self . norm is not None : x = self . norm ( x ) return x","title":"TokenEmbedding"},{"location":"mkdocs/module.html#head","text":"","title":"Head"},{"location":"mkdocs/module.html#models.modules.head.FCHead","text":"Bases: Module Fully Connected Head Parameters: embed_dim ( int ) \u2013 Number of input channels. Default: 512. pred_num ( int ) \u2013 Number of prediction mode ( str , default: 'sequence' ) \u2013 If set to 'sequence', apply predictions to (B, L, C), else (B, C, H, W) post_process ( str , default: None ) \u2013 processor after Linear Source code in uchiha\\models\\modules\\head.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @MODULE . register_module () class FCHead ( nn . Module ): \"\"\" Fully Connected Head Args: embed_dim (int): Number of input channels. Default: 512. pred_num (int): Number of prediction mode (str): If set to 'sequence', apply predictions to (B, L, C), else (B, C, H, W) post_process (str): processor after `Linear ` \"\"\" def __init__ ( self , embed_dim , pred_num , mode = 'sequence' , post_process = None ): super () . __init__ () if mode == 'sequence' : self . pooling = nn . AdaptiveAvgPool1d ( 1 ) else : self . pooling = nn . AdaptiveAvgPool2d ( 1 ) self . head = nn . Linear ( embed_dim , pred_num ) if post_process == 'RELU' : self . activate = nn . ReLU () else : self . activate = nn . Identity () def forward ( self , x ): if len ( x . shape ) == 3 : # B,L,C = x.shape pooling = self . pooling ( x . transpose ( 1 , 2 )) . squeeze ( 2 ) else : # B,C,H,W = x.shape pooling = self . pooling ( x ) . squeeze ( - 1 ) . squeeze ( - 1 ) out = self . head ( pooling ) return self . activate ( out )","title":"FCHead"},{"location":"mkdocs/module.html#pre-process","text":"","title":"Pre-process"},{"location":"mkdocs/module.html#models.modules.preprocessor.DWT1d","text":"Bases: Module 1D Discrete Wavelet Transform Parameters: scales ( int , default: 1 ) \u2013 Number of scales (number of transform performed) wave ( str , default: 'haar' ) \u2013 The type of wavelet padding ( str , default: 'zero' ) \u2013 data padding mode before transformation origin ( bool , default: False ) \u2013 Whether to return original data. Source code in uchiha\\models\\modules\\preprocessor.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @MODULE . register_module () class DWT1d ( nn . Module ): \"\"\" 1D Discrete Wavelet Transform Args: scales (int): Number of scales (number of transform performed) wave (str): The type of wavelet padding (str): data padding mode before transformation origin (bool): Whether to return original data. \"\"\" def __init__ ( self , scales = 1 , wave = 'haar' , padding = 'zero' , origin = False ): super () . __init__ () self . wavelet_transform = DWT1DForward ( J = scales , wave = wave , mode = padding ) self . origin = origin def forward ( self , x ): # x.shape:B, C, H, W if len ( x . shape ) == 4 : x = x . flatten ( 2 ) . transpose ( 1 , 2 ) L , H = self . wavelet_transform ( x ) parallel = [ sequence_to_image ( L )] for idx , h in enumerate ( H ): parallel . append ( sequence_to_image ( h )) if self . origin : return x , parallel else : return parallel","title":"DWT1d"},{"location":"mkdocs/module.html#models.modules.preprocessor.DWT2d","text":"Bases: Module 2D Discrete Wavelet Transform Parameters: scales ( int , default: 1 ) \u2013 Number of scales (number of transform performed) wave ( str , default: 'haar' ) \u2013 The type of wavelet padding ( str , default: 'zero' ) \u2013 data padding mode before transformation Source code in uchiha\\models\\modules\\preprocessor.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 @MODULE . register_module () class DWT2d ( nn . Module ): \"\"\" 2D Discrete Wavelet Transform Args: scales (int): Number of scales (number of transform performed) wave (str): The type of wavelet padding (str): data padding mode before transformation \"\"\" def __init__ ( self , scales = 1 , wave = 'haar' , padding = 'zero' ): super () . __init__ () self . wavelet_transform = DWTForward ( J = scales , wave = wave , mode = padding ) def forward ( self , x ): # x.shape:B, C, H, W out = self . wavelet_transform ( x ) LL , H = out LH , HL , HH = H [ 0 ][:, :, 0 , :, :], H [ 0 ][:, :, 1 , :, :], H [ 0 ][:, :, 2 , :, :] return torch . cat (( LL , LH , HL , HH ), dim = 1 )","title":"DWT2d"},{"location":"mkdocs/module.html#models.modules.preprocessor.GroupRCP","text":"Bases: Module Group-based Residue Channel Prior Parameters: split_bands ( List , default: None ) \u2013 \u5212\u5206\u7684\u6ce2\u6bb5\u7d22\u5f15\uff0c\u5728\u5176\u4e24\u4fa7\u8fdb\u884c\u6b8b\u5dee\u8fd0\u7b97 Source code in uchiha\\models\\modules\\preprocessor.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 @MODULE . register_module () class GroupRCP ( nn . Module ): \"\"\" Group-based Residue Channel Prior Args: split_bands (List): \u5212\u5206\u7684\u6ce2\u6bb5\u7d22\u5f15\uff0c\u5728\u5176\u4e24\u4fa7\u8fdb\u884c\u6b8b\u5dee\u8fd0\u7b97 \"\"\" def __init__ ( self , split_bands = None , strategy = None ): super () . __init__ () self . split_bands = split_bands if strategy is None : self . strategy = [ 'sorted' , 'serial' ] else : self . strategy = strategy def forward ( self , x ): # x.shape (B, C, H, W) rcp = [] for s in self . strategy : if s == 'sorted' : rcp . append ( get_sorted_rcp ( x , self . split_bands )) elif s == 'serial' : rcp . append ( get_serial_rcp ( x , self . split_bands )) elif s == 'sorted_pw' : rcp . append ( get_sorted_rcp_pw ( x , self . split_bands )) else : raise ValueError ( f \"Invalid strategy: { s } \" ) rcp = torch . cat ( rcp , dim = 1 ) return rcp","title":"GroupRCP"},{"location":"mkdocs/module.html#models.modules.preprocessor.get_sorted_rcp_pw","text":"\u8f93\u5165: CHW torch.Tensor \u529f\u80fd: \u6392\u5e8f + \u5207\u5206\u70b9\u5dee\u503c \u8f93\u51fa: HWN torch.Tensor Source code in uchiha\\models\\modules\\preprocessor.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def get_sorted_rcp_pw ( image : torch . Tensor , split_bands : list ) -> torch . Tensor : \"\"\" \u8f93\u5165: CHW torch.Tensor \u529f\u80fd: \u6392\u5e8f + \u5207\u5206\u70b9\u5dee\u503c \u8f93\u51fa: HWN torch.Tensor \"\"\" c , h , w = image . shape flattened = image . permute ( 1 , 2 , 0 ) . reshape ( - 1 , c ) # L x C sorted_flattened , _ = torch . sort ( flattened , dim = 1 ) results = [] for split_idx in split_bands : if not ( 0 < split_idx < c ): raise ValueError ( f \"\u5207\u5206\u70b9 { split_idx } \u8d85\u51fa\u901a\u9053\u8303\u56f4 (1~ { c - 1 } )\" ) front = sorted_flattened [:, : split_idx ] back = sorted_flattened [:, split_idx :] diff = front . mean ( dim = 1 ) - back . mean ( dim = 1 ) results . append ( diff ) output = torch . stack ( results , dim = 1 ) # L x N return output . reshape ( h , w , - 1 ) # HWN","title":"get_sorted_rcp_pw"},{"location":"mkdocs/module.html#post-process","text":"","title":"Post-process"},{"location":"mkdocs/module.html#models.modules.postprocessor.IDWT1d","text":"Bases: Module 1D Inverse Discrete Wavelet Transform Parameters: wave ( str , default: 'haar' ) \u2013 The type of wavelet in_channel ( int , default: 1024 ) \u2013 Number of input channels. out_channel ( int , default: 1 ) \u2013 Number of output channels. Source code in uchiha\\models\\modules\\postprocessor.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @MODULE . register_module () class IDWT1d ( nn . Module ): \"\"\" 1D Inverse Discrete Wavelet Transform Args: wave (str): The type of wavelet in_channel (int): Number of input channels. out_channel (int): Number of output channels. \"\"\" def __init__ ( self , wave = 'haar' , in_channel = 1024 , out_channel = 1 ): super () . __init__ () self . IDWT = DWT1DInverse ( wave = wave ) self . pooling = nn . AdaptiveAvgPool1d ( 1 ) self . fc = nn . Linear ( in_channel , out_channel ) def forward ( self , x ): L , H = x H = ( H ,) out = self . IDWT (( L , H )) pooling = self . pooling ( out . transpose ( 1 , 2 )) . squeeze ( 2 ) return self . fc ( pooling )","title":"IDWT1d"},{"location":"mkdocs/module.html#models.modules.postprocessor.IDWT2d","text":"Bases: Module 2D Inverse Discrete Wavelet Transform Parameters: wave ( str , default: 'haar' ) \u2013 The type of wavelet Source code in uchiha\\models\\modules\\postprocessor.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @MODULE . register_module () class IDWT2d ( nn . Module ): \"\"\" 2D Inverse Discrete Wavelet Transform Args: wave (str): The type of wavelet \"\"\" def __init__ ( self , wave = 'haar' ): super () . __init__ () self . IDWT = DWTInverse ( wave = wave ) def forward ( self , x ): # x.shape:B, C, H, W out = self . wavlet_transform ( x ) LL , H = out LH , HL , HH = H [ 0 ][:, :, 0 , :, :], H [ 0 ][:, :, 1 , :, :], H [ 0 ][:, :, 2 , :, :] return torch . cat (( LL , LH , HL , HH ), dim = 1 )","title":"IDWT2d"},{"location":"mkdocs/module.html#models.modules.postprocessor.WeightedSum","text":"Bases: Module assign weights to multiple inputs and accumulate If a list is provided, it will be weighted according to the value of the list, if not, the mean weight will be assigned initially and updated with backward propagation. Parameters: weights \u2013 weights of each input Source code in uchiha\\models\\modules\\postprocessor.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 @MODULE . register_module () class WeightedSum ( nn . Module ): \"\"\" assign weights to multiple inputs and accumulate If a list is provided, it will be weighted according to the value of the list, if not, the mean weight will be assigned initially and updated with backward propagation. Args: weights (): weights of each input \"\"\" def __init__ ( self , weights ): super () . __init__ () if isinstance ( weights , list ): self . weights = weights else : self . weights = nn . ParameterList ([ nn . Parameter ( torch . tensor ( 1 / weights )) for _ in range ( weights )]) def forward ( self , x : List [ Tensor ]): result = torch . zeros_like ( x [ 0 ]) for idx , parallel in enumerate ( x ): result += self . weights [ idx ] * parallel return result","title":"WeightedSum"},{"location":"mkdocs/module.html#fusion","text":"","title":"Fusion"},{"location":"mkdocs/module.html#models.modules.fusion.CatPWConvFusion","text":"Bases: Module Concat followed by PW-Conv Parameters: in_channels ( int ) \u2013 Number of input channels. out_channels ( int , default: None ) \u2013 Number of output channels. Source code in uchiha\\models\\modules\\fusion.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @MODULE . register_module () class CatPWConvFusion ( nn . Module ): \"\"\" Concat followed by PW-Conv Args: in_channels (int): Number of input channels. out_channels (int): Number of output channels. \"\"\" def __init__ ( self , in_channels , out_channels = None ): super () . __init__ () self . in_channels = in_channels if out_channels is None : out_channels = in_channels // 2 self . fusion = nn . Conv2d ( in_channels , out_channels , 1 ) def forward ( self , x , y ): # x y.shape: (B, C, H, W ) cat = torch . cat ([ x , y ], dim = 1 ) out = self . fusion ( cat ) # B H*W C return out","title":"CatPWConvFusion"},{"location":"mkdocs/module.html#resnet","text":"","title":"ResNet"},{"location":"mkdocs/module.html#models.modules.basic_resnet.BasicResidualBlock","text":"Bases: Module basic residual block (RB) Parameters: inplanes ( int ) \u2013 number of input channels planes ( int ) \u2013 number of output channels stride ( int , default: 1 ) \u2013 the stride for the convolution operation. Default: 1 downsample ( Module , default: None ) \u2013 operations performed when residual is applied. Default: None groups ( int , default: 1 ) \u2013 the groups for the convolution operation. Default: 1 base_width ( int , default: 64 ) \u2013 base width of image. Default: 64 dilation ( int , default: 1 ) \u2013 the dilation for the convolution operation. Default: 1 norm_layer ( Module , default: None ) \u2013 normalization layer after convolution layer. Default: None Source code in uchiha\\models\\modules\\basic_resnet.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 @MODULE . register_module () class BasicResidualBlock ( nn . Module ): \"\"\" basic residual block (RB) Args: inplanes (int): number of input channels planes (int): number of output channels stride (int): the stride for the convolution operation. Default: 1 downsample (nn.Module): operations performed when residual is applied. Default: None groups (int): the groups for the convolution operation. Default: 1 base_width (int): base width of image. Default: 64 dilation (int): the dilation for the convolution operation. Default: 1 norm_layer (nn.Module): normalization layer after convolution layer. Default: None \"\"\" expansion : int = 1 def __init__ ( self , inplanes : int , planes : int , stride : int = 1 , downsample : Optional [ nn . Module ] = None , groups : int = 1 , base_width : int = 64 , dilation : int = 1 , norm_layer : Optional [ Callable [ ... , nn . Module ]] = None ) -> None : super ( BasicResidualBlock , self ) . __init__ () if norm_layer is None : norm_layer = nn . BatchNorm2d if groups != 1 or base_width != 64 : raise ValueError ( 'BasicBlock only supports groups=1 and base_width=64' ) if dilation > 1 : raise NotImplementedError ( \"Dilation > 1 not supported in BasicBlock\" ) # Both self.conv1 and self.downsample layers downsample the input when stride != 1 self . conv1 = conv3x3 ( inplanes , planes , stride ) self . bn1 = norm_layer ( planes ) self . relu = nn . ReLU ( inplace = True ) self . conv2 = conv3x3 ( planes , planes ) self . bn2 = norm_layer ( planes ) self . downsample = downsample self . stride = stride def forward ( self , x : Tensor ) -> Tensor : identity = x out = self . conv1 ( x ) out = self . bn1 ( out ) out = self . relu ( out ) out = self . conv2 ( out ) out = self . bn2 ( out ) if self . downsample is not None : identity = self . downsample ( x ) out += identity out = self . relu ( out ) return out","title":"BasicResidualBlock"},{"location":"mkdocs/module.html#models.modules.basic_resnet.ResidualBottleneck","text":"Bases: Module basic residual bottleneck Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2) while original implementation places the stride at the first 1x1 convolution(self.conv1) according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385. This variant is also known as ResNet V1.5 and improves accuracy according to https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch. Parameters: inplanes ( int ) \u2013 number of input channels planes ( int ) \u2013 number of output channels stride ( int , default: 1 ) \u2013 the stride for the convolution operation. Default: 1 downsample ( Module , default: None ) \u2013 operations performed when residual is applied. Default: None groups ( int , default: 1 ) \u2013 the groups for the convolution operation. Default: 1 base_width ( int , default: 64 ) \u2013 base width of image. Default: 64 dilation ( int , default: 1 ) \u2013 the dilation for the convolution operation. Default: 1 norm_layer ( Module , default: None ) \u2013 normalization layer after convolution layer. Default: None Source code in uchiha\\models\\modules\\basic_resnet.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 @MODULE . register_module () class ResidualBottleneck ( nn . Module ): \"\"\" basic residual bottleneck Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2) while original implementation places the stride at the first 1x1 convolution(self.conv1) according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385. This variant is also known as ResNet V1.5 and improves accuracy according to https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch. Args: inplanes (int): number of input channels planes (int): number of output channels stride (int): the stride for the convolution operation. Default: 1 downsample (nn.Module): operations performed when residual is applied. Default: None groups (int): the groups for the convolution operation. Default: 1 base_width (int): base width of image. Default: 64 dilation (int): the dilation for the convolution operation. Default: 1 norm_layer (nn.Module): normalization layer after convolution layer. Default: None \"\"\" expansion : int = 4 def __init__ ( self , inplanes : int , planes : int , stride : int = 1 , downsample : Optional [ nn . Module ] = None , groups : int = 1 , base_width : int = 64 , dilation : int = 1 , norm_layer : Optional [ Callable [ ... , nn . Module ]] = None ) -> None : super ( ResidualBottleneck , self ) . __init__ () if norm_layer is None : norm_layer = nn . BatchNorm2d width = int ( planes * ( base_width / 64. )) * groups # Both self.conv2 and self.downsample layers downsample the input when stride != 1 self . conv1 = conv1x1 ( inplanes , width ) self . bn1 = norm_layer ( width ) self . conv2 = conv3x3 ( width , width , stride , groups , dilation ) self . bn2 = norm_layer ( width ) self . conv3 = conv1x1 ( width , planes * self . expansion ) self . bn3 = norm_layer ( planes * self . expansion ) self . relu = nn . ReLU ( inplace = True ) self . downsample = downsample self . stride = stride def forward ( self , x : Tensor ) -> Tensor : identity = x out = self . conv1 ( x ) out = self . bn1 ( out ) out = self . relu ( out ) out = self . conv2 ( out ) out = self . bn2 ( out ) out = self . relu ( out ) out = self . conv3 ( out ) out = self . bn3 ( out ) if self . downsample is not None : identity = self . downsample ( x ) out += identity out = self . relu ( out ) return out","title":"ResidualBottleneck"},{"location":"mkdocs/module.html#vit","text":"","title":"ViT"},{"location":"mkdocs/module.html#models.modules.basic_transformer.SimpleVisionTransformerLayer","text":"Bases: Module Vision Transformer (the simplest implementation) Parameters: dim ( int ) \u2013 input dimension depth ( int ) \u2013 number of stacked basic-transformer-blocks num_heads ( int ) \u2013 number of heads in Multi-Head Attention sequence_length ( int ) \u2013 The length of the sequence after changing the shape to (B ,L, C). qkv_bias ( bool ) \u2013 If True, add a learnable bias to query, key, value. Default: True mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP. Default: 4.0. cls ( bool , default: False ) \u2013 Class token will be used if set to True. pos ( bool , default: False ) \u2013 Position encoding will be used if set to True. dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 @MODULE . register_module () class SimpleVisionTransformerLayer ( nn . Module ): \"\"\" Vision Transformer (the simplest implementation) Args: dim (int): input dimension depth (int): number of stacked basic-transformer-blocks num_heads (int): number of heads in `Multi-Head Attention` sequence_length (int): The length of the sequence after changing the shape to (B ,L, C). qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True mlp_ratio (float): ratio of hidden layer to input channel in MLP. Default: 4.0. cls (bool): Class token will be used if set to True. pos (bool): Position encoding will be used if set to True. dropout (float): the rate of `Dropout` layer. Default: 0.0 \"\"\" def __init__ ( self , dim , depth , num_heads , sequence_length , qkv_bias , mlp_ratio = 4. , cls = False , pos = False , dropout = 0. ): super () . __init__ () self . sequence_length = sequence_length self . pos = pos self . cls = cls if cls : self . cls_token = nn . Parameter ( torch . zeros ( 1 , 1 , dim )) self . sequence_length += 1 if pos : self . pos_embed = nn . Parameter ( torch . zeros ( 1 , self . sequence_length , dim )) self . pos_drop = nn . Dropout ( dropout ) mlp_dim = int ( dim * mlp_ratio ) self . blocks = nn . Sequential ( * [ Block ( dim , num_heads , qkv_bias , mlp_dim , dropout ) for _ in range ( depth ) ]) self . norm = nn . LayerNorm ( dim ) if self . pos : nn . init . trunc_normal_ ( self . pos_embed , std = 0.02 ) if self . cls : nn . init . trunc_normal_ ( self . cls_token , std = 0.02 ) self . apply ( _init_vit_weights ) def forward ( self , x ): if self . cls : cls_token = self . cls_token . expand ( x . shape [ 0 ], - 1 , - 1 ) x = torch . cat (( cls_token , x ), dim = 1 ) if self . pos : x = x + self . pos_embed x = self . pos_drop ( x ) x = self . blocks ( x ) x = self . norm ( x ) return x [:, 0 ] . unsqueeze ( 1 ) if self . cls else x","title":"SimpleVisionTransformerLayer"},{"location":"mkdocs/module.html#atom_1","text":"","title":"Atom"},{"location":"mkdocs/module.html#models.modules.basic_transformer.Attention","text":"Bases: Module self attention module Parameters: dim ( int ) \u2013 input dimension num_heads ( int , default: 8 ) \u2013 number of heads in Multi-Head Attention qkv_bias ( bool , default: False ) \u2013 If True, add a learnable bias to query, key, value. Default: True dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer after qkv calculation. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class Attention ( nn . Module ): \"\"\" self attention module Args: dim (int): input dimension num_heads (int): number of heads in `Multi-Head Attention` qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True dropout (float): the rate of `Dropout` layer after qkv calculation. Default: 0.0 \"\"\" def __init__ ( self , dim , num_heads = 8 , qkv_bias = False , dropout = 0. ): super () . __init__ () self . num_heads = num_heads head_dim = dim // num_heads self . scale = head_dim ** - 0.5 self . qkv = nn . Linear ( dim , dim * 3 , bias = qkv_bias ) self . attn_drop = nn . Dropout ( dropout ) self . proj = nn . Linear ( dim , dim ) self . proj_drop = nn . Dropout ( dropout ) def forward ( self , x ): B , N , C = x . shape qkv = self . qkv ( x ) . reshape ( B , N , 3 , self . num_heads , C // self . num_heads ) . permute ( 2 , 0 , 3 , 1 , 4 ) q , k , v = qkv [ 0 ], qkv [ 1 ], qkv [ 2 ] attn = ( q @ k . transpose ( - 2 , - 1 )) * self . scale attn = attn . softmax ( dim =- 1 ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B , N , C ) x = self . proj ( x ) x = self . proj_drop ( x ) return x","title":"Attention"},{"location":"mkdocs/module.html#models.modules.basic_transformer.Block","text":"Bases: Module basic Transformer block Parameters: dim ( int ) \u2013 input dimension num_heads ( int ) \u2013 number of heads in Multi-Head Attention qkv_bias ( bool ) \u2013 If True, add a learnable bias to query, key, value. Default: True mlp_dim ( int ) \u2013 dimension of hidden layers in FeedForward dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class Block ( nn . Module ): \"\"\" basic Transformer block Args: dim (int): input dimension num_heads (int): number of heads in `Multi-Head Attention` qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True mlp_dim (int): dimension of hidden layers in FeedForward dropout (float): the rate of `Dropout` layer. Default: 0.0 \"\"\" def __init__ ( self , dim , num_heads , qkv_bias , mlp_dim , dropout = 0. ): super () . __init__ () self . norm1 = nn . LayerNorm ( dim ) self . attn = Attention ( dim , num_heads , qkv_bias , dropout ) self . drop = nn . Dropout ( dropout ) self . norm2 = nn . LayerNorm ( dim ) self . mlp = FeedForward ( dim , mlp_dim , dropout ) def forward ( self , x ): x = x + self . drop ( self . attn ( self . norm1 ( x ))) x = x + self . drop ( self . mlp ( self . norm2 ( x ))) return x","title":"Block"},{"location":"mkdocs/module.html#models.modules.basic_transformer.FeedForward","text":"Bases: Module Feedforward-Network Parameters: dim ( int ) \u2013 input dimension hidden_dim ( int ) \u2013 dimension of hidden layers dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class FeedForward ( nn . Module ): \"\"\" Feedforward-Network Args: dim (int): input dimension hidden_dim (int): dimension of hidden layers dropout (float): the rate of `Dropout` layer. Default: 0.0 \"\"\" def __init__ ( self , dim , hidden_dim , dropout = 0. ): super () . __init__ () self . net = nn . Sequential ( # nn.LayerNorm(dim), nn . Linear ( dim , hidden_dim ), nn . GELU (), nn . Dropout ( dropout ), nn . Linear ( hidden_dim , dim ), nn . Dropout ( dropout ) ) def forward ( self , x ): return self . net ( x )","title":"FeedForward"},{"location":"mkdocs/module.html#models.modules.basic_transformer._init_vit_weights","text":"ViT weight initialization Parameters: m ( Module ) \u2013 modules that need to initialize weights Source code in uchiha\\models\\modules\\basic_transformer.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def _init_vit_weights ( m ): \"\"\" ViT weight initialization Args: m (nn.Module): modules that need to initialize weights \"\"\" if isinstance ( m , nn . Linear ): nn . init . trunc_normal_ ( m . weight , std = .01 ) if m . bias is not None : nn . init . zeros_ ( m . bias ) elif isinstance ( m , nn . Conv2d ): nn . init . kaiming_normal_ ( m . weight , mode = \"fan_out\" ) if m . bias is not None : nn . init . zeros_ ( m . bias ) elif isinstance ( m , nn . LayerNorm ): nn . init . zeros_ ( m . bias ) nn . init . ones_ ( m . weight )","title":"_init_vit_weights"},{"location":"mkdocs/module.html#cbam","text":"","title":"CBAM"},{"location":"mkdocs/module.html#models.modules.cbam.BasicCAM","text":"Bases: Module basic Channel Attention Module(CAM) Parameters: in_channel ( int ) \u2013 number of input channels out_channel ( int ) \u2013 number of output channels stride ( int , default: 1 ) \u2013 the stride for the convolution operation. Default: 1 downsample ( dict , default: None ) \u2013 operations performed when residual is applied. Default: None Source code in uchiha\\models\\modules\\cbam.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 @MODULE . register_module () class BasicCAM ( nn . Module ): \"\"\" basic Channel Attention Module(CAM) Args: in_channel (int): number of input channels out_channel (int): number of output channels stride (int): the stride for the convolution operation. Default: 1 downsample (dict): operations performed when residual is applied. Default: None \"\"\" expansion = 1 def __init__ ( self , in_channel , out_channel , stride = 1 , downsample = None ): super ( BasicCAM , self ) . __init__ () self . conv1 = conv3x3 ( in_channel , out_channel , stride ) self . bn1 = nn . BatchNorm2d ( out_channel ) self . relu = nn . ReLU ( inplace = True ) self . conv2 = conv3x3 ( out_channel , out_channel ) self . bn2 = nn . BatchNorm2d ( out_channel ) self . ca = CAM ( out_channel ) self . downsample_opt = build_module ( downsample ) self . stride = stride def forward ( self , x ): residual = x out = self . conv1 ( x ) out = self . bn1 ( out ) out = self . relu ( out ) out = self . conv2 ( out ) out = self . bn2 ( out ) out = self . ca ( out ) * out if self . downsample_opt is not None : residual = self . downsample_opt ( x ) out += residual out = self . relu ( out ) return out","title":"BasicCAM"},{"location":"mkdocs/module.html#models.modules.cbam.BasicSAM","text":"Bases: Module basic Spatial Attention Module(SAM) Parameters: in_channel ( int ) \u2013 number of input channels out_channel ( int ) \u2013 number of output channels stride ( int , default: 1 ) \u2013 the stride for the convolution operation. Default: 1 downsample ( dict , default: None ) \u2013 operations performed when residual is applied. Default: None Source code in uchiha\\models\\modules\\cbam.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 @MODULE . register_module () class BasicSAM ( nn . Module ): \"\"\" basic Spatial Attention Module(SAM) Args: in_channel (int): number of input channels out_channel (int): number of output channels stride (int): the stride for the convolution operation. Default: 1 downsample (dict): operations performed when residual is applied. Default: None \"\"\" expansion = 1 def __init__ ( self , in_channel , out_channel , stride = 1 , downsample = None ): super ( BasicSAM , self ) . __init__ () self . conv1 = conv3x3 ( in_channel , out_channel , stride ) self . bn1 = nn . BatchNorm2d ( out_channel ) self . relu = nn . ReLU ( inplace = True ) self . conv2 = conv3x3 ( out_channel , out_channel ) self . bn2 = nn . BatchNorm2d ( out_channel ) self . sa = SAM () self . downsample_opt = build_module ( downsample ) self . stride = stride def forward ( self , x ): residual = x out = self . conv1 ( x ) out = self . bn1 ( out ) out = self . relu ( out ) out = self . conv2 ( out ) out = self . bn2 ( out ) out = self . sa ( out ) * out if self . downsample_opt is not None : residual = self . downsample_opt ( x ) out += residual out = self . relu ( out ) return out","title":"BasicSAM"},{"location":"mkdocs/module.html#models.modules.cbam.BasicCBAM","text":"Bases: Module Conv*2 ==> CAM ==> SAM ==> Parameters: inplanes ( int ) \u2013 number of input channels planes ( int ) \u2013 number of output channels stride ( int , default: 1 ) \u2013 the stride for the convolution operation. Default: 1 downsample ( Module , default: None ) \u2013 operations performed when residual is applied. Default: None Source code in uchiha\\models\\modules\\cbam.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 class BasicCBAM ( nn . Module ): \"\"\" Conv*2 ==> CAM ==> SAM ==> Args: inplanes (int): number of input channels planes (int): number of output channels stride (int): the stride for the convolution operation. Default: 1 downsample (nn.Module): operations performed when residual is applied. Default: None \"\"\" expansion = 1 def __init__ ( self , inplanes , planes , stride = 1 , downsample = None ): super ( BasicCBAM , self ) . __init__ () self . conv1 = conv3x3 ( inplanes , planes , stride ) self . bn1 = nn . BatchNorm2d ( planes ) self . relu = nn . ReLU ( inplace = True ) self . conv2 = conv3x3 ( planes , planes ) self . bn2 = nn . BatchNorm2d ( planes ) self . ca = CAM ( planes ) self . sa = SAM () self . downsample = downsample self . stride = stride def forward ( self , x ): residual = x out = self . conv1 ( x ) out = self . bn1 ( out ) out = self . relu ( out ) out = self . conv2 ( out ) out = self . bn2 ( out ) out = self . ca ( out ) * out out = self . sa ( out ) * out if self . downsample is not None : residual = self . downsample ( x ) out += residual out = self . relu ( out ) return out","title":"BasicCBAM"},{"location":"mkdocs/module.html#models.modules.cbam.CBAMBottleneck","text":"Bases: Module Conv*3 ==> CAM ==> SAM ==> the actual number of output channels is expanded by expansion Parameters: inplanes ( int ) \u2013 number of input channels planes ( int ) \u2013 number of output channels stride ( int , default: 1 ) \u2013 the stride for the convolution operation. Default: 1 downsample ( Module , default: None ) \u2013 operations performed when residual is applied. Default: None Source code in uchiha\\models\\modules\\cbam.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 class CBAMBottleneck ( nn . Module ): \"\"\" Conv*3 ==> CAM ==> SAM ==> the actual number of output channels is expanded by expansion Args: inplanes (int): number of input channels planes (int): number of output channels stride (int): the stride for the convolution operation. Default: 1 downsample (nn.Module): operations performed when residual is applied. Default: None \"\"\" expansion = 4 def __init__ ( self , inplanes , planes , stride = 1 , downsample = None ): super ( CBAMBottleneck , self ) . __init__ () self . conv1 = nn . Conv2d ( inplanes , planes , kernel_size = 1 , bias = False ) self . bn1 = nn . BatchNorm2d ( planes ) self . conv2 = nn . Conv2d ( planes , planes , kernel_size = 3 , stride = stride , padding = 1 , bias = False ) self . bn2 = nn . BatchNorm2d ( planes ) self . conv3 = nn . Conv2d ( planes , planes * self . expansion , kernel_size = 1 , bias = False ) self . bn3 = nn . BatchNorm2d ( planes * self . expansion ) self . relu = nn . ReLU ( inplace = True ) self . ca = CAM ( planes * self . expansion ) self . sa = SAM () self . downsample = downsample self . stride = stride def forward ( self , x ): residual = x out = self . conv1 ( x ) out = self . bn1 ( out ) out = self . relu ( out ) out = self . conv2 ( out ) out = self . bn2 ( out ) out = self . relu ( out ) out = self . conv3 ( out ) out = self . bn3 ( out ) out = self . ca ( out ) * out out = self . sa ( out ) * out if self . downsample is not None : residual = self . downsample ( x ) out += residual out = self . relu ( out ) return out","title":"CBAMBottleneck"},{"location":"mkdocs/module.html#atom_2","text":"","title":"Atom"},{"location":"mkdocs/module.html#models.modules.basic_transformer.Attention","text":"Bases: Module self attention module Parameters: dim ( int ) \u2013 input dimension num_heads ( int , default: 8 ) \u2013 number of heads in Multi-Head Attention qkv_bias ( bool , default: False ) \u2013 If True, add a learnable bias to query, key, value. Default: True dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer after qkv calculation. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class Attention ( nn . Module ): \"\"\" self attention module Args: dim (int): input dimension num_heads (int): number of heads in `Multi-Head Attention` qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True dropout (float): the rate of `Dropout` layer after qkv calculation. Default: 0.0 \"\"\" def __init__ ( self , dim , num_heads = 8 , qkv_bias = False , dropout = 0. ): super () . __init__ () self . num_heads = num_heads head_dim = dim // num_heads self . scale = head_dim ** - 0.5 self . qkv = nn . Linear ( dim , dim * 3 , bias = qkv_bias ) self . attn_drop = nn . Dropout ( dropout ) self . proj = nn . Linear ( dim , dim ) self . proj_drop = nn . Dropout ( dropout ) def forward ( self , x ): B , N , C = x . shape qkv = self . qkv ( x ) . reshape ( B , N , 3 , self . num_heads , C // self . num_heads ) . permute ( 2 , 0 , 3 , 1 , 4 ) q , k , v = qkv [ 0 ], qkv [ 1 ], qkv [ 2 ] attn = ( q @ k . transpose ( - 2 , - 1 )) * self . scale attn = attn . softmax ( dim =- 1 ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B , N , C ) x = self . proj ( x ) x = self . proj_drop ( x ) return x","title":"Attention"},{"location":"mkdocs/module.html#models.modules.basic_transformer.Block","text":"Bases: Module basic Transformer block Parameters: dim ( int ) \u2013 input dimension num_heads ( int ) \u2013 number of heads in Multi-Head Attention qkv_bias ( bool ) \u2013 If True, add a learnable bias to query, key, value. Default: True mlp_dim ( int ) \u2013 dimension of hidden layers in FeedForward dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 class Block ( nn . Module ): \"\"\" basic Transformer block Args: dim (int): input dimension num_heads (int): number of heads in `Multi-Head Attention` qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True mlp_dim (int): dimension of hidden layers in FeedForward dropout (float): the rate of `Dropout` layer. Default: 0.0 \"\"\" def __init__ ( self , dim , num_heads , qkv_bias , mlp_dim , dropout = 0. ): super () . __init__ () self . norm1 = nn . LayerNorm ( dim ) self . attn = Attention ( dim , num_heads , qkv_bias , dropout ) self . drop = nn . Dropout ( dropout ) self . norm2 = nn . LayerNorm ( dim ) self . mlp = FeedForward ( dim , mlp_dim , dropout ) def forward ( self , x ): x = x + self . drop ( self . attn ( self . norm1 ( x ))) x = x + self . drop ( self . mlp ( self . norm2 ( x ))) return x","title":"Block"},{"location":"mkdocs/module.html#models.modules.basic_transformer.FeedForward","text":"Bases: Module Feedforward-Network Parameters: dim ( int ) \u2013 input dimension hidden_dim ( int ) \u2013 dimension of hidden layers dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 class FeedForward ( nn . Module ): \"\"\" Feedforward-Network Args: dim (int): input dimension hidden_dim (int): dimension of hidden layers dropout (float): the rate of `Dropout` layer. Default: 0.0 \"\"\" def __init__ ( self , dim , hidden_dim , dropout = 0. ): super () . __init__ () self . net = nn . Sequential ( # nn.LayerNorm(dim), nn . Linear ( dim , hidden_dim ), nn . GELU (), nn . Dropout ( dropout ), nn . Linear ( hidden_dim , dim ), nn . Dropout ( dropout ) ) def forward ( self , x ): return self . net ( x )","title":"FeedForward"},{"location":"mkdocs/module.html#models.modules.basic_transformer.SimpleVisionTransformerLayer","text":"Bases: Module Vision Transformer (the simplest implementation) Parameters: dim ( int ) \u2013 input dimension depth ( int ) \u2013 number of stacked basic-transformer-blocks num_heads ( int ) \u2013 number of heads in Multi-Head Attention sequence_length ( int ) \u2013 The length of the sequence after changing the shape to (B ,L, C). qkv_bias ( bool ) \u2013 If True, add a learnable bias to query, key, value. Default: True mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP. Default: 4.0. cls ( bool , default: False ) \u2013 Class token will be used if set to True. pos ( bool , default: False ) \u2013 Position encoding will be used if set to True. dropout ( float , default: 0.0 ) \u2013 the rate of Dropout layer. Default: 0.0 Source code in uchiha\\models\\modules\\basic_transformer.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 @MODULE . register_module () class SimpleVisionTransformerLayer ( nn . Module ): \"\"\" Vision Transformer (the simplest implementation) Args: dim (int): input dimension depth (int): number of stacked basic-transformer-blocks num_heads (int): number of heads in `Multi-Head Attention` sequence_length (int): The length of the sequence after changing the shape to (B ,L, C). qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True mlp_ratio (float): ratio of hidden layer to input channel in MLP. Default: 4.0. cls (bool): Class token will be used if set to True. pos (bool): Position encoding will be used if set to True. dropout (float): the rate of `Dropout` layer. Default: 0.0 \"\"\" def __init__ ( self , dim , depth , num_heads , sequence_length , qkv_bias , mlp_ratio = 4. , cls = False , pos = False , dropout = 0. ): super () . __init__ () self . sequence_length = sequence_length self . pos = pos self . cls = cls if cls : self . cls_token = nn . Parameter ( torch . zeros ( 1 , 1 , dim )) self . sequence_length += 1 if pos : self . pos_embed = nn . Parameter ( torch . zeros ( 1 , self . sequence_length , dim )) self . pos_drop = nn . Dropout ( dropout ) mlp_dim = int ( dim * mlp_ratio ) self . blocks = nn . Sequential ( * [ Block ( dim , num_heads , qkv_bias , mlp_dim , dropout ) for _ in range ( depth ) ]) self . norm = nn . LayerNorm ( dim ) if self . pos : nn . init . trunc_normal_ ( self . pos_embed , std = 0.02 ) if self . cls : nn . init . trunc_normal_ ( self . cls_token , std = 0.02 ) self . apply ( _init_vit_weights ) def forward ( self , x ): if self . cls : cls_token = self . cls_token . expand ( x . shape [ 0 ], - 1 , - 1 ) x = torch . cat (( cls_token , x ), dim = 1 ) if self . pos : x = x + self . pos_embed x = self . pos_drop ( x ) x = self . blocks ( x ) x = self . norm ( x ) return x [:, 0 ] . unsqueeze ( 1 ) if self . cls else x","title":"SimpleVisionTransformerLayer"},{"location":"mkdocs/module.html#models.modules.basic_transformer._init_vit_weights","text":"ViT weight initialization Parameters: m ( Module ) \u2013 modules that need to initialize weights Source code in uchiha\\models\\modules\\basic_transformer.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def _init_vit_weights ( m ): \"\"\" ViT weight initialization Args: m (nn.Module): modules that need to initialize weights \"\"\" if isinstance ( m , nn . Linear ): nn . init . trunc_normal_ ( m . weight , std = .01 ) if m . bias is not None : nn . init . zeros_ ( m . bias ) elif isinstance ( m , nn . Conv2d ): nn . init . kaiming_normal_ ( m . weight , mode = \"fan_out\" ) if m . bias is not None : nn . init . zeros_ ( m . bias ) elif isinstance ( m , nn . LayerNorm ): nn . init . zeros_ ( m . bias ) nn . init . ones_ ( m . weight )","title":"_init_vit_weights"},{"location":"mkdocs/module.html#swin-transformer","text":"","title":"Swin Transformer"},{"location":"mkdocs/module.html#models.modules.swin_transformer.SwinTransformerLayer","text":"Bases: Module A basic Swin Transformer layer for one stage. Parameters: dim ( int ) \u2013 Number of input channels. input_resolution ( tuple [ int ] | int ) \u2013 Input resolution. depth ( int ) \u2013 Number of blocks. num_heads ( int ) \u2013 Number of attention heads. window_size ( int ) \u2013 Local window size. mlp_ratio ( float , default: 4.0 ) \u2013 Ratio of mlp hidden dim to embedding dim. qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set. drop_rate ( float , default: 0.0 ) \u2013 Dropout rate. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Attention dropout rate. Default: 0.0 drop_path ( float | tuple [ float ] , default: 0.0 ) \u2013 Stochastic depth rate. Default: 0.0 act_layer ( Module | str , default: GELU ) \u2013 activation function in MLP. norm_layer ( Module , default: LayerNorm ) \u2013 Normalization layer. Default: nn.LayerNorm downsample ( Module | None , default: None ) \u2013 Downsample layer at the end of the layer. Default: None Source code in uchiha\\models\\modules\\swin_transformer.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 @MODULE . register_module () class SwinTransformerLayer ( nn . Module ): \"\"\" A basic Swin Transformer layer for one stage. Args: dim (int): Number of input channels. input_resolution (tuple[int] | int): Input resolution. depth (int): Number of blocks. num_heads (int): Number of attention heads. window_size (int): Local window size. mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set. drop_rate (float, optional): Dropout rate. Default: 0.0 attn_drop (float, optional): Attention dropout rate. Default: 0.0 drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0 act_layer (nn.Module | str): activation function in MLP. norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None \"\"\" def __init__ ( self , dim , input_resolution , depth , num_heads , window_size , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop_rate = 0. , attn_drop = 0. , drop_path = 0. , act_layer = nn . GELU , norm_layer = nn . LayerNorm , downsample = None ): super () . __init__ () self . dim = dim self . input_resolution = to_2tuple ( input_resolution ) self . depth = depth self . downsample = build_module ( downsample ) if isinstance ( norm_layer , str ): norm_layer = build_norm ( norm_layer ) if isinstance ( act_layer , str ): act_layer = build_act ( act_layer ) # build blocks self . blocks = nn . ModuleList ([ SwinTransformerBlock ( dim = dim , input_resolution = self . input_resolution , num_heads = num_heads , window_size = window_size , shift_size = 0 if ( i % 2 == 0 ) else window_size // 2 , mlp_ratio = mlp_ratio , qkv_bias = qkv_bias , qk_scale = qk_scale , drop = drop_rate , attn_drop = attn_drop , drop_path = drop_path [ i ] if isinstance ( drop_path , list ) else drop_path , act_layer = act_layer , norm_layer = norm_layer , ) for i in range ( depth )]) def forward ( self , x ): for blk in self . blocks : x = blk ( x ) out = self . downsample ( x ) if self . downsample else x return out def extra_repr ( self ) -> str : return f \"dim= { self . dim } , input_resolution= { self . input_resolution } , depth= { self . depth } \" def flops ( self ): flops = 0 for blk in self . blocks : flops += blk . flops () if self . downsample is not None : flops += self . downsample . flops () return flops","title":"SwinTransformerLayer"},{"location":"mkdocs/module.html#models.modules.swin_transformer.SwinTransformerLayers","text":"Bases: Module Collection of Channel-Transformer-Layers Parameters: dims ( List [ int ] ) \u2013 Number of input channels. input_resolutions ( List [ int | Tuple ( int )] ) \u2013 spatial resolution of input features depths ( List [ int ] ) \u2013 number of stacked channel-transformer-blocks num_heads ( List [ int ] ) \u2013 Number of heads in Multi-Head Attention mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set drop_rate \u2013 Dropout ratio of output of Attention. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 drop_path_rate ( float , default: 0.1 ) \u2013 Rate required to generate drop path, it will be called by torch.linspace(0, drop_path_rate, depth) designed to increase the probability of DropPath as the depth increases act_layer ( Module | str , default: GELU ) \u2013 activation function in MLP. norm_layer ( Module | str , default: LayerNorm ) \u2013 normalization layer before Attention and MLP. Default: None Source code in uchiha\\models\\modules\\swin_transformer.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 @MODULE . register_module () class SwinTransformerLayers ( nn . Module ): \"\"\" Collection of Channel-Transformer-Layers Args: dims (List[int]): Number of input channels. input_resolutions (List[int | Tuple(int)]): spatial resolution of input features depths (List[int]): number of stacked channel-transformer-blocks num_heads (List[int]): Number of heads in `Multi-Head Attention` mlp_ratio (float): ratio of hidden layer to input channel in MLP qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set drop_rate (): Dropout ratio of output of Attention. Default: 0.0 attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 drop_path_rate (float): Rate required to generate drop path, it will be called by `torch.linspace(0, drop_path_rate, depth)` designed to increase the probability of DropPath as the depth increases act_layer (nn.Module | str): activation function in MLP. norm_layer (nn.Module | str): normalization layer before Attention and MLP. Default: None \"\"\" def __init__ ( self , dims , input_resolutions , depths , num_heads , window_sizes , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop_rate = 0. , attn_drop = 0. , drop_path_rate = 0.1 , act_layer = nn . GELU , norm_layer = nn . LayerNorm , downsamples = None ): super () . __init__ () self . dims = dims self . input_resolutions = input_resolutions self . depths = depths self . num_heads = num_heads self . window_sizes = window_sizes self . num_layers = len ( dims ) self . downsamples = cfg_decomposition ( downsamples ) if len ( self . downsamples ) < self . num_layers : self . downsamples . extend ([ None ] * ( self . num_layers - len ( self . downsamples ))) # stochastic depth self . dpr = [ x . item () for x in torch . linspace ( 0 , drop_path_rate , sum ( depths ))] # build layers self . layers = nn . ModuleList () for i in range ( self . num_layers ): dim = self . dims [ i ] input_resolution = self . input_resolutions [ i ] num_heads = self . num_heads [ i ] window_size = self . window_sizes [ i ] depth = self . depths [ i ] downsample = self . downsamples [ i ] drop_path = self . dpr [ sum ( depths [: i ]): sum ( depths [: i + 1 ])] # build layer layer = SwinTransformerLayer ( dim = dim , input_resolution = input_resolution , depth = depth , num_heads = num_heads , window_size = window_size , mlp_ratio = mlp_ratio , qkv_bias = qkv_bias , qk_scale = qk_scale , drop_rate = drop_rate , attn_drop = attn_drop , drop_path = drop_path , act_layer = act_layer , norm_layer = norm_layer , downsample = downsample ) self . layers . append ( layer )","title":"SwinTransformerLayers"},{"location":"mkdocs/module.html#atom_3","text":"","title":"Atom"},{"location":"mkdocs/module.html#models.modules.swin_transformer.SwinTransformerBlock","text":"Bases: Module Swin Transformer Block. Parameters: dim ( int ) \u2013 Number of input channels. input_resolution ( tuple [ int ] ) \u2013 Input resolution. num_heads ( int ) \u2013 Number of attention heads. window_size ( int , default: 7 ) \u2013 Window size. shift_size ( int , default: 0 ) \u2013 Shift size for SW-MSA. mlp_ratio ( float , default: 4.0 ) \u2013 Ratio of mlp hidden dim to embedding dim. qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set. drop ( float , default: 0.0 ) \u2013 Dropout rate. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Attention dropout rate. Default: 0.0 drop_path ( float , default: 0.0 ) \u2013 Stochastic depth rate. Default: 0.0 act_layer ( Module , default: GELU ) \u2013 Activation layer. Default: nn.GELU norm_layer ( Module , default: LayerNorm ) \u2013 Normalization layer. Default: nn.LayerNorm Source code in uchiha\\models\\modules\\swin_transformer.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 class SwinTransformerBlock ( nn . Module ): r \"\"\" Swin Transformer Block. Args: dim (int): Number of input channels. input_resolution (tuple[int]): Input resolution. num_heads (int): Number of attention heads. window_size (int): Window size. shift_size (int): Shift size for SW-MSA. mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set. drop (float, optional): Dropout rate. Default: 0.0 attn_drop (float, optional): Attention dropout rate. Default: 0.0 drop_path (float, optional): Stochastic depth rate. Default: 0.0 act_layer (nn.Module, optional): Activation layer. Default: nn.GELU norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm \"\"\" def __init__ ( self , dim , input_resolution , num_heads , window_size = 7 , shift_size = 0 , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop = 0. , attn_drop = 0. , drop_path = 0. , act_layer = nn . GELU , norm_layer = nn . LayerNorm ): super () . __init__ () self . dim = dim self . input_resolution = input_resolution self . num_heads = num_heads self . window_size = window_size self . shift_size = shift_size self . mlp_ratio = mlp_ratio if min ( self . input_resolution ) <= self . window_size : # if window size is larger than input resolution, we don't partition windows self . shift_size = 0 self . window_size = min ( self . input_resolution ) assert 0 <= self . shift_size < self . window_size , \"shift_size must in 0-window_size\" self . norm1 = norm_layer ( dim ) self . attn = WindowAttention ( dim , window_size = to_2tuple ( self . window_size ), num_heads = num_heads , qkv_bias = qkv_bias , qk_scale = qk_scale , attn_drop = attn_drop , proj_drop = drop ) self . drop_path = DropPath ( drop_path ) if drop_path > 0. else nn . Identity () self . norm2 = norm_layer ( dim ) mlp_hidden_dim = int ( dim * mlp_ratio ) self . mlp = MLP ( in_features = dim , hidden_features = mlp_hidden_dim , act_layer = act_layer , drop = drop ) if self . shift_size > 0 : # calculate attention mask for SW-MSA H , W = self . input_resolution img_mask = torch . zeros (( 1 , H , W , 1 )) # 1 H W 1 h_slices = ( slice ( 0 , - self . window_size ), slice ( - self . window_size , - self . shift_size ), slice ( - self . shift_size , None )) w_slices = ( slice ( 0 , - self . window_size ), slice ( - self . window_size , - self . shift_size ), slice ( - self . shift_size , None )) cnt = 0 for h in h_slices : for w in w_slices : img_mask [:, h , w , :] = cnt cnt += 1 mask_windows = window_partition ( img_mask , self . window_size ) # nW, window_size, window_size, 1 mask_windows = mask_windows . view ( - 1 , self . window_size * self . window_size ) attn_mask = mask_windows . unsqueeze ( 1 ) - mask_windows . unsqueeze ( 2 ) attn_mask = attn_mask . masked_fill ( attn_mask != 0 , float ( - 100.0 )) . masked_fill ( attn_mask == 0 , float ( 0.0 )) else : attn_mask = None self . register_buffer ( \"attn_mask\" , attn_mask ) def forward ( self , x ): H , W = self . input_resolution B , L , C = x . shape assert L == H * W , \"input feature has wrong size\" shortcut = x x = self . norm1 ( x ) x = x . view ( B , H , W , C ) # cyclic shift if self . shift_size > 0 : shifted_x = torch . roll ( x , shifts = ( - self . shift_size , - self . shift_size ), dims = ( 1 , 2 )) # partition windows x_windows = window_partition ( shifted_x , self . window_size ) # nW*B, window_size, window_size, C else : shifted_x = x # partition windows x_windows = window_partition ( shifted_x , self . window_size ) # nW*B, window_size, window_size, C x_windows = x_windows . view ( - 1 , self . window_size * self . window_size , C ) # nW*B, window_size*window_size, C # W-MSA/SW-MSA attn_windows = self . attn ( x_windows , mask = self . attn_mask ) # nW*B, window_size*window_size, C # merge windows attn_windows = attn_windows . view ( - 1 , self . window_size , self . window_size , C ) # reverse cyclic shift if self . shift_size > 0 : shifted_x = window_reverse ( attn_windows , self . window_size , H , W ) # B H' W' C x = torch . roll ( shifted_x , shifts = ( self . shift_size , self . shift_size ), dims = ( 1 , 2 )) else : shifted_x = window_reverse ( attn_windows , self . window_size , H , W ) # B H' W' C x = shifted_x x = x . view ( B , H * W , C ) x = shortcut + self . drop_path ( x ) # FFN x = x + self . drop_path ( self . mlp ( self . norm2 ( x ))) return x def extra_repr ( self ) -> str : return f \"dim= { self . dim } , input_resolution= { self . input_resolution } , num_heads= { self . num_heads } , \" \\ f \"window_size= { self . window_size } , shift_size= { self . shift_size } , mlp_ratio= { self . mlp_ratio } \" def flops ( self ): flops = 0 H , W = self . input_resolution # norm1 flops += self . dim * H * W # W-MSA/SW-MSA nW = H * W / self . window_size / self . window_size flops += nW * self . attn . flops ( self . window_size * self . window_size ) # mlp flops += 2 * H * W * self . dim * self . dim * self . mlp_ratio # norm2 flops += self . dim * H * W return flops","title":"SwinTransformerBlock"},{"location":"mkdocs/module.html#models.modules.swin_transformer.WindowAttention","text":"Bases: Module Window based multi-head self attention (W-MSA) module with relative position bias. It supports both of shifted and non-shifted window. Parameters: dim ( int ) \u2013 Number of input channels. window_size ( tuple [ int ] ) \u2013 The height and width of the window. num_heads ( int ) \u2013 Number of attention heads. qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 proj_drop ( float , default: 0.0 ) \u2013 Dropout ratio of output. Default: 0.0 Source code in uchiha\\models\\modules\\swin_transformer.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 class WindowAttention ( nn . Module ): r \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias. It supports both of shifted and non-shifted window. Args: dim (int): Number of input channels. window_size (tuple[int]): The height and width of the window. num_heads (int): Number of attention heads. qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 proj_drop (float, optional): Dropout ratio of output. Default: 0.0 \"\"\" def __init__ ( self , dim , window_size , num_heads , qkv_bias = True , qk_scale = None , attn_drop = 0. , proj_drop = 0. ): super () . __init__ () self . dim = dim self . window_size = window_size # Wh, Ww self . num_heads = num_heads head_dim = dim // num_heads self . scale = qk_scale or head_dim ** - 0.5 # define a parameter table of relative position bias self . relative_position_bias_table = nn . Parameter ( torch . zeros (( 2 * window_size [ 0 ] - 1 ) * ( 2 * window_size [ 1 ] - 1 ), num_heads )) # 2*Wh-1 * 2*Ww-1, nH # get pair-wise relative position index for each token inside the window coords_h = torch . arange ( self . window_size [ 0 ]) coords_w = torch . arange ( self . window_size [ 1 ]) coords = torch . stack ( torch . meshgrid ([ coords_h , coords_w ])) # 2, Wh, Ww coords_flatten = torch . flatten ( coords , 1 ) # 2, Wh*Ww relative_coords = coords_flatten [:, :, None ] - coords_flatten [:, None , :] # 2, Wh*Ww, Wh*Ww relative_coords = relative_coords . permute ( 1 , 2 , 0 ) . contiguous () # Wh*Ww, Wh*Ww, 2 relative_coords [:, :, 0 ] += self . window_size [ 0 ] - 1 # shift to start from 0 relative_coords [:, :, 1 ] += self . window_size [ 1 ] - 1 relative_coords [:, :, 0 ] *= 2 * self . window_size [ 1 ] - 1 relative_position_index = relative_coords . sum ( - 1 ) # Wh*Ww, Wh*Ww self . register_buffer ( \"relative_position_index\" , relative_position_index ) self . qkv = nn . Linear ( dim , dim * 3 , bias = qkv_bias ) self . attn_drop = nn . Dropout ( attn_drop ) self . proj = nn . Linear ( dim , dim ) self . proj_drop = nn . Dropout ( proj_drop ) trunc_normal_ ( self . relative_position_bias_table , std = .02 ) self . softmax = nn . Softmax ( dim =- 1 ) def forward ( self , x , mask = None ): \"\"\" Args: x (Tensor):: input features with shape of (num_windows*B, N, C) mask (Tensor):: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None \"\"\" B_ , N , C = x . shape qkv = self . qkv ( x ) . reshape ( B_ , N , 3 , self . num_heads , C // self . num_heads ) . permute ( 2 , 0 , 3 , 1 , 4 ) q , k , v = qkv [ 0 ], qkv [ 1 ], qkv [ 2 ] # make torchscript happy (cannot use tensor as tuple) q = q * self . scale attn = ( q @ k . transpose ( - 2 , - 1 )) relative_position_bias = self . relative_position_bias_table [ self . relative_position_index . view ( - 1 )] . view ( self . window_size [ 0 ] * self . window_size [ 1 ], self . window_size [ 0 ] * self . window_size [ 1 ], - 1 ) # Wh*Ww,Wh*Ww,nH relative_position_bias = relative_position_bias . permute ( 2 , 0 , 1 ) . contiguous () # nH, Wh*Ww, Wh*Ww attn = attn + relative_position_bias . unsqueeze ( 0 ) if mask is not None : nW = mask . shape [ 0 ] attn = attn . view ( B_ // nW , nW , self . num_heads , N , N ) + mask . unsqueeze ( 1 ) . unsqueeze ( 0 ) attn = attn . view ( - 1 , self . num_heads , N , N ) attn = self . softmax ( attn ) else : attn = self . softmax ( attn ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B_ , N , C ) x = self . proj ( x ) x = self . proj_drop ( x ) return x def extra_repr ( self ) -> str : return f 'dim= { self . dim } , window_size= { self . window_size } , num_heads= { self . num_heads } ' def flops ( self , N ): # calculate flops for 1 window with token length of N flops = 0 # qkv = self.qkv(x) flops += N * self . dim * 3 * self . dim # attn = (q @ k.transpose(-2, -1)) flops += self . num_heads * N * ( self . dim // self . num_heads ) * N # x = (attn @ v) flops += self . num_heads * N * N * ( self . dim // self . num_heads ) # x = self.proj(x) flops += N * self . dim * self . dim return flops","title":"WindowAttention"},{"location":"mkdocs/module.html#models.modules.swin_transformer.WindowAttention.forward","text":"Parameters: x ( Tensor ) \u2013 : input features with shape of (num_windows*B, N, C) mask ( Tensor , default: None ) \u2013 : (0/-inf) mask with shape of (num_windows, Wh Ww, Wh Ww) or None Source code in uchiha\\models\\modules\\swin_transformer.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def forward ( self , x , mask = None ): \"\"\" Args: x (Tensor):: input features with shape of (num_windows*B, N, C) mask (Tensor):: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None \"\"\" B_ , N , C = x . shape qkv = self . qkv ( x ) . reshape ( B_ , N , 3 , self . num_heads , C // self . num_heads ) . permute ( 2 , 0 , 3 , 1 , 4 ) q , k , v = qkv [ 0 ], qkv [ 1 ], qkv [ 2 ] # make torchscript happy (cannot use tensor as tuple) q = q * self . scale attn = ( q @ k . transpose ( - 2 , - 1 )) relative_position_bias = self . relative_position_bias_table [ self . relative_position_index . view ( - 1 )] . view ( self . window_size [ 0 ] * self . window_size [ 1 ], self . window_size [ 0 ] * self . window_size [ 1 ], - 1 ) # Wh*Ww,Wh*Ww,nH relative_position_bias = relative_position_bias . permute ( 2 , 0 , 1 ) . contiguous () # nH, Wh*Ww, Wh*Ww attn = attn + relative_position_bias . unsqueeze ( 0 ) if mask is not None : nW = mask . shape [ 0 ] attn = attn . view ( B_ // nW , nW , self . num_heads , N , N ) + mask . unsqueeze ( 1 ) . unsqueeze ( 0 ) attn = attn . view ( - 1 , self . num_heads , N , N ) attn = self . softmax ( attn ) else : attn = self . softmax ( attn ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B_ , N , C ) x = self . proj ( x ) x = self . proj_drop ( x ) return x","title":"forward"},{"location":"mkdocs/module.html#models.modules.swin_transformer.window_partition","text":"images --> window patches Parameters: x ( Tensor ) \u2013 (B, H, W, C) window_size ( int ) \u2013 window size Returns: windows ( Tensor ) \u2013 (num_windows*B, window_size, window_size, C) Source code in uchiha\\models\\modules\\swin_transformer.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def window_partition ( x , window_size ): \"\"\" images --> window patches Args: x (Tensor): (B, H, W, C) window_size (int): window size Returns: windows (Tensor): (num_windows*B, window_size, window_size, C) \"\"\" B , H , W , C = x . shape x = x . view ( B , H // window_size , window_size , W // window_size , window_size , C ) windows = x . permute ( 0 , 1 , 3 , 2 , 4 , 5 ) . contiguous () . view ( - 1 , window_size , window_size , C ) return windows","title":"window_partition"},{"location":"mkdocs/module.html#models.modules.swin_transformer.window_reverse","text":"window patches --> images Parameters: windows ( Tensor ) \u2013 : (num_windows*B, window_size, window_size, C) window_size ( int ) \u2013 Window size H ( int ) \u2013 Height of image W ( int ) \u2013 Width of image Returns: x ( Tensor ) \u2013 : (B, H, W, C) Source code in uchiha\\models\\modules\\swin_transformer.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def window_reverse ( windows , window_size , H , W ): \"\"\" window patches --> images Args: windows (Tensor):: (num_windows*B, window_size, window_size, C) window_size (int): Window size H (int): Height of image W (int): Width of image Returns: x (Tensor):: (B, H, W, C) \"\"\" B = int ( windows . shape [ 0 ] / ( H * W / window_size / window_size )) x = windows . view ( B , H // window_size , W // window_size , window_size , window_size , - 1 ) x = x . permute ( 0 , 1 , 3 , 2 , 4 , 5 ) . contiguous () . view ( B , H , W , - 1 ) return x","title":"window_reverse"},{"location":"mkdocs/module.html#channel-transformer","text":"","title":"Channel Transformer"},{"location":"mkdocs/module.html#models.modules.channel_transformer.ChannelTransformerLayer","text":"Bases: Module Stacked Channel-Transformer-Block Parameters: dim ( int ) \u2013 Number of input channels. input_resolution ( Tuple ( int ) | int ) \u2013 spatial resolution of input features depth ( int ) \u2013 number of stacked channel-transformer-blocks num_heads ( int ) \u2013 Number of heads in Multi-Head Attention mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set drop_rate \u2013 Dropout ratio of output of Attention. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 drop_path ( List | float , default: 0.0 ) \u2013 The probability of DropPath of each ChannelTransformer Block . act_layer ( Module | str , default: GELU ) \u2013 activation function in MLP. norm_layer ( Module | str , default: LayerNorm ) \u2013 normalization layer before Attention and MLP. Default: None Source code in uchiha\\models\\modules\\channel_transformer.py 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 @MODULE . register_module () class ChannelTransformerLayer ( nn . Module ): \"\"\" Stacked Channel-Transformer-Block Args: dim (int): Number of input channels. input_resolution (Tuple(int) | int): spatial resolution of input features depth (int): number of stacked channel-transformer-blocks num_heads (int): Number of heads in `Multi-Head Attention` mlp_ratio (float): ratio of hidden layer to input channel in MLP qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set drop_rate (): Dropout ratio of output of Attention. Default: 0.0 attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 drop_path (List | float): The probability of DropPath of each `ChannelTransformer Block`. act_layer (nn.Module | str): activation function in MLP. norm_layer (nn.Module | str): normalization layer before Attention and MLP. Default: None \"\"\" def __init__ ( self , dim , input_resolution , depth , num_heads , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop_rate = 0. , attn_drop = 0. , drop_path = 0. , act_layer = nn . GELU , norm_layer = nn . LayerNorm , downsample = None ): super () . __init__ () self . dim = dim self . input_resolution = to_2tuple ( input_resolution ) if isinstance ( input_resolution , int ) else input_resolution self . depth = depth self . downsample = build_module ( downsample ) if isinstance ( norm_layer , str ): norm_layer = build_norm ( norm_layer ) if isinstance ( act_layer , str ): act_layer = build_act ( act_layer ) # build blocks self . blocks = nn . ModuleList ([ ChannelTransformerBlock ( dim = dim , input_resolution = self . input_resolution , num_heads = num_heads , mlp_ratio = mlp_ratio , qkv_bias = qkv_bias , qk_scale = qk_scale , drop = drop_rate , attn_drop = attn_drop , drop_path = drop_path [ i ] if isinstance ( drop_path , list ) else drop_path , act_layer = act_layer , norm_layer = norm_layer ) for i in range ( depth )]) def forward ( self , x ): for blk in self . blocks : x = blk ( x ) out = self . downsample ( x ) if self . downsample else x return out","title":"ChannelTransformerLayer"},{"location":"mkdocs/module.html#models.modules.channel_transformer.ChannelTransformerLayers","text":"Bases: Module Collection of Channel-Transformer-Layers Parameters: dims ( List [ int ] ) \u2013 Number of input channels. input_resolutions ( List [ int | Tuple ( int )] ) \u2013 spatial resolution of input features depths ( List [ int ] ) \u2013 number of stacked channel-transformer-blocks num_heads ( List [ int ] ) \u2013 Number of heads in Multi-Head Attention mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set drop_rate \u2013 Dropout ratio of output of Attention. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 drop_path_rate ( float , default: 0.1 ) \u2013 Rate required to generate drop path, it will be called by torch.linspace(0, drop_path_rate, depth) designed to increase the probability of DropPath as the depth increases act_layer ( Module | str , default: GELU ) \u2013 activation function in MLP. norm_layer ( Module | str , default: LayerNorm ) \u2013 normalization layer before Attention and MLP. Default: None Source code in uchiha\\models\\modules\\channel_transformer.py 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 @MODULE . register_module () class ChannelTransformerLayers ( nn . Module ): \"\"\" Collection of Channel-Transformer-Layers Args: dims (List[int]): Number of input channels. input_resolutions (List[int | Tuple(int)]): spatial resolution of input features depths (List[int]): number of stacked channel-transformer-blocks num_heads (List[int]): Number of heads in `Multi-Head Attention` mlp_ratio (float): ratio of hidden layer to input channel in MLP qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set drop_rate (): Dropout ratio of output of Attention. Default: 0.0 attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 drop_path_rate (float): Rate required to generate drop path, it will be called by `torch.linspace(0, drop_path_rate, depth)` designed to increase the probability of DropPath as the depth increases act_layer (nn.Module | str): activation function in MLP. norm_layer (nn.Module | str): normalization layer before Attention and MLP. Default: None \"\"\" def __init__ ( self , dims , input_resolutions , depths , num_heads , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop_rate = 0. , attn_drop = 0. , drop_path_rate = 0.1 , act_layer = nn . GELU , norm_layer = nn . LayerNorm , downsamples = None ): super () . __init__ () self . dims = dims self . input_resolutions = input_resolutions self . depths = depths self . num_heads = num_heads self . num_layers = len ( dims ) self . downsamples = cfg_decomposition ( downsamples ) if len ( self . downsamples ) < self . num_layers : self . downsamples . extend ([ None ] * ( self . num_layers - len ( self . downsamples ))) # stochastic depth self . dpr = [ x . item () for x in torch . linspace ( 0 , drop_path_rate , sum ( depths ))] # build layers self . layers = nn . ModuleList () for i in range ( self . num_layers ): dim = self . dims [ i ] input_resolution = self . input_resolutions [ i ] num_heads = self . num_heads [ i ] depth = self . depths [ i ] downsample = self . downsamples [ i ] drop_path = self . dpr [ sum ( depths [: i ]): sum ( depths [: i + 1 ])] # build layer layer = ChannelTransformerLayer ( dim = dim , input_resolution = input_resolution , depth = depth , num_heads = num_heads , mlp_ratio = mlp_ratio , qkv_bias = qkv_bias , qk_scale = qk_scale , drop_rate = drop_rate , attn_drop = attn_drop , drop_path = drop_path , act_layer = act_layer , norm_layer = norm_layer , downsample = downsample ) self . layers . append ( layer )","title":"ChannelTransformerLayers"},{"location":"mkdocs/module.html#models.modules.channel_transformer.UnetChannelTransformerLayers","text":"Bases: Module Collection of Channel-Transformer-Layers in the shape of unet Parameters: dims ( List [ int ] ) \u2013 Number of input channels. input_resolutions ( List [ int ] ) \u2013 spatial resolution of input features depths ( List [ int ] ) \u2013 number of stacked channel-transformer-blocks num_heads ( List [ int ] ) \u2013 Number of heads in Multi-Head Attention mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set drop_rate \u2013 Dropout ratio of output of Attention. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 drop_path_rate ( float , default: 0.1 ) \u2013 Rate required to generate drop path, it will be called by torch.linspace(0, drop_path_rate, depth) designed to increase the probability of DropPath as the depth increases act_layer ( Module | str , default: GELU ) \u2013 activation function in MLP. norm_layer ( Module | str , default: LayerNorm ) \u2013 normalization layer before Attention and MLP. Default: None Source code in uchiha\\models\\modules\\channel_transformer.py 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 @MODULE . register_module () class UnetChannelTransformerLayers ( nn . Module ): \"\"\" Collection of Channel-Transformer-Layers in the shape of unet Args: dims (List[int]): Number of input channels. input_resolutions (List[int]): spatial resolution of input features depths (List[int]): number of stacked channel-transformer-blocks num_heads (List[int]): Number of heads in `Multi-Head Attention` mlp_ratio (float): ratio of hidden layer to input channel in MLP qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set drop_rate (): Dropout ratio of output of Attention. Default: 0.0 attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 drop_path_rate (float): Rate required to generate drop path, it will be called by `torch.linspace(0, drop_path_rate, depth)` designed to increase the probability of DropPath as the depth increases act_layer (nn.Module | str): activation function in MLP. norm_layer (nn.Module | str): normalization layer before Attention and MLP. Default: None \"\"\" def __init__ ( self , dims , input_resolutions , depths , num_heads , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop_rate = 0. , attn_drop = 0. , drop_path_rate = 0.1 , act_layer = nn . GELU , norm_layer = nn . LayerNorm , downsamples = None ): super () . __init__ () self . dims = dims self . input_resolutions = input_resolutions self . depths = depths self . num_heads = num_heads self . num_layers = len ( dims ) // 2 self . downsamples = cfg_decomposition ( downsamples ) if len ( self . downsamples ) < self . num_layers * 2 : self . downsamples . extend ([ None ] * ( self . num_layers * 2 - len ( self . downsamples ))) # stochastic depth self . dpr = [ x . item () for x in torch . linspace ( 0 , drop_path_rate , sum ( depths [: len ( depths ) // 2 ]))] self . dec_dpr = self . dpr [:: - 1 ] # build layers self . layers = nn . ModuleList () for i in range ( self . num_layers * 2 ): dim = self . dims [ i ] input_resolution = self . input_resolutions [ i ] depth = self . depths [ i ] num_heads = self . num_heads [ i ] downsample = self . downsamples [ i ] drop_path = self . dpr [ sum ( depths [: i ]): sum ( depths [: i + 1 ])] if i >= self . num_layers : drop_path = self . dec_dpr [ sum ( depths [ self . num_layers : i ]): sum ( depths [ self . num_layers : i + 1 ])] # build layer layer = ChannelTransformerLayer ( dim = dim , input_resolution = input_resolution , depth = depth , num_heads = num_heads , mlp_ratio = mlp_ratio , qkv_bias = qkv_bias , qk_scale = qk_scale , drop_rate = drop_rate , attn_drop = attn_drop , drop_path = drop_path , act_layer = act_layer , norm_layer = norm_layer , downsample = downsample ) self . layers . append ( layer )","title":"UnetChannelTransformerLayers"},{"location":"mkdocs/module.html#atom_4","text":"","title":"Atom"},{"location":"mkdocs/module.html#models.modules.channel_transformer.ChannelAttention","text":"Bases: Module attention operations at the channel draw inspiration from Window based multi-head self attention (W-MSA) Parameters: dim ( int ) \u2013 Number of input channels. num_heads ( int ) \u2013 Number of heads in Multi-Head Attention qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 proj_drop ( float , default: 0.0 ) \u2013 Dropout ratio of output. Default: 0.0 Source code in uchiha\\models\\modules\\channel_transformer.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class ChannelAttention ( nn . Module ): r \"\"\" attention operations at the channel draw inspiration from Window based multi-head self attention (W-MSA) Args: dim (int): Number of input channels. num_heads (int): Number of heads in `Multi-Head Attention` qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 proj_drop (float, optional): Dropout ratio of output. Default: 0.0 \"\"\" def __init__ ( self , dim , num_heads , qkv_bias = True , qk_scale = None , attn_drop = 0. , proj_drop = 0. ): super () . __init__ () self . dim = dim self . num_heads = num_heads head_dim = dim // num_heads self . scale = qk_scale or head_dim ** - 0.5 self . qkv = nn . Linear ( dim , dim * 3 , bias = qkv_bias ) self . v = nn . Identity () self . attn_drop = nn . Dropout ( attn_drop ) self . proj = nn . Linear ( dim , dim ) self . proj_drop = nn . Dropout ( proj_drop ) self . softmax = nn . Softmax ( dim =- 1 ) def forward ( self , x ): \"\"\" Args: x (Tensor):: input features with shape of (num_windows*B, N, C) \"\"\" B_ , N , C = x . shape qkv = self . qkv ( x ) qkv = qkv . reshape ( B_ , N , 3 , C ) . permute ( 2 , 0 , 1 , 3 ) q = qkv [ 0 ] . reshape ( B_ , N , self . num_heads , C // self . num_heads ) . permute ( 0 , 2 , 1 , 3 ) k = qkv [ 1 ] . reshape ( B_ , N , self . num_heads , C // self . num_heads ) . permute ( 0 , 2 , 1 , 3 ) v = qkv [ 2 ] v = self . v ( v ) v = v . reshape ( B_ , N , self . num_heads , C // self . num_heads ) . permute ( 0 , 2 , 1 , 3 ) q = q . transpose ( - 2 , - 1 ) k = k . transpose ( - 2 , - 1 ) v = v . transpose ( - 2 , - 1 ) q = F . normalize ( q , dim =- 1 , p = 2 ) k = F . normalize ( k , dim =- 1 , p = 2 ) attn = ( k @ q . transpose ( - 2 , - 1 )) attn = attn * self . scale attn = attn . softmax ( dim =- 1 ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B_ , N , C ) x = self . proj ( x ) x = self . proj_drop ( x ) return x","title":"ChannelAttention"},{"location":"mkdocs/module.html#models.modules.channel_transformer.ChannelAttention.forward","text":"Parameters: x ( Tensor ) \u2013 : input features with shape of (num_windows*B, N, C) Source code in uchiha\\models\\modules\\channel_transformer.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def forward ( self , x ): \"\"\" Args: x (Tensor):: input features with shape of (num_windows*B, N, C) \"\"\" B_ , N , C = x . shape qkv = self . qkv ( x ) qkv = qkv . reshape ( B_ , N , 3 , C ) . permute ( 2 , 0 , 1 , 3 ) q = qkv [ 0 ] . reshape ( B_ , N , self . num_heads , C // self . num_heads ) . permute ( 0 , 2 , 1 , 3 ) k = qkv [ 1 ] . reshape ( B_ , N , self . num_heads , C // self . num_heads ) . permute ( 0 , 2 , 1 , 3 ) v = qkv [ 2 ] v = self . v ( v ) v = v . reshape ( B_ , N , self . num_heads , C // self . num_heads ) . permute ( 0 , 2 , 1 , 3 ) q = q . transpose ( - 2 , - 1 ) k = k . transpose ( - 2 , - 1 ) v = v . transpose ( - 2 , - 1 ) q = F . normalize ( q , dim =- 1 , p = 2 ) k = F . normalize ( k , dim =- 1 , p = 2 ) attn = ( k @ q . transpose ( - 2 , - 1 )) attn = attn * self . scale attn = attn . softmax ( dim =- 1 ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B_ , N , C ) x = self . proj ( x ) x = self . proj_drop ( x ) return x","title":"forward"},{"location":"mkdocs/module.html#models.modules.channel_transformer.ChannelAttentionV2","text":"Bases: Module Channel Attention V2: Linear in spatial Parameters: sequence ( int ) \u2013 length of input sequences. factor ( float ) \u2013 Control the spatial Linear qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 proj_drop ( float , default: 0.0 ) \u2013 Dropout ratio of output. Default: 0.0 Source code in uchiha\\models\\modules\\channel_transformer.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class ChannelAttentionV2 ( nn . Module ): r \"\"\" Channel Attention V2: Linear in spatial Args: sequence (int): length of input sequences. factor (float): Control the spatial `Linear` qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 proj_drop (float, optional): Dropout ratio of output. Default: 0.0 \"\"\" def __init__ ( self , sequence , factor , qkv_bias = True , qk_scale = None , attn_drop = 0. , proj_drop = 0. ): super () . __init__ () self . sequence = sequence self . factor = factor self . scale = qk_scale or sequence ** - 0.5 self . inner_len = int ( self . sequence * self . factor ) self . qk = nn . Linear ( sequence , self . inner_len * 2 , bias = qkv_bias ) self . v = nn . Linear ( sequence , sequence , bias = qkv_bias ) self . attn_drop = nn . Dropout ( attn_drop ) self . proj = nn . Linear ( sequence , sequence ) self . proj_drop = nn . Dropout ( proj_drop ) self . softmax = nn . Softmax ( dim =- 1 ) def forward ( self , x ): \"\"\" Args: x (Tensor):: input features with shape of (B, N, C) \"\"\" B_ , C , N = x . shape N_ = int ( N * self . factor ) qk = self . qk ( x ) qk = qk . reshape ( B_ , C , 2 , N_ ) . permute ( 2 , 0 , 1 , 3 ) q = qk [ 0 ] k = qk [ 1 ] v = self . v ( x ) q = F . normalize ( q , dim =- 1 , p = 2 ) k = F . normalize ( k , dim =- 1 , p = 2 ) attn = ( k @ q . transpose ( - 2 , - 1 )) attn = attn * self . scale attn = attn . softmax ( dim =- 1 ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B_ , C , N ) x = self . proj ( x ) x = self . proj_drop ( x ) return x","title":"ChannelAttentionV2"},{"location":"mkdocs/module.html#models.modules.channel_transformer.ChannelAttentionV2.forward","text":"Parameters: x ( Tensor ) \u2013 : input features with shape of (B, N, C) Source code in uchiha\\models\\modules\\channel_transformer.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def forward ( self , x ): \"\"\" Args: x (Tensor):: input features with shape of (B, N, C) \"\"\" B_ , C , N = x . shape N_ = int ( N * self . factor ) qk = self . qk ( x ) qk = qk . reshape ( B_ , C , 2 , N_ ) . permute ( 2 , 0 , 1 , 3 ) q = qk [ 0 ] k = qk [ 1 ] v = self . v ( x ) q = F . normalize ( q , dim =- 1 , p = 2 ) k = F . normalize ( k , dim =- 1 , p = 2 ) attn = ( k @ q . transpose ( - 2 , - 1 )) attn = attn * self . scale attn = attn . softmax ( dim =- 1 ) attn = self . attn_drop ( attn ) x = ( attn @ v ) . transpose ( 1 , 2 ) . reshape ( B_ , C , N ) x = self . proj ( x ) x = self . proj_drop ( x ) return x","title":"forward"},{"location":"mkdocs/module.html#models.modules.channel_transformer.ChannelTransformerBlock","text":"Bases: Module The block containing Channel Attention, norm and MLP DropPath: Randomly dropout the entire path, usually at the network structure level, dropout a computational path, such as the residual path of the network or some sub-module in the Transformer. Parameters: dim ( int ) \u2013 Number of input channels. input_resolution ( Tuple(int ) \u2013 spatial resolution of input features num_heads ( int ) \u2013 Number of heads in Multi-Head Attention mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set drop ( float , default: 0.0 ) \u2013 Dropout ratio of output of AttentionBlock. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 drop_path ( float , default: 0.0 ) \u2013 Dropout ratio of entire path act_layer ( Module , default: GELU ) \u2013 activation function norm_layer ( Module , default: LayerNorm ) \u2013 normalization layer before Attention and MLP. Default: None Source code in uchiha\\models\\modules\\channel_transformer.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 class ChannelTransformerBlock ( nn . Module ): \"\"\" The block containing Channel Attention, norm and MLP DropPath: Randomly dropout the entire path, usually at the network structure level, dropout a computational path, such as the residual path of the network or some sub-module in the Transformer. Args: dim (int): Number of input channels. input_resolution (Tuple(int)): spatial resolution of input features num_heads (int): Number of heads in `Multi-Head Attention` mlp_ratio (float): ratio of hidden layer to input channel in MLP qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set drop (float): Dropout ratio of output of AttentionBlock. Default: 0.0 attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 drop_path (float): Dropout ratio of entire path act_layer (nn.Module): activation function norm_layer (nn.Module): normalization layer before Attention and MLP. Default: None \"\"\" def __init__ ( self , dim , input_resolution , num_heads , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop = 0. , attn_drop = 0. , drop_path = 0. , act_layer = nn . GELU , norm_layer = nn . LayerNorm ): super () . __init__ () self . dim = dim self . input_resolution = input_resolution self . num_heads = num_heads self . mlp_ratio = mlp_ratio self . norm1 = norm_layer ( dim ) self . attn_C = ChannelAttention ( dim , num_heads = num_heads , qkv_bias = qkv_bias , qk_scale = qk_scale , attn_drop = attn_drop , proj_drop = drop ) self . drop_path : nn . Module = DropPath ( drop_path ) if drop_path > 0. else nn . Identity () self . norm2 = norm_layer ( dim ) mlp_hidden_dim = int ( dim * mlp_ratio ) self . mlp = MLP ( in_features = dim , hidden_features = mlp_hidden_dim , act_layer = act_layer , drop = drop ) def forward ( self , x ): H , W = self . input_resolution B , L , C = x . shape assert L == H * W , \"input feature has wrong size\" shortcut = x x = self . norm1 ( x ) # W-MSA/SW-MSA x = self . attn_C ( x ) # FFN x = shortcut + self . drop_path ( x ) x = x + self . drop_path ( self . mlp ( self . norm2 ( x ))) return x","title":"ChannelTransformerBlock"},{"location":"mkdocs/module.html#models.modules.channel_transformer.ChannelTransformerBlockV2","text":"Bases: Module The block containing Channel Attention, norm and MLP DropPath: Randomly dropout the entire path, usually at the network structure level, dropout a computational path, such as the residual path of the network or some sub-module in the Transformer. Parameters: sequence ( int ) \u2013 length of input sequences. input_resolution ( Tuple(int ) \u2013 spatial resolution of input features num_heads ( int ) \u2013 Number of heads in Multi-Head Attention factor ( float ) \u2013 Control the spatial Linear mlp_ratio ( float , default: 4.0 ) \u2013 ratio of hidden layer to input channel in MLP qkv_bias ( bool , default: True ) \u2013 If True, add a learnable bias to query, key, value. Default: True qk_scale ( float | None , default: None ) \u2013 Override default qk scale of head_dim ** -0.5 if set drop ( float , default: 0.0 ) \u2013 Dropout ratio of output of AttentionBlock. Default: 0.0 attn_drop ( float , default: 0.0 ) \u2013 Dropout ratio of attention weight. Default: 0.0 drop_path ( float , default: 0.0 ) \u2013 Dropout ratio of entire path act_layer ( Module , default: GELU ) \u2013 activation function norm_layer ( Module , default: LayerNorm ) \u2013 normalization layer before Attention and MLP. Default: None Source code in uchiha\\models\\modules\\channel_transformer.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 class ChannelTransformerBlockV2 ( nn . Module ): \"\"\" The block containing Channel Attention, norm and MLP DropPath: Randomly dropout the entire path, usually at the network structure level, dropout a computational path, such as the residual path of the network or some sub-module in the Transformer. Args: sequence (int): length of input sequences. input_resolution (Tuple(int)): spatial resolution of input features num_heads (int): Number of heads in `Multi-Head Attention` factor (float): Control the spatial `Linear` mlp_ratio (float): ratio of hidden layer to input channel in MLP qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set drop (float): Dropout ratio of output of AttentionBlock. Default: 0.0 attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 drop_path (float): Dropout ratio of entire path act_layer (nn.Module): activation function norm_layer (nn.Module): normalization layer before Attention and MLP. Default: None \"\"\" def __init__ ( self , sequence , input_resolution , factor , mlp_ratio = 4. , qkv_bias = True , qk_scale = None , drop = 0. , attn_drop = 0. , drop_path = 0. , act_layer = nn . GELU , norm_layer = nn . LayerNorm ): super () . __init__ () self . sequence = sequence self . input_resolution = input_resolution self . mlp_ratio = mlp_ratio self . norm1 = norm_layer ( sequence ) self . attn_C = ChannelAttentionV2 ( sequence , factor = factor , qkv_bias = qkv_bias , qk_scale = qk_scale , attn_drop = attn_drop , proj_drop = drop ) self . drop_path : nn . Module = DropPath ( drop_path ) if drop_path > 0. else nn . Identity () self . norm2 = norm_layer ( sequence ) mlp_hidden_dim = int ( sequence * mlp_ratio ) self . mlp = MLP ( in_features = sequence , hidden_features = mlp_hidden_dim , act_layer = act_layer , drop = drop ) def forward ( self , x ): H , W = self . input_resolution B , L , C = x . shape assert L == H * W , \"input feature has wrong size\" x = x . permute ( 0 , 2 , 1 ) shortcut = x x = self . norm1 ( x ) # W-MSA/SW-MSA x = self . attn_C ( x ) # FFN x = shortcut + self . drop_path ( x ) x = x + self . drop_path ( self . mlp ( self . norm2 ( x ))) return x . permute ( 0 , 2 , 1 )","title":"ChannelTransformerBlockV2"}]}